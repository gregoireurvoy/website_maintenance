[
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Grégoire Urvoy, Msc - Research & Teaching Assistant",
    "section": "",
    "text": "I am Grégoire Urvoy and I am research and teaching assistant at HEG Geneva. This site, developed with R, aims to share my work, professional and personal projects.\nIn a few words, I studied economics and finance at HEC Lausanne where I was research assistant for many years (in macro economics and marketing classes). I also hold a certificate of open studies in applied data science from EPFL. I am particularly interested in how to use applied statistical models in the fields of economics, sports, new technologies and video games. I mainly work with R and Python.\nMy competences in data science are mainly data collection, working with API, web scraping, data cleaning, analysis with statistics and machine learning, data visualization. I really like to get insights from data and the story telling around it.\nIn my life, I do triathlons, running and cycling. I love watching football and sport in general.\n\n\nFor more information, you can download my CV here:\n\n\n Download CV"
  },
  {
    "objectID": "w_projet_3.html",
    "href": "w_projet_3.html",
    "title": "Relationship between performance and injuries in professional football (In progress)",
    "section": "",
    "text": "In development"
  },
  {
    "objectID": "visualization_using_R.html",
    "href": "visualization_using_R.html",
    "title": "Visualization using R",
    "section": "",
    "text": "Code\n# See tutorial\n\n\n\n\n\n\n\n\n\n\n\nCode\n# See tutorial"
  },
  {
    "objectID": "visualization_using_R.html#density-map",
    "href": "visualization_using_R.html#density-map",
    "title": "Visualization using R",
    "section": "",
    "text": "Code\n# See tutorial"
  },
  {
    "objectID": "visualization_using_R.html#choropleth-map",
    "href": "visualization_using_R.html#choropleth-map",
    "title": "Visualization using R",
    "section": "",
    "text": "Code\n# See tutorial"
  },
  {
    "objectID": "visualization_using_R.html#line-chart",
    "href": "visualization_using_R.html#line-chart",
    "title": "Visualization using R",
    "section": "2.1. Line chart",
    "text": "2.1. Line chart\n\n\n\nCode\ndata &lt;- data %&gt;% \n  mutate(Writers = factor(Writers, levels = unique(Writers)))\n\nggplot(data, aes(x = Writers, y = Value, color = Categorie, group = Categorie)) + \n  geom_line(linewidth = 1) +\n  geom_point(size = 3) + \n  scale_color_manual(values = c(\"#738ab3\", \"#fb7319\", \"grey65\"), guide = \"none\") +\n  scale_x_discrete(labels = c(\"Repeat writers \\n92K\", \"New writers\\n45K\"),\n                   limits = c(\"Repeat_writers_92K\", \"New_writers_45K\"),\n                   expand = c(0,0)) +\n  scale_y_continuous(limit = c(0, 0.6)) +\n  labs(title = \"Product X writers by promo type\",\n       subtitle = \"% OF TOTAL\") + \n  theme(panel.background = element_rect(fill = \"white\"),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.text.x = element_text(size = 12, hjust = c(0,1)),\n        axis.line.x = element_line(color = \"grey85\"),\n        axis.ticks.x = element_line(color = \"grey85\"), \n        plot.title = element_text(size = 11, hjust = -1.5),\n        plot.subtitle = element_text(size = 9, hjust = -0.25),\n        plot.margin = unit(c(2, 10, 1, 2), \"cm\")) +\n  coord_cartesian(clip = \"off\") +\n  geom_text(data = data %&gt;% filter(Writers == \"Repeat_writers_92K\"),\n            aes(label = scales::percent(Value)), \n            hjust = 1.4)  +\n  geom_text(data = data %&gt;% filter(Writers == \"New_writers_45K\"),\n            aes(label = scales::percent(Value)), \n            hjust = -0.4) +\n  geom_text(data = data %&gt;% filter(Writers == \"New_writers_45K\"), \n             aes(label = Categorie),\n             hjust = -0.7) +\n  geom_textbox(x = 2.6, y = 0.3, width = unit(1, \"npc\"), hjust = 0,\n               aes(label = \"&lt;span style='font-size:12pt; color:black'&gt;Though &lt;span style='font-size:12pt; color:#738ab3'&gt;**Promo A**&lt;/span&gt; makes up the buggest segment overall, it contributes less to new writers than to repeat writers.&lt;/span&gt; &lt;br&gt; &lt;br&gt; &lt;span style='font-size:12pt; color:black'&gt;Both &lt;span style='font-size:12pt; color:#fb7319'&gt;**Promo B**&lt;/span&gt; and &lt;span style='font-size:12pt; color:grey65'&gt;**Promo C**&lt;/span&gt; brought in higher proportions of new writers compared to repeat writers.&lt;/span&gt; &lt;br&gt;&lt;br&gt; &lt;span style='font-size:12pt; color:black'&gt;**How should we use this data for our future promotion strategy?** &lt;/span&gt;\", box.colour = \"white\")) + \n  geom_textbox(x = 0.79, y = 0.8, width = unit(3.2, \"npc\"),  hjust = 0,\n              aes(label = \"&lt;span style='font-size:14pt; color:black;'&gt;**There were 45K new writers in the past year.** &lt;/span&gt; &lt;br&gt; &lt;span style='font-size:10pt; color:grey25'&gt;% The distribution across promo types looks different than repeat writers.&lt;/span&gt;\"), box.colour = \"white\")\n\n\n\n\n\n\n\n\nCode\ndata &lt;- data %&gt;% \n  pivot_longer(!Year, names_to = \"Type\", values_to = \"Values\")\n\n\nggplot(data, aes(x = Year, y = Values, color = Type, group = Type)) +\n  geom_line(size = 1) +\n  geom_point(data = data %&gt;% filter(Year == 2019), \n             aes(color = Type),\n             size = 2) +\n  geom_point(data = data %&gt;% filter((Year == 2010 | Year == 2016), Type == \"Referral\"),\n             aes(color = Type),\n             size = 2) +\n  geom_text(data = data %&gt;% filter(Year == 2019),\n            aes(label = scales::percent(Values)),\n            x = 2019.2,\n            hjust = 0,\n            fontface = \"bold\") +\n  geom_text(data = data %&gt;% filter(Year == 2019), \n            aes(label = Type),\n            hjust = 0,\n            x = 2020,\n            fontface = \"bold\") +\n  labs(x = \"FISCAL YEAR\",\n       y = \"CONVERSION RATE\") +\n  scale_color_manual(values = c(\"grey55\",\"#d54545\",\"grey55\"),\n                     guide = \"none\") +\n  scale_y_continuous(limits = c(0,0.1),\n                     breaks = seq(0,0.1,0.01),\n                     labels = label_percent(accuracy = 1),\n                     expand = c(0,0) ) +\n  scale_x_continuous(limits = c(2005,2019),\n                     breaks = seq(2005,2019,1),\n                     expand = c(0,0)) +\n  theme(panel.background = element_blank(),\n      axis.line = element_line(color = \"grey\"),\n      axis.ticks.x = element_line(color = \"grey\"),\n      axis.ticks.y = element_line(color = \"grey\"),\n      axis.text = element_text(color = \"grey35\"),\n      axis.title.x = element_text(color = \"grey35\", hjust = 0, size = 9 ),\n      axis.title.y = element_text(color = \"grey35\", hjust = 1, size = 9),\n      plot.margin = unit(c(2, 5, 1, 1), \"cm\")) +\n  coord_cartesian(clip = \"off\") +\n  geom_textbox(x = 2012.5, y = 0.11, width = unit(20, \"cm\"), \n               aes(label = \"&lt;span style='font-size:16pt; color:black'&gt; Conversion rate over time: &lt;span style='color:#d54545'&gt;**referral decreasing markedly since 2010**&lt;/span&gt; &lt;/span&gt;\"), box.color = \"white\") +\n  annotate(\"segment\", x = 2010.02, xend = 2010.02, y = 0.025, yend = 0.053) +\n  annotate(\"segment\", x = 2016.02, xend = 2016.02, y = 0.02, yend = 0.033) +\n  geom_textbox(x = 2009.8, y = 0.014, width = unit(7, \"cm\"), \n               aes(label = \"&lt;span style='font-size:8pt; color:#d54545'&gt;**2010: all-time referral conversion high**&lt;/span&gt;&lt;span style='font-size:8pt; color:grey35'&gt; (5.5%). Strong partnerships historically meant steady coversion. Entry of competitor ABC has markedly impacted refferal quality: fewer are buying. &lt;/span&gt;\"), box.color = \"white\", hjust = 0) +\n  geom_textbox(x = 2015.8, y = 0.010, width = unit(4, \"cm\"), \n               aes(label = \"&lt;span style='font-size:8pt; color:#d54545'&gt;**2016: new compaigns** &lt;/span&gt; &lt;span style='font-size:8pt; color:grey35'&gt; led to brief uptick; steady decrease since then.&lt;/span&gt;\"), box.color = \"white\", hjust = 0)\n\n\n\n\n\n\n\n\nCode\nggplot(dataset_2, aes(annee, valeurs, color = pays, size = pays)) + \n  geom_line() + \n  scale_color_manual(values = c(\"Switzerland\" = \"salmon\",\n                                \"United States\" = \"lightblue\"),\n                     na.value = \"grey50\") +\n  scale_size_manual(values = c(\"Switzerland\" = 1,\n                               \"United States\" = 1),\n                    na.value = 0.5) +\n    labs(title = \"L'augmentation et la diminution de la consommation de cigarettes\",\n         subtitle = \"Ventes de cigarettes par adulte et par jour, dans certains pays. Les chiffres incluent les cigarettes manufacturées, \\nainsi qu'une estimation du nombre de cigarettes roulées à la main, par adulte (15 ans et plus) et par jour.\",\n         x = \"\",\n         y = \"cigarettes vendues par jour et par adulte\",\n         caption = \"Source:  National statistics, Our world in data\") +\n  guides(size = FALSE) +\n  theme_bw() + \n  theme(plot.title = element_text( face = \"bold\", size =14),\n        plot.subtitle = element_text( face = \"italic\"))\n\n\n\n\n\n\n\n\nCode\ndata &lt;- data %&gt;% \n  mutate(Date = str_replace_all(Date, \".19\",\"\"),\n         Date = factor(Date, levels = unique(Date)))\n\nggplot(data , aes(x = Date, y = Values)) + \n  geom_line(aes(color = Type, group = Type, linetype = Type), linewidth = 0.8) +\n  geom_point(data = data %&gt;% filter( Type == \"Indirect\" , Values &gt; 90), \n             color = \"#fb7319\", size = 2) + \n  geom_text(data = data %&gt;% filter(Type == \"Indirect\" , Values &gt; 90), \n            aes(label = Values), vjust = - 1, color = \"#fb7319\", fontface = \"bold\") +\n  scale_linetype_manual(values = c(\"solid\", \"longdash\", \"solid\"), guide = \"none\") +\n  scale_color_manual(values = c(\"grey55\", \"grey55\", \"#738ab3\"), guide=\"none\") + \n  scale_y_continuous(limits = c(0, 135), breaks = seq(0, 135, 15), expand = c(0, 0))+\n  labs(x = \"2019\", y = \"DAYS TO CLOSE\") + \n  theme(panel.background = element_blank(),\n        axis.line = element_line(color = \"grey\"),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_line(color = \"grey\"),\n        axis.text = element_text(color = \"grey35\"),\n        axis.title.x = element_text(color = \"grey35\", hjust = 0.03, size = 9 ),\n        axis.title.y = element_text(color = \"grey35\", hjust = 1, size = 9),\n        plot.margin = unit(c(2, 2, 1, 1), \"cm\")) +\n  coord_cartesian(clip = \"off\") +\n  geom_textbox(x = 6, y = 145, width = unit(15, \"cm\"), \n               aes(label = \"&lt;span style='font-size:14pt; color:black'&gt; Time to close deal: &lt;span style='font-size:14pt; color:#fb7319'&gt;**indirect sales missed goal 3 times**&lt;/span&gt; &lt;/span&gt;\"), box.color = \"white\") +\n  geom_text(data = data %&gt;% filter(Date == \"DEC\"), \n            aes(label = Type, color = Type), hjust = -0.1, fontface = \"bold\")"
  },
  {
    "objectID": "visualization_using_R.html#bar-chart",
    "href": "visualization_using_R.html#bar-chart",
    "title": "Visualization using R",
    "section": "2.2. Bar chart",
    "text": "2.2. Bar chart\n\n\n\nCode\ndata_1$month &lt;- factor(data_1$month, levels = unique(data_1$month))\n\nggplot(data_1) +\n  geom_col(aes(month, rate,  fill = month), color = \"grey40\") +\n  labs(title = \"2019 monthly voluntary attrition rate\",\n       x= \"2019\",\n       y= \"ATTRITION RATE\") + \n  scale_y_continuous(limits = c(0,0.01), \n                     labels = scales::percent,\n                     breaks = seq(0,0.01,0.001),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = c(\"APR\" = \"#ed1d25\",\n                               \"JUL\" = \"#ec7c30\",\n                               \"AUG\" = \"#ec7c30\",\n                               \"NOV\" = \"#5d9bd1\",\n                               \"DEC\" = \"#5d9bd1\"),\n                     na.value = \"grey65\") +\n  theme(panel.background = element_rect(fill = \"white\"),\n        axis.line = element_line(color = \"grey65\", linewidth = 0.1),\n        axis.ticks = element_line(color = \"grey65\", linewidth = 0.2), \n        axis.text = element_text(color = \"grey45\"),\n        axis.title = element_text(color = \"grey45\", size = 12),\n        axis.title.y = element_text(hjust = 1),\n        axis.title.x = element_text(hjust = 0),\n        plot.title = element_text(color = \"grey25\", \n                                  size = 18, margin=margin(0,0,15,0)),\n        plot.margin = unit(c(0.1, 6, 0.1, 0.1), \"cm\"),\n        legend.position=\"none\") +\n  coord_cartesian(xlim = c(1, 12), clip = \"off\") +\n  annotate(\"text\", hjust = 0, x = 13, y = 0.0095, size = 5,  \n           label = \"Highlights:\") +\n  annotate(\"text\", hjust = 0, x = 13, y = 0.0072, label = \"In April there was a \\nreorganization. No jobs \\nwere eliminated, but \\nmany people chose to \\nleave.\") +\n  annotate(\"text\", hjust = 0, x = 13, y = 0.0038, label = \"Attrition rates tend to be \\nhigher in the Summer \\nmonths when it is \\ncommon for associates \\nto leave to go back to \\nschool.\") +\n  annotate(\"text\", hjust = 0, x = 13, y = 0.00075, label = \"Attrition is typically low in \\nNovember & December \\ndue to the holidays.\") +\n  annotate(\"rect\", xmin=3.5, xmax= 18.7, ymin=0.006, ymax= 0.0084, fill = \"#ed1d25\", alpha= 0.15) + \n  annotate(\"rect\", xmin=6.5, xmax= 18.7, ymin=0.0024, ymax= 0.0052, fill = \"#ec7c30\", alpha= 0.15) + \n  annotate(\"rect\", xmin=10.5, xmax= 18.7, ymin=0, ymax= 0.0015, fill = \"#5d9bd1\", alpha= 0.15)\n\n\n\n\n\n\n\n\nCode\ndataset_1_c &lt;- dataset_1 %&gt;%\n  mutate(col_bar=ifelse(Value &gt; 5.5, \"1\", \"0\"))\n\nggplot(dataset_1_c, aes(date, Value, fill=col_bar)) + \n  geom_col(width = 0.7, alpha = 0.9) +\n  scale_fill_manual(values = c(\"grey65\", \"red3\")) +\n  theme(legend.position = \"none\") + \n  scale_x_discrete(breaks = c(\"2016/01\",\"2017/01\", \"2018/01\",\"2019/01\", \"2020/01\") ,\n                   labels = c(2016, 2017, 2018, 2019, 2020)) +\n  scale_y_continuous( breaks = c(0,2,4,6,8,10,12,14,16),\n                      labels = c(0,2,4,6,8,10,12,14,16))  +\n  labs(title = \"Taux de chômage aux États-Unis de 2016 à 2020\",\n       subtitle = \"Malgré une lente diminution depuis des années, le taux de chômage \\na explosé à cause de l'épidémie du Covid.\",\n       x = \"\",\n       y = \"Taux de chômage (en %)\",\n       caption = \"Source: U.S. BUREAU OF LABOR STATISTICS\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCode\nggplot( profits, aes(annees, valeur_milliards, fill = type)) +\n  geom_col() +\n  scale_fill_manual( values = c( \"#bfa6a2\", \"#506b93\")) + \n  labs(title = \"Les profits volatils de la banque d'investissement\",\n       subtitle = \"Bénéfices de la banque d'investissement et des autres divisions de Credit Suisse depuis 2006\",\n       x = \"\",\n       y = \"\",\n       caption = \"Source: S&P Capital IQ & The Wall Street Journal\") +\n  scale_x_continuous(breaks = c(2006,2008,2010,2012,2014,2016,2018,2020) , \n                     labels = c(2006,2008,2010,2012,2014,2016,2018,2020)  ) +\n  scale_y_continuous(limits = c(-15,15), breaks = c(-15, -10, -5, 0, 5 ,10, 15) , \n                     labels = c(\"-15 milliards $\", \"-10\", \"-5\", \"0\", \"5\" ,\"10\" , \"15 milliards $\") ) +\n  theme(plot.title = element_text( face = \"bold\"),\n        plot.caption = element_text( color =\"#676767\"),\n        axis.line.x = element_line(color= \"black\"),\n        axis.text = element_text( color =\"gray\"),\n        axis.ticks.y = element_blank(), \n        panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = c(\"darkgray\", \"darkgray\", \"darkgray\", \"black\", \"darkgray\", \"darkgray\", \"darkgray\"), \n                                          linewidth = 0.6,\n                                          linetype = c(\"dotted\", \"dotted\", \"dotted\", \"solid\",\"dotted\", \"dotted\", \"dotted\" )),\n        panel.grid.major.x = element_blank(),\n        legend.position = \"none\"\n        )"
  },
  {
    "objectID": "visualization_using_R.html#lolipop-chart",
    "href": "visualization_using_R.html#lolipop-chart",
    "title": "Visualization using R",
    "section": "2.3. Lolipop chart",
    "text": "2.3. Lolipop chart\n\n\n\nCode\ndf &lt;- df %&gt;% \n  arrange(moyenne) %&gt;% \n  mutate(pays = factor(pays, pays))\n\nggplot(df) +\n  geom_segment(aes(x = pays, xend = pays, y = a_1950, yend = a_2019), color = \"grey\") +\n  geom_point(aes(x = pays, y = a_1950), color = \"firebrick2\", alpha = 0.5,  size=4 ) +\n  geom_point(aes(x = pays, y = a_2019), color = \"forestgreen\", alpha = 0.5, size=4 ) +\n  coord_flip()+\n  theme_minimal() +\n  theme( legend.position = \"none\" ) +\n  xlab(\"\") +\n  ylab(\"Esperance de vie (en années)\") +\n  labs(title = \"Evolution de l'espérance de vie de 1950 à 2019\")"
  },
  {
    "objectID": "visualization_using_R.html#simple-bar-charts",
    "href": "visualization_using_R.html#simple-bar-charts",
    "title": "Visualization using R",
    "section": "3.1. Simple bar charts",
    "text": "3.1. Simple bar charts\n\n\nCode\ndata &lt;- data %&gt;% \n  mutate(Name = factor(Name, levels = rev(unique(Name))),\n         col_bar = case_when(Volume &lt; -0.1 ~ \"0\",\n                             -0.1 &lt;= Volume & Volume &lt; -0.05 ~ \"1\",\n                             -0.05 &lt;= Volume & Volume &lt; 0.05 ~ \"2\",\n                             0.05 &lt;= Volume & Volume &lt; 0.1 ~ \"3\",\n                             .default = \"4\"),\n         level = ifelse(Volume &lt; 0, \"negative\", \"positive\"))\n\n\nggplot(data, aes(x = Volume, y = Name)) +\n  geom_col(aes(fill = col_bar)) + \n  geom_text(data = data %&gt;% filter(level == \"negative\"),\n            aes(label = Name, color = col_bar),\n            hjust = 0,\n            x = 0.01) +\n  geom_text(data = data %&gt;% filter(level == \"positive\"),\n            aes(label = Name, color = col_bar),\n            hjust = 1,\n            x = -0.01) +\n  scale_color_manual(values = c(\"#ea7b2f\",\"#f3b279\",\"grey\",\"#8daad9\",\"#4574b2\"),\n                     guide = \"none\") +\n  scale_fill_manual(values = c(\"#ea7b2f\",\"#f3b279\",\"grey\",\"#8daad9\",\"#4574b2\"),\n                    guide = \"none\") +\n  scale_x_continuous(limits = c(-0.2,0.2),\n                     breaks = seq(-0.2,0.2,0.05),\n                     labels = label_percent(accuracy = 1),\n                     position = \"top\",\n                     expand = c(0,0)) +\n  theme(panel.background = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.x = element_line(color = \"grey\"),\n        axis.text.x = element_text(color = \"grey45\", size = 10),\n        axis.line.x = element_line(color= \"grey\"),\n        plot.margin = unit(c(3, 7, 1, 1), \"cm\")) +\n  coord_cartesian(clip = \"off\") +\n  geom_textbox(x = 0.21, y = 0,\n               hjust = 0,\n               vjust = 0,\n               width = unit(6, \"cm\"),\n               aes(label = \"&lt;span style='font-size:9pt; color:black'&gt; Eight key cat food brands declined in sales year-over-year, with five brands decreasing 7%+. This was expected in some cases due to focus shift toward higher margin brands. &lt;span style='color:#ea7b2f'&gt;**Fran's Recipe and Wholesome Goodness each declined by more than 13%**, which was more than expected.&lt;/span&gt; &lt;br&gt; &lt;br&gt;\nOn the positive side, &lt;span style='color:#4574b2'&gt;five brands increased 8%+ year-over-year, with **marked 16%+ increases for NutriBalance and Farm Fresh Basics.**&lt;/span&gt;  &lt;br&gt; &lt;br&gt;\n**What can we learn from increasing brands that we can apply elsewhere?** Let's discuss next steps. &lt;/span&gt;\"), box.color = \"white\") +\n  geom_textbox(x = 0.01 , y = 21 , \n               vjust = 0, \n               width = unit(6, \"cm\"),\n    aes(label = \"&lt;span style='font-size:11pt; color:#ea7b2f'&gt;DECREASED&lt;/span&gt; | &lt;span style='font-size:11pt; color:#4574b2'&gt;INCREASED&lt;/span&gt;\"), box.color = \"white\") +\n  geom_textbox(x = -0.22, y = 23,\n               vjust = 0,\n               hjust = 0,\n               width = unit(15,\"cm\"),\n               aes(label = \"&lt;span style='font-size:14pt; color:black'&gt;Cat food brands: **mixed results in sales year-over-year** &lt;/span&gt; &lt;br&gt; &lt;span style='font-size:10pt; color:grey'&gt; YEAR-OVER-YEAR % CHANGE IN SALES VOLUME ($) &lt;/span&gt;\"), box.color = \"white\")\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes( reorder(pays, nombre), nombre  )) + \n  geom_col( fill = c(\"#df6e6e\", rep(\"#076ea1\",19)), width = 0.7) +\n  coord_flip() + \n  scale_y_continuous(\"Potential immigrants, m\", position = 'right') +\n  scale_x_discrete(\"\") +\n  labs(title = \"Countries to which people want to migrate\",\n       caption=\"Source: Gallup\") +\n  theme(plot.title = element_text(hjust = 0, face = \"bold\", size =12),\n        plot.caption = element_text(hjust = 0, color = \"darkgray\"),\n        axis.title.x = element_text(hjust = 0),\n        axis.text.y = element_text(hjust=0),\n        axis.line.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.ticks.x = element_line(color = c(\"#201f21\", rep(\"#c2cfd7\",3))),\n        panel.background = element_blank(),\n        panel.grid.major.x = element_line(color = c(\"#201f21\", rep(\"#c2cfd7\",3))),\n        legend.position = \"none\"\n        )\n\n\n\n\n\n\n\n\nCode\ndata &lt;- data %&gt;% \n  mutate(diff = buildingsway - competitors,\n         category = factor(category, levels = rev(unique(category))))\n\nggplot(data, aes(diff, category, fill = diff)) + \n  geom_col() +\n  labs(title = \"Buildingsways vs Competitors Difference\",\n       x = \"\",\n       y = \"Category\") +\n  scale_x_continuous(limits = c(-0.2,0.4),\n                     breaks = seq(-0.2,0.2,0.1),\n                     expand = c(0,0)) +\n  scale_fill_gradient2(low = \"salmon\", mid = \"#c8dbe0\", high = \"#0051c7\", guide = \"none\") +\n  scale_y_discrete(labels = wrap_format(25)) +\n  theme_minimal() +\n  theme(plot.margin = unit(c(0.1, 5, 1, 0.1), \"cm\"),\n        panel.grid = element_blank(),\n        plot.title = element_text(color = \"grey45\", face = \"bold\", hjust = -0.75),\n        axis.title.y = element_blank(),\n        axis.text.y = element_text(face = \"bold\", color = \"grey15\")\n  ) +\n  coord_cartesian(xlim = c(-0.2,0.2) , ylim = c(1,10), clip = \"off\") +\n  annotate(\"text\", hjust = 0,  x = -0.078, y = -1, size = 4,  label = \"Underperform\", color = \"salmon\") +\n  annotate(\"text\", hjust = 0,  x = 0.01, y = -1, size = 4,  label = \"Overperform\", color = \"#0051c7\") +\n  annotate(\"text\", hjust = 0,  x = 0, y = -1, size= 4,  label = \"|\") +\n  annotate(\"text\", hjust = 0,  x = 0.21, y = 9, size= 3.5,  label = \"Buildingsway leads the \\nmarket in exclusive \\ntreehouse options and \\ncreates a strong breand \\nagainst our competitors\", color = \"#0051c7\") +\n  annotate(\"text\", hjust = 0,  x = 0.21, y = 5, size= 3.5,  label = \"Areas of improvement \\nmainly focus around \\ncustomer interactions \\nwith staff\", color = \"salmon\") +\n  annotate(\"text\", hjust = 0,  x = 0.21, y = 1.5, size= 3.5, fontface =2, label = \"Recommendation: \\nFocus on improving \\nstaff and customer \\nrelationships\")"
  },
  {
    "objectID": "visualization_using_R.html#stacked-bar-charts",
    "href": "visualization_using_R.html#stacked-bar-charts",
    "title": "Visualization using R",
    "section": "3.2. Stacked bar charts",
    "text": "3.2. Stacked bar charts\n\n\n\nCode\ndf$salaire_mensuel_CHF &lt;- factor(df$salaire_mensuel_CHF,\n                                 levels = unique(df$salaire_mensuel_CHF))\n\nggplot(df, aes(x = salaire_mensuel_CHF, y = pourcent, fill = genre)) + \n  geom_bar(position = \"fill\", stat = \"identity\") +\n  scale_fill_manual(\"Genre\", values = c(   \"#e4676a\" ,\"#d0d0d0\") ) +\n  scale_x_discrete(\"Salaire mensuel (en CHF)\") +\n  scale_y_continuous(\"\", labels = scales::percent) +\n  labs( title = \"Répartition des salaires entre les genres en Suisse en 2020\") +\n  geom_hline( yintercept = 0.5, linetype=\"dotted\", color = \"gray30\", size = 0.5 ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(line = element_blank())"
  },
  {
    "objectID": "visualization_using_R.html#waffle-chart",
    "href": "visualization_using_R.html#waffle-chart",
    "title": "Visualization using R",
    "section": "4.1. Waffle chart",
    "text": "4.1. Waffle chart\n\n\n\nCode\nggplot(df, aes(fill = groupe, values = valeurs)) +\n  geom_waffle( size = 1,  color = \"white\", flip = TRUE) +\n  scale_fill_manual(\"Légende\",\n                    values = c(\"#357eb8\", \"#d9d9d9\"),\n                    labels = c(\"Diplômés\", \"Sans étude\")) +\n  labs(title = \"Education dans le monde\",\n       subtitle = \"Proportion mondiale de personnes diplomés en 2015\") +\n  coord_equal() +\n  theme_void() + \n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14 ),\n        plot.subtitle = element_text(hjust = 0.5 , size = 11)) \n\n\n\n\n\n\n\n\nCode\nggplot(dataset_4, aes(fill = groupe, values = valeurs)) +\n  geom_waffle( size = 1,  color = \"white\", flip = TRUE) +\n  scale_fill_manual(\"Légende\",\n                    values = c(\"chocolate1\", \"grey80\"),\n                    labels = c(\"Accès à Internet\", \"Pas d'accès\")) +\n  labs(title = \"Un monde connnecté?\",\n       subtitle = \"Proportion mondiale de personnes ayant accès à Internet\") +\n  coord_equal() +\n  theme_void() + \n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14 ),\n        plot.subtitle = element_text(hjust = 0.5 , size = 11))"
  },
  {
    "objectID": "visualization_using_R.html#tables",
    "href": "visualization_using_R.html#tables",
    "title": "Visualization using R",
    "section": "4.2. Tables",
    "text": "4.2. Tables\n\n\n\nCode\nggplot(df, aes(x = Score , y = Entreprise , fill = Valeur)) +\n  geom_tile(color = \"black\" ) +\n  geom_text(aes(label = Valeur), color = \"grey20\", size = 3.5) +\n  scale_x_discrete(\"\", position = \"top\" )  +\n  scale_y_discrete(\"\")  +\n  scale_fill_gradient2(low = \"#0d5829\" , mid = \"gold\", high = \"red3\", midpoint = 6)  +\n  labs(title = \"Score ESG des entreprises en 2022 \") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text( face = \"bold\", size =14), \n        panel.grid = element_blank(),\n        axis.text.x = element_text(  size = 10))"
  },
  {
    "objectID": "visualization_using_R.html#treemap",
    "href": "visualization_using_R.html#treemap",
    "title": "Visualization using R",
    "section": "4.3. Treemap",
    "text": "4.3. Treemap\n\n\n\nCode\nggplot(G20, aes(area = gdp_mil_usd, fill = hdi, label = country)) +\n  geom_treemap() +\n  geom_treemap_text(fontface = 'italic', colour = \"white\", \n                    place =\"topleft\", min.size= 4) +\n  scale_fill_gradient2(\"IDH\", low = \"#8b0034\" , mid = \"#ed5733\", \n                       high = \"#fddb79\" , midpoint = 0.75) + \n  labs(title = \"Proportion des richesses mondiales en fonction de l'IDH pour les pays du G20\")"
  },
  {
    "objectID": "visualization_using_R.html#scatter-plots",
    "href": "visualization_using_R.html#scatter-plots",
    "title": "Visualization using R",
    "section": "5.1. Scatter plots",
    "text": "5.1. Scatter plots\n\n\n\nCode\nggplot(df_all, aes(PIB_2020, viande_conso, color = continent,size = population_2020)) +\n  geom_point() +\n  scale_x_continuous(trans='log2', \n                     breaks = c(1000, 2000, 5000, 10000, 20000, 50000, 100000), \n                     labels = c(\"1000\", \"2 000\", \"5 000\", \"10 000\", \"20 000\", \"50 000\", \"100 000$\"))  +\n  scale_y_continuous( breaks = c(0, 50, 100),\n                      labels = c(0, 50, \"100 kg\")) + \n  scale_size_continuous(trans = \"hms\", range = c(0.5,10), guide = \"none\") +\n  scale_color_manual(\"\",\n                     labels = c(\"Afrique\", \"Amériques\",\"Asie\", \"Europe\" , \"Océanie\"), \n                     values = c(\"#b56661\", \"#647a3c\",  \"#b58a2e\", \"#5f9299\", \"#c7bbb8\")) +\n  guides( color = guide_legend(override.aes = list(size=5))) +\n  labs(title = \"L'apétit pour la viande augmente avec la richesse\",\n       subtitle = \"Consommation moyenne de viande en fonction du PIB par habitant et par pays en 2020\",\n       caption = \"Source: FAO & Banque mondiale\",\n       y = \"Viande consommée\",\n       x = \"PIB\") +\n  theme(plot.title = element_text( face = \"bold\", size =14),\n        plot.subtitle = element_text( color = \"#676767\" ,  size =12),\n        plot.caption = element_text( hjust= 0, color =\"#676767\"),\n        axis.title.x = element_text( hjust = 1, color = \"#676767\"),\n        axis.title.y = element_text( color = \"#676767\"),\n        axis.ticks = element_blank(),\n        axis.line = element_line( color = \"#676767\" ),\n        axis.text = element_text( color  = \"#676767\" ),\n        panel.background = element_blank(),\n        panel.grid.major = element_line( color = \"#f0f0f0\", linewidth = 0.6),\n        legend.key = element_rect( fill = \"white\"),\n        legend.position = \"top\",\n        legend.justification ='left')\n\n\n\n\n\n\n\n\nCode\nggplot(dataset_3, aes(GDP_per_capita, Life_expectancy)) +\n  geom_point(aes(size = Population, color = Continent)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"grey40\") +\n  scale_color_manual(values = c(\"Africa\" = \"#e3c730\", \"America\" = \"#66c2a5\",\n                                \"Asia\" = \"#b73e51\", \"Europe\" = \"#8fa1cb\", \n                                \"Oceania\" = \"#325e54\"),\n                     labels = c(\"Africa\" = \"Afrique\", \"America\" = \"Amériques\",\n                                \"Asia\" = \"Asie\", \"Oceania\" = \"Océanie\")) +\n  scale_x_continuous(trans='log2', \n                     breaks = c(1000, 2000, 5000, 10000, 20000, 50000, 150000), \n                     labels = c(\"1000\", \"2 000\", \"5 000\", \"10 000\", \"20 000\",\n                                \"50 000\", \"150 000$\")) +\n  labs(title = \"Richesse et espérance de vie\",\n       subtitle = \"La richesse est l'espérance de vie semblent être deux variables très liées.\",\n       x = \"PIB par habitant (en $) \",\n       y = \"Espérance de vie (en années)\",\n       caption = \"Source: Gapminder\")+ \n  theme_bw() +\n  theme(plot.title = element_text( face = \"bold\", size =14))  +\n  scale_size_continuous(range = c(0.5,15), guide = \"none\")"
  },
  {
    "objectID": "visualization_using_R.html#boxplots",
    "href": "visualization_using_R.html#boxplots",
    "title": "Visualization using R",
    "section": "6.1. Boxplots",
    "text": "6.1. Boxplots\n\n\n\nCode\nggplot(gapminder_2007,  aes(x=continent, y=lifeExp, fill=continent)) +\n  geom_boxplot() +\n  scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n  geom_jitter(color=\"grey20\", size=0.4, alpha=0.9) +\n  theme_ipsum() +\n  theme(\n    legend.position=\"none\",\n    plot.title = element_text(size=13)\n  ) +\n  labs(title = \"Distribution des espérances de vie par continent en 2007\") +\n  ylab(\"Esperance de vie (en années)\")"
  },
  {
    "objectID": "projet_5.html",
    "href": "projet_5.html",
    "title": "Images classifier using deep learning and machine learning models",
    "section": "",
    "text": "Images classifier using deep learning and machine learning models\nThe goal of this project was to build different images classifier. I worked with the Swissroads data set which contains thousands images of vehicles found including cars, trucks, vans, bikes, motorcycles and others.\nThe ultimate goal is to test different classifiers and techniques and choose the best method.\n\n\n1. Feature extraction\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nI decide to use the MobileNet V2 module. As written in the course, “By default, TensorFlow Hub image modules work with float32 images normalized between zero and one. The expected input size is written on the MobileNet V2 module page. In this case, it’s 224x224.”\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ngenerator = ImageDataGenerator(rescale=1/255, validation_split=0.2)\n\n\nimport os\n\ntrainset = generator.flow_from_directory(\n    os.path.join('swissroads', 'train'), batch_size=32, target_size=(224, 224), class_mode='sparse',\n    shuffle=True)\nvalidset = generator.flow_from_directory(\n    os.path.join('swissroads', 'valid'), batch_size=32, target_size=(224, 224), class_mode='sparse',\n    shuffle=False)\ntestset = generator.flow_from_directory(\n    os.path.join('swissroads', 'test'), batch_size=32, target_size=(224, 224), class_mode='sparse',\n    shuffle=False)\n\nFound 280 images belonging to 6 classes.\nFound 139 images belonging to 6 classes.\nFound 50 images belonging to 6 classes.\n\n\nEverything seems to work correctly. There are indeed 6 different classes.\nI create a graph, with characteristics 224x224 pour the number of pixels in width and length and 3 for the RBF colors.\n\nimg_graph = tf.Graph()\n\nwith img_graph.as_default():\n    module_url = 'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2'\n\n    feature_extractor = hub.Module(module_url)\n\n    input_imgs = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3])\n\n    imgs_features = feature_extractor(input_imgs)\n\n    init_op = tf.group([\n        tf.global_variables_initializer(), tf.tables_initializer()\n    ])\n\nimg_graph.finalize() \n\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\n\n\nNow, we can save our model.\nWe can see the different classes of the set:\n\ntrainset.class_indices\n\n{'bike': 0, 'car': 1, 'motorcycle': 2, 'other': 3, 'truck': 4, 'van': 5}\n\n\nWe can see what is inside the trainset for example:\n\nlen(trainset)\n\n9\n\n\nThis correspond to the number of batch. We have 280 and a batch size of 32 so it we should have no more than 9 batch.\n\nlen(trainset[0])\n\n2\n\n\nIn each batch we have two elements.\n\nlen(trainset[7][0])\n\n32\n\n\nThe first element of the list in the image (in pixels).\n\ntrainset[0][1][0]\n\n1.0\n\n\nAnd the second element is the label. Since it as a 1 on the second element, this image should correspond to a car.\n\nimport numpy as np\n\n\ntrain_image= []\ntrain_label=[]\ntrain_feature=[]\n\nwith tf.Session(graph=img_graph) as sess:\n    sess.run(init_op)\n    for i in np.arange(0,len(trainset)):\n        image = trainset[i][0]\n        label = trainset[i][1]\n        feature = sess.run(imgs_features, feed_dict={input_imgs: image})\n        train_image.append(image)\n        train_feature.append(feature)\n        train_label.append(label)\ntrain_feature = [val for sublist in train_feature for val in sublist] #found on internet to flatten the list (https://www.quora.com/How-can-I-drop-brackets-in-a-Python-list-in-order-to-go-from-1-2-3-to-1-2-3#:~:text=Flatten%20the%20list%20to%20%22remove,in%20your%20list%20of%20lists!&text=Nested%20list%20comprehensions%20evaluate%20in,tab%20for%20each%20new%20loop.)\ntrain_image = [val for sublist in train_image for val in sublist]\ntrain_label = [val for sublist in train_label for val in sublist]\n\nI can then store the informations in a npz file:\n\nnp.savez('trainset.npz', train_image=train_image ,train_feature=train_feature, train_label=[int(a) for a in train_label], train_name=list(trainset.class_indices.keys()))\n\n\nvalid_image= []\nvalid_label=[]\nvalid_feature=[]\n\nwith tf.Session(graph=img_graph) as sess:\n    sess.run(init_op)\n    for i in np.arange(0,len(validset)):\n        image = validset[i][0]\n        label = validset[i][1]\n        feature = sess.run(imgs_features, feed_dict={input_imgs: image})\n        valid_image.append(image)\n        valid_feature.append(feature)\n        valid_label.append(label)\nvalid_feature = [val for sublist in valid_feature for val in sublist]\nvalid_image = [val for sublist in valid_image for val in sublist]\nvalid_label = [val for sublist in valid_label for val in sublist]\n\n\nnp.savez('validset.npz', valid_image=valid_image ,valid_feature=valid_feature, valid_label=[int(a) for a in valid_label], valid_name=list(validset.class_indices.keys()))\n\n\ntest_image= []\ntest_label=[]\ntest_feature=[]\n\nwith tf.Session(graph=img_graph) as sess:\n    sess.run(init_op)\n    for i in np.arange(0,len(testset)):\n        image = testset[i][0]\n        label = testset[i][1]\n        feature = sess.run(imgs_features, feed_dict={input_imgs: image})\n        test_image.append(image)\n        test_feature.append(feature)\n        test_label.append(label)\ntest_feature = [val for sublist in test_feature for val in sublist] \ntest_image = [val for sublist in test_image for val in sublist]\ntest_label = [val for sublist in test_label for val in sublist]\n\n\nnp.savez('testset.npz', test_image=test_image ,test_feature=test_feature, test_label=[int(a) for a in test_label], test_name=list(validset.class_indices.keys()))\n\nI also need to create the csv file to load my results at each step of the project.\n\nimport pandas as pd \n\n\nResults = pd.DataFrame(columns = ['model','test_accuracy']).set_index('model')\n\n\nResults.to_csv(r\"Results.csv\")\n\nSource: 1. Feature extraction\n\n2. Data exploration\n\nPlot a few images from each category\n\nFor this task, I plot two images for each of the 6 categories: bike, car, motorcycle, other, truck and van.\n\nimport PIL.Image as Image\nimport numpy as np\n\nbike1 = Image.open('swissroads/test/bike/bike-0100.png')\nbike1\n\n\n\n\n\n\n\n\n\nbike2 = Image.open('swissroads/test/bike/bike-0101.png')\nbike2\n\n\n\n\n\n\n\n\n\ncar1 = Image.open('swissroads/test/car/car-0097.png')\ncar1\n\n\n\n\n\n\n\n\n\ncar2 = Image.open('swissroads/test/car/car-0098.png')\ncar2\n\n\n\n\n\n\n\n\n\nmotorcycle1 = Image.open('swissroads/test/motorcycle/motorcycle-0077.png')\nmotorcycle1\n\n\n\n\n\n\n\n\n\nmotorcycle2 = Image.open('swissroads/test/motorcycle/motorcycle-0078.png')\nmotorcycle2\n\n\n\n\n\n\n\n\n\nother1 = Image.open('swissroads/test/other/other-0049.png')\nother1\n\n\n\n\n\n\n\n\n\nother2 = Image.open('swissroads/test/other/other-0050.png')\nother2\n\n\n\n\n\n\n\n\n\ntruck1 = Image.open('swissroads/test/truck/truck-0064.png')\ntruck1\n\n\n\n\n\n\n\n\n\ntruck2= Image.open('swissroads/test/truck/truck-0065.png')\ntruck2\n\n\n\n\n\n\n\n\n\nvan1 = Image.open('swissroads/test/van/van-0038.png')\nvan1\n\n\n\n\n\n\n\n\n\nvan2 = Image.open('swissroads/test/van/van-0039.png')\nvan2\n\n\n\n\n\n\n\n\n\nMake a scree plot, how many PCA components explain 10%, 20%, …, 90% and 100% of the variance?\n\nFirst, I load the different elements of the train set.\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X = npz_file['train_feature']\n    Y = npz_file['train_label']\n    names = npz_file['train_name']\n\nThen I apply the PCA transformer.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\npca3 = PCA(n_components=None)\n\npca3.fit(X)\n\npve = pca3.explained_variance_ratio_\n\nAnd I compute the cumulative sum.\n\nsumpve = np.cumsum(pve)\nsumpve\n\narray([0.1344936 , 0.2087489 , 0.2799862 , 0.31217173, 0.33760956,\n       0.3583333 , 0.377379  , 0.3941875 , 0.40971527, 0.4247254 ,\n       0.43830603, 0.4505505 , 0.46202305, 0.4730017 , 0.48389196,\n       0.4941155 , 0.50366163, 0.51275325, 0.5213581 , 0.5296294 ,\n       0.53732485, 0.5449765 , 0.55249184, 0.5598207 , 0.5669916 ,\n       0.5740723 , 0.580814  , 0.58740735, 0.59383476, 0.600106  ,\n       0.6061526 , 0.6121337 , 0.61808836, 0.62399113, 0.62979263,\n       0.6354872 , 0.6409344 , 0.64629495, 0.65137005, 0.6563524 ,\n       0.6611421 , 0.665837  , 0.67046773, 0.6750259 , 0.67948896,\n       0.68384   , 0.68816215, 0.69243425, 0.6966555 , 0.70074064,\n       0.7048151 , 0.7088197 , 0.7126935 , 0.7165307 , 0.7203316 ,\n       0.7240459 , 0.72764814, 0.73117197, 0.7346603 , 0.738116  ,\n       0.7415551 , 0.7449048 , 0.74818265, 0.7514316 , 0.75464255,\n       0.7578165 , 0.76097137, 0.7641028 , 0.7670954 , 0.770061  ,\n       0.7730154 , 0.77593666, 0.7788439 , 0.78169763, 0.78454155,\n       0.78733283, 0.79009765, 0.79281014, 0.7954061 , 0.7979701 ,\n       0.8005114 , 0.8030465 , 0.805536  , 0.80797756, 0.8103965 ,\n       0.8127681 , 0.81513125, 0.8174775 , 0.8197751 , 0.8220363 ,\n       0.8242911 , 0.82650936, 0.8286814 , 0.8308453 , 0.83299565,\n       0.835124  , 0.8372309 , 0.8393252 , 0.84137243, 0.84337884,\n       0.8453535 , 0.84732413, 0.8492778 , 0.8512061 , 0.85310227,\n       0.85497504, 0.85683876, 0.85869616, 0.8604995 , 0.8622945 ,\n       0.8640699 , 0.86583835, 0.8675804 , 0.8693112 , 0.8710044 ,\n       0.8726823 , 0.8743508 , 0.8759999 , 0.87763053, 0.87923443,\n       0.8808169 , 0.882386  , 0.88394123, 0.8854769 , 0.8870015 ,\n       0.8884885 , 0.8899591 , 0.8914156 , 0.8928605 , 0.89429235,\n       0.89570874, 0.8971153 , 0.8985121 , 0.8999    , 0.9012585 ,\n       0.90258634, 0.90389186, 0.9051934 , 0.9064794 , 0.90774477,\n       0.9090051 , 0.9102497 , 0.911492  , 0.91272146, 0.9139403 ,\n       0.91514087, 0.9163284 , 0.91749656, 0.9186646 , 0.91982037,\n       0.92095804, 0.92208314, 0.92319673, 0.92429626, 0.9253844 ,\n       0.92646194, 0.92752826, 0.9285914 , 0.92964846, 0.9306856 ,\n       0.93171144, 0.93272966, 0.9337378 , 0.9347365 , 0.9357302 ,\n       0.936715  , 0.9376876 , 0.9386499 , 0.9395963 , 0.9405348 ,\n       0.94146496, 0.94238806, 0.9433005 , 0.94420344, 0.9451    ,\n       0.94598716, 0.946868  , 0.9477421 , 0.94860727, 0.94945776,\n       0.9502957 , 0.9511272 , 0.951952  , 0.95276606, 0.9535655 ,\n       0.95436054, 0.9551487 , 0.9559321 , 0.95670617, 0.95747715,\n       0.9582351 , 0.9589877 , 0.9597309 , 0.9604704 , 0.961201  ,\n       0.96192163, 0.9626343 , 0.96333694, 0.9640375 , 0.9647304 ,\n       0.9654158 , 0.96609175, 0.96676433, 0.96743315, 0.968094  ,\n       0.9687449 , 0.96938056, 0.97001123, 0.9706379 , 0.97126275,\n       0.97188425, 0.9724932 , 0.9730992 , 0.9736906 , 0.97427183,\n       0.97484964, 0.9754159 , 0.97597986, 0.9765347 , 0.97708565,\n       0.9776333 , 0.97817403, 0.97870815, 0.97923785, 0.979764  ,\n       0.98028374, 0.98080057, 0.9813143 , 0.98181945, 0.982322  ,\n       0.98280835, 0.9832896 , 0.98376817, 0.9842444 , 0.98471665,\n       0.9851765 , 0.98563117, 0.9860849 , 0.9865305 , 0.98697007,\n       0.9874059 , 0.9878333 , 0.9882546 , 0.9886743 , 0.98909026,\n       0.98949885, 0.98989886, 0.99029535, 0.9906846 , 0.991068  ,\n       0.9914451 , 0.99181914, 0.9921912 , 0.9925563 , 0.9929159 ,\n       0.993274  , 0.99362445, 0.9939673 , 0.9943079 , 0.99464387,\n       0.99497235, 0.9952983 , 0.99561816, 0.9959284 , 0.99623525,\n       0.99654055, 0.9968375 , 0.99713254, 0.9974232 , 0.9977091 ,\n       0.9979863 , 0.998261  , 0.9985314 , 0.99879605, 0.99905246,\n       0.9992982 , 0.99954075, 0.9997777 , 1.0000001 , 1.0000001 ],\n      dtype=float32)\n\n\nI compute how many principal components I need to explain 5%, 10%, 15%, 20%, etc. of variance.\n\nrank = []\nfor i in np.arange(0.05,1,0.05):\n    j=0\n    while sumpve[j] &lt; i:\n        j = 1 + j\n    rank.append(j)\nrank\n\n[0, 0, 1, 1, 2, 3, 5, 8, 11, 16, 22, 29, 38, 49, 63, 80, 103, 134, 180]\n\n\nTherefore, we need: - 1 component to get 10% of the variance - 2 components to get 20% - 4 components to get 30% - 9 components to get 40% - 17 components to get 50% - 30 components to get 60% - 50 components to get 70% - 81 components to get 80% - 135 components to get 90%\nWe can verify that on the folowwing scree plot:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,10))\n\nxcor = np.arange(0, 250,10)\nplt.plot(xcor, pve[xcor])\nplt.xticks(xcor)\n\npve_cumsum = np.cumsum(pve)\nplt.step(\n    xcor+0.5, \n    sumpve[xcor], \n    label='cumulative'\n    ,color='red'\n)\n\nplt.xlabel('principal component')\nplt.ylabel('proportion of variance explained')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nVisualize the features on a 2d-plot with PCA using the first two components. What can you say about the results?\n\nI apply the transformer with only 2 components this time.\n\npca2 = PCA(n_components=2)\n\nX_2d = pca2.fit_transform(X)\n\nAnd I plot the two components and use a different color for each of the 6 categories in order to see if we can already distinguish clusters.\n\nfig = plt.figure()\nfor kind in [0,1, 2, 3,4,5]:\n    idx = (Y == kind)\n\n    plt.scatter(\n        X_2d[idx, 0], X_2d[idx, 1],\n        label= names[kind]\n    )\n\nplt.legend(loc='best')\nplt.xlabel('1st component')\nplt.ylabel('2nd component')\nplt.show()\n\n\n\n\n\n\n\n\nWe can already see some clear clusters with only the fist two components, like the bike and motorcycle clusters.\nSource: 2. Data exploration\n\n3. Visual search with k-NN\n\nFit and tune a k-NN classifier\n\nAgain, I load the train set, validation set and test set.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\nI fit the KNClassifier. I test for n=5.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nknn_pipe = Pipeline([\n    \n    ('knn', KNeighborsClassifier(n_neighbors=5))\n])\n\nknn_pipe.fit(X_tr, Y_tr)\n\naccuracy = knn_pipe.score(X_val, Y_val)\nprint('Accuracy: {:.3f}'.format(accuracy))\n\nAccuracy: 0.892\n\n\nI generate the different k values in order to tune the model.\n\nk_values = np.r_[1, np.arange(5, 101, step=1)]\n\nI apply the grid search to find the optimal k.\n\nimport pandas as pd\n\ngs_results = []\n\nfor k in k_values:\n    knn_pipe.set_params(knn__n_neighbors=k)\n    knn_pipe.fit(X_tr, Y_tr)\n\n    gs_results.append({\n        'k': k,\n        'train_accuracy': knn_pipe.score(X_tr, Y_tr),\n        'valid_accuracy': knn_pipe.score(X_val, Y_val)\n    })\n\ngs_results = pd.DataFrame(gs_results)\ngs_results.sort_values(by='valid_accuracy', ascending=False).head()\n\n\n\n\n\n\n\n\nk\ntrain_accuracy\nvalid_accuracy\n\n\n\n\n2\n6\n0.892857\n0.899281\n\n\n0\n1\n1.000000\n0.892086\n\n\n3\n7\n0.907143\n0.892086\n\n\n4\n8\n0.889286\n0.892086\n\n\n5\n9\n0.900000\n0.892086\n\n\n\n\n\n\n\nThe best valid accuracy is obtain with k=6. I plot the curves.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.plot(gs_results['k'], gs_results['train_accuracy'], label='train curve')\nplt.plot(gs_results['k'], gs_results['valid_accuracy'], label='valid curve')\nplt.ylabel('accuracy')\nplt.xlabel('k')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nI can compute the associated test score:\n\nknn_pipe = Pipeline([\n    ('knn', KNeighborsClassifier(n_neighbors=6))\n])\n\nknn_pipe.fit(X_tr, Y_tr)\n\naccuracy = knn_pipe.score(X_te, Y_te)\nprint('Accuracy: {:.3f}'.format(accuracy))\n\nAccuracy: 0.960\n\n\nWe can also tune our model with the other parameters such as the weights functions and the distance type. To do that, we need to apply the grid search method for multiple parameters.\n\nk_values = np.arange(1, 21) \nweights_functions = ['uniform', 'distance']\ndistance_types = [1, 2] \n\n\npipe = Pipeline([\n    ('knn', KNeighborsClassifier())\n])\n\nvalidation_scores = []\n\nfor k in k_values:\n    for f in weights_functions:\n        for d in distance_types:\n            pipe.set_params(knn__n_neighbors=k, knn__weights=f, knn__p=d)\n\n            pipe.fit(X_tr, Y_tr)\n\n            val_accuracy = pipe.score(X_val, Y_val)\n            \n            test_accuracy = pipe.score(X_te, Y_te)\n            \n            validation_scores.append({\n                'knn__n_neighbors': k,\n                'knn__weights': f,\n                'knn__p': d,\n                'valid_accuracy': val_accuracy,\n                'test_accuracy': test_accuracy\n            })\n            \n            \n\n\nscores_df = pd.DataFrame(validation_scores)\n\nscores_df.sort_values(by='valid_accuracy', ascending=False).head(5)\n\n\n\n\n\n\n\n\nknn__n_neighbors\nknn__p\nknn__weights\ntest_accuracy\nvalid_accuracy\n\n\n\n\n20\n6\n1\nuniform\n0.98\n0.920863\n\n\n28\n8\n1\nuniform\n0.96\n0.899281\n\n\n21\n6\n2\nuniform\n0.96\n0.899281\n\n\n5\n2\n2\nuniform\n0.94\n0.899281\n\n\n16\n5\n1\nuniform\n0.96\n0.899281\n\n\n\n\n\n\n\n\nResults = pd.read_csv('Results.csv')\n\n\nnew_row = {'model':'k-NN','test_accuracy':scores_df.sort_values(by='valid_accuracy', ascending=False)['test_accuracy'][20]}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\nThe best model is obtained with k=6, the uniform weight function and the first distance type. I can compute the associated test score:\n\nk=6\nweights_functions = 'uniform'\ndistance_types = 1\n\npipe.set_params(knn__n_neighbors=k, knn__weights=weights_functions, knn__p=distance_types)\n\ntest_score = pipe.score(X_te, Y_te)\nprint('Test score:',test_score)\n\nTest score: 0.96\n\n\n\nPick an image from the test set and plot its 10 nearest neighbors from the train set. Hint: take a look at the kneighbors() method from Scikit-learn k-NN estimators.\n\nI create the knn using 6 neighbors and a distance type of 1 since it gave me the best results (as we can see above).\n\nfrom sklearn.neighbors import NearestNeighbors\n\nknn = NearestNeighbors(n_neighbors=6,p=1)\nknn.fit(X_te)\nten_kneighbors = knn.kneighbors(X_te[0].reshape(1,-1),n_neighbors=10 ,return_distance=False)\n\n\nten_kneighbors\n\narray([[ 0,  4, 10,  7,  2,  1,  9,  6, 11,  8]])\n\n\nI plot the images of the 10 nearest neighbors.\n\nfrom PIL import Image\n\nfor i in range(1,ten_kneighbors.shape[1]):\n    plt.imshow(images_te[ten_kneighbors[0,i]])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, we can see that all the nearest neighbors are also bycicles (since the first image was also a bike).\nSource: 3. Visual search with k-NN\n\n4. Simple decision trees\n\nWhat accuracy can you achieve with a depth of 3?\n\nI first load the sets.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\nI create the tree.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(\n    criterion='gini', max_depth=3, random_state=0)\n\nI fit the tree with the train set and get the score of the test set.\n\ndt.fit(X_tr, Y_tr)\n\ndt.score(X_te,Y_te)\n\n0.66\n\n\n\nPlot the corresponding tree with graphviz\n\nI create the tree with the data from above.\n\nfrom sklearn.tree import export_graphviz\n\ndot_data = export_graphviz(\n    dt, out_file=None,\n    class_names=names_te,\n    filled=True, rounded=True, proportion=True   \n)\n\nI plot the tree.\n\nimport graphviz\n\ngraphviz.Source(dot_data)\n\n\n\n\n\n\n\n\n\nDo you get better results if you reduce the number of dimensions with PCA first?\n\nI use the same code as the second task of the project.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\nX_2d_tr = pca.fit_transform(X_tr)\nX_2d_te = pca.transform(X_te)\n\nWe can verify the shapes of the data.\n\nprint(X_2d_tr.shape)\nprint(Y_tr.shape)\nprint(X_2d_te.shape)\nprint(Y_te.shape)\n\n(280, 2)\n(280,)\n(50, 2)\n(50,)\n\n\nWe recreate the same decision tree to avoid clutter.\n\ndt = DecisionTreeClassifier(\n    criterion='gini', max_depth=3, random_state=0)\n\nWe fit the tree with the training set and get the score with the test set.\n\ndt.fit(X_2d_tr, Y_tr)\n\ndt.score(X_2d_te,Y_te)\n\n0.74\n\n\nIndeed, we obtain a better score with the PCA (0.74&gt;0.66).\nI upload the results in the csv file.\n\nimport pandas as pd\n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'decision tree','test_accuracy':dt.score(X_2d_te,Y_te)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\nWe could try to see what is the best PCA number.\n\nimport pandas as pd \n\nscores = pd.DataFrame(np.zeros((10, 2)),columns=['n','Test_Score'])\n\nfor i in range(2,11):\n    pca = PCA(n_components=i)\n\n    X_2d_tr = pca.fit_transform(X_tr)\n    X_2d_te = pca.transform(X_te)\n    \n    dt.fit(X_2d_tr, Y_tr)\n\n    dt.score(X_2d_te,Y_te)\n    \n    scores.n[i]=i\n    scores.Test_Score[i]=dt.score(X_2d_te,Y_te)\n\nscores = scores.iloc[2:]\n\n\nscores.head(5)\n\n\n\n\n\n\n\n\nn\nTest_Score\n\n\n\n\n2\n2.0\n0.74\n\n\n3\n3.0\n0.74\n\n\n4\n4.0\n0.74\n\n\n5\n5.0\n0.74\n\n\n6\n6.0\n0.74\n\n\n\n\n\n\n\nThe best score is obtained when n&gt;2.\nSource: 4. Simple decision trees\n\n5. Logistic regression\n\nEvaluate a logistic regression model (without any hyperparameters tuning)\n\nI first load the data sets.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\nI apply the softmax regression method.\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.simplefilter('ignore', ConvergenceWarning)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\nlogreg = make_pipeline( LogisticRegression(multi_class='multinomial', solver='saga'))\n\nlogreg.fit(X_tr, Y_tr)\n\naccuracy = logreg.score(X_val, Y_val)\nprint('Validation accuracy: {:.3f}'.format(accuracy))\n\naccuracy = logreg.score(X_te, Y_te)\nprint('Test accuracy: {:.3f}'.format(accuracy))\n\nValidation accuracy: 0.914\nTest accuracy: 0.940\n\n\n\nTune its regularization strength parameter with cross-validated grid-search and compare the accuracy to the untuned one\n\nThe cross-validated grid-search function that we have seen during the course (cross_validate()) takes the dataset and splits automatically into a train and a validation set. However, the validation set is already given separately for this project. Therefore I decide to merge the our train and validation set and use this as my basic dataset. Since the validation set is random data from the whole set, it should not be a problem to do that.\n\nprint(X_tr.shape)\nprint(X_val.shape)\nprint(Y_tr.shape)\nprint(Y_val.shape)\n\n(280, 1280)\n(139, 1280)\n(280,)\n(139,)\n\n\n\nX = np.concatenate((X_tr,X_val),axis=0)\nY = np.concatenate((Y_tr,Y_val),axis=0)\nprint(X.shape)\nprint(Y.shape)\n\n(419, 1280)\n(419,)\n\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nlogreg_estimator = Pipeline([\n    ('logreg', LogisticRegression())\n])\n\nCs = np.logspace(-3, 4, num=20)\ngrids = [{\n    'logreg__multi_class': ['ovr'],\n    'logreg__solver': ['liblinear'],\n    'logreg__C': Cs\n}, {\n    'logreg__multi_class': ['multinomial'],\n    'logreg__solver': ['saga'],\n    'logreg__C': Cs\n}]\nlogreg_gscv = GridSearchCV(logreg_estimator, grids, cv=5, refit=True, return_train_score=True,verbose=1,n_jobs=-1)\n\n\nimport pandas as pd\n\nlogreg_gscv.fit(X, Y)\n\nlogreg_results = pd.DataFrame({\n    'strategy': logreg_gscv.cv_results_['param_logreg__multi_class'],\n    'C': logreg_gscv.cv_results_['param_logreg__C'],\n    'mean_tr': logreg_gscv.cv_results_['mean_train_score'],\n    'mean_te': logreg_gscv.cv_results_['mean_test_score'],\n    'std_te': logreg_gscv.cv_results_['std_test_score']\n})\n\nlogreg_results.sort_values(by='mean_te', ascending=False).head(10)\n\nFitting 5 folds for each of 40 candidates, totalling 200 fits\n\n\n\n\n\n\n\n\n\nstrategy\nC\nmean_tr\nmean_te\nstd_te\n\n\n\n\n33\nmultinomial\n61.5848\n1.0\n0.914081\n0.017486\n\n\n38\nmultinomial\n4281.33\n1.0\n0.914081\n0.014418\n\n\n37\nmultinomial\n1832.98\n1.0\n0.911695\n0.015967\n\n\n30\nmultinomial\n4.83293\n1.0\n0.911695\n0.015761\n\n\n35\nmultinomial\n335.982\n1.0\n0.909308\n0.012290\n\n\n34\nmultinomial\n143.845\n1.0\n0.909308\n0.012290\n\n\n32\nmultinomial\n26.3665\n1.0\n0.909308\n0.019254\n\n\n31\nmultinomial\n11.2884\n1.0\n0.909308\n0.012439\n\n\n29\nmultinomial\n2.06914\n1.0\n0.909308\n0.015569\n\n\n25\nmultinomial\n0.0695193\n1.0\n0.909308\n0.016423\n\n\n\n\n\n\n\n\nlogreg = make_pipeline( LogisticRegression(multi_class='multinomial', solver='saga',C=0.0297635))\n\nlogreg.fit(X, Y)\n\naccuracy = logreg.score(X_te, Y_te)\nprint('Accuracy: {:.3f}'.format(accuracy))\n\nAccuracy: 0.920\n\n\nRegularization does not seem to enhance the accuracy of the model.\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'logistic','test_accuracy':logreg.score(X_te, Y_te)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\ntableau = logreg_results.sort_values(by='C', ascending=False)\n \nplt.semilogx(tableau[tableau['strategy']== 'ovr'].sort_values(by='C', ascending=False)['C'], tableau[tableau['strategy']== 'ovr'].sort_values(by='C', ascending=False)['mean_te'])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nI plot with the other strategy.\n\nplt.semilogx(tableau[tableau['strategy']== 'multinomial'].sort_values(by='C', ascending=False)['C'], tableau[tableau['strategy']== 'multinomial'].sort_values(by='C', ascending=False)['mean_te'])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nPick ten images and compute the probability for each class using the predict_proba() function of your estimator. Plot the results for each image and comment your findings. For example, you can plot the ten images in subplots and collect the probabilities in a DataFrame.\n\nI select 10 random images from the test set and compute the associated probabilities with my best estimator.\n\nfrom random import randrange\nfrom random import seed\n\nseed(0)\ni=0\nfor i in range(1,10):\n    a=randrange(0,len(X_te))\n    plt.imshow(images_te[a])\n    plt.show()\n    print(names_te)\n    print(logreg.predict_proba(X_te[a].reshape(1,-1)))\n    i=1+i\n\n\n\n\n\n\n\n\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[1.67733772e-03 1.31682584e-03 9.84926992e-01 9.41871430e-03\n  2.02255744e-03 6.37572917e-04]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[0.00848476 0.0425322  0.00605633 0.23887549 0.08617596 0.61787526]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[1.52949697e-02 4.86864710e-03 9.63393794e-01 8.74335736e-03\n  6.78372346e-03 9.15508023e-04]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[9.97388287e-01 2.94495481e-04 1.34330750e-03 7.41756132e-04\n  1.00760168e-04 1.31393680e-04]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[1.01474072e-03 9.88172314e-01 8.63270453e-04 1.23008703e-03\n  1.37771077e-03 7.34187679e-03]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[0.00648867 0.00426102 0.00696245 0.94648625 0.03263764 0.00316397]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[1.90600581e-03 1.78061064e-03 9.90901807e-01 3.62147450e-03\n  1.20981638e-03 5.80285434e-04]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[0.00873421 0.02237223 0.86490402 0.0933992  0.00926808 0.00132226]]\n['bike' 'car' 'motorcycle' 'other' 'truck' 'van']\n[[7.01139866e-04 9.91077878e-01 7.74640152e-04 1.09207282e-03\n  5.86732763e-04 5.76753684e-03]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the model always find the good image (in terms of biggest probability).\n\nFinally, vary the regularization strength of your estimator. What is the effect on the probabilities? Write your observations in a markdown cell.\n\nI take one image from each category and vary the regularization term C to see the effect on the probabilities.\n\ncol_names = ['C','bike', 'car', 'motorcycle', 'other', 'truck', 'van']\n\n\nrange_C = np.logspace(-8,4,num=10)\n\n\nBike\n\nbike_table = pd.DataFrame(columns = col_names)\n\n\nbike_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[0]);\n\n\n\n\n\n\n\n\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline( LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    bike_table.loc[j,'bike'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][0]\n    bike_table.loc[j,'car'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][1]\n    bike_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][2]\n    bike_table.loc[j,'other'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][3]\n    bike_table.loc[j,'truck'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][4]\n    bike_table.loc[j,'van'] = logreg.predict_proba(X_te[0].reshape(1,-1))[0][5]\n\nbike_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.16671\n0.166696\n0.166673\n0.166641\n0.166654\n0.166626\n\n\n1\n2.154435e-07\n0.173206\n0.172175\n0.167969\n0.161903\n0.165045\n0.159703\n\n\n2\n4.641589e-06\n0.228178\n0.212173\n0.175621\n0.125435\n0.149497\n0.109096\n\n\n3\n1.000000e-04\n0.364277\n0.168192\n0.161751\n0.102864\n0.123202\n0.0797131\n\n\n4\n2.154435e-03\n0.857242\n0.0251372\n0.0371105\n0.0373727\n0.0229972\n0.0201403\n\n\n5\n4.641589e-02\n0.991742\n0.00124352\n0.00173849\n0.00343773\n0.000949455\n0.000889036\n\n\n6\n1.000000e+00\n0.999745\n1.57503e-05\n4.38189e-05\n0.000172652\n1.23125e-05\n1.06864e-05\n\n\n7\n2.154435e+01\n0.999904\n6.77624e-06\n1.59024e-05\n6.75801e-05\n2.50978e-06\n2.93493e-06\n\n\n8\n4.641589e+02\n0.999959\n1.28155e-06\n6.5071e-06\n3.11746e-05\n1.11263e-06\n1.05689e-06\n\n\n9\n1.000000e+04\n0.999894\n6.25005e-06\n1.40269e-05\n7.96064e-05\n3.13887e-06\n3.23863e-06\n\n\n\n\n\n\n\n\n\nCar\n\ncar_table = pd.DataFrame(columns = col_names)\n\n\ncar_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[13]);\n\n\n\n\n\n\n\n\n\nj=0\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline(LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    car_table.loc[j,'bike'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][0]\n    car_table.loc[j,'car'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][1]\n    car_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][2]\n    car_table.loc[j,'other'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][3]\n    car_table.loc[j,'truck'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][4]\n    car_table.loc[j,'van'] = logreg.predict_proba(X_te[13].reshape(1,-1))[0][5]\n\ncar_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.166687\n0.1667\n0.166671\n0.166645\n0.166658\n0.166638\n\n\n1\n2.154435e-07\n0.172878\n0.172528\n0.167943\n0.161914\n0.16502\n0.159716\n\n\n2\n4.641589e-06\n0.218768\n0.221134\n0.174619\n0.125522\n0.150345\n0.109612\n\n\n3\n1.000000e-04\n0.172705\n0.348096\n0.151556\n0.105887\n0.130944\n0.0908117\n\n\n4\n2.154435e-03\n0.0179324\n0.887413\n0.0205776\n0.0245222\n0.0187384\n0.0308163\n\n\n5\n4.641589e-02\n0.000563818\n0.995474\n0.000656942\n0.000755771\n0.000515995\n0.00203337\n\n\n6\n1.000000e+00\n1.07619e-05\n0.999865\n9.81102e-06\n1.25539e-05\n7.19507e-06\n9.44414e-05\n\n\n7\n2.154435e+01\n2.19065e-06\n0.999953\n2.87351e-06\n3.45583e-06\n2.1278e-06\n3.68259e-05\n\n\n8\n4.641589e+02\n4.39436e-06\n0.999927\n3.90908e-06\n5.43666e-06\n2.54504e-06\n5.69631e-05\n\n\n9\n1.000000e+04\n3.06237e-06\n0.999949\n4.51504e-06\n3.74466e-06\n2.29489e-06\n3.73611e-05\n\n\n\n\n\n\n\n\n\nMotorcycle\n\nmotorcycle_table = pd.DataFrame(columns = col_names)\n\n\nmotorcycle_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[26])\n\n\n\n\n\n\n\n\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline( LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    motorcycle_table.loc[j,'bike'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][0]\n    motorcycle_table.loc[j,'car'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][1]\n    motorcycle_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][2]\n    motorcycle_table.loc[j,'other'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][3]\n    motorcycle_table.loc[j,'truck'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][4]\n    motorcycle_table.loc[j,'van'] = logreg.predict_proba(X_te[26].reshape(1,-1))[0][5]\n\nmotorcycle_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.166692\n0.166692\n0.16668\n0.166646\n0.166658\n0.166632\n\n\n1\n2.154435e-07\n0.172987\n0.172233\n0.168144\n0.16189\n0.165048\n0.159698\n\n\n2\n4.641589e-06\n0.222368\n0.213497\n0.178799\n0.125745\n0.150598\n0.108993\n\n\n3\n1.000000e-04\n0.233316\n0.189038\n0.245599\n0.112467\n0.136367\n0.083213\n\n\n4\n2.154435e-03\n0.106879\n0.0509277\n0.697114\n0.0673747\n0.0530585\n0.0246463\n\n\n5\n4.641589e-02\n0.0131358\n0.00412597\n0.96856\n0.00624701\n0.00707047\n0.000860373\n\n\n6\n1.000000e+00\n0.00108783\n0.000183078\n0.997918\n0.000314871\n0.000485094\n1.074e-05\n\n\n7\n2.154435e+01\n0.000615538\n5.65981e-05\n0.998906\n0.000159\n0.000259885\n2.72862e-06\n\n\n8\n4.641589e+02\n0.000515385\n7.49919e-05\n0.998964\n0.000174475\n0.000268118\n3.39689e-06\n\n\n9\n1.000000e+04\n0.000619043\n6.61948e-05\n0.998876\n0.000164927\n0.000270052\n4.11387e-06\n\n\n\n\n\n\n\n\n\nOther\n\nother_table = pd.DataFrame(columns = col_names)\n\n\nother_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[33])\n\n\n\n\n\n\n\n\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline( LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    other_table.loc[j,'bike'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][0]\n    other_table.loc[j,'car'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][1]\n    other_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][2]\n    other_table.loc[j,'other'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][3]\n    other_table.loc[j,'truck'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][4]\n    other_table.loc[j,'van'] = logreg.predict_proba(X_te[33].reshape(1,-1))[0][5]\n\nother_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.166693\n0.16669\n0.166673\n0.166647\n0.166661\n0.166636\n\n\n1\n2.154435e-07\n0.172891\n0.172302\n0.168024\n0.161979\n0.165082\n0.159722\n\n\n2\n4.641589e-06\n0.22049\n0.214569\n0.177028\n0.126408\n0.15203\n0.109475\n\n\n3\n1.000000e-04\n0.205163\n0.205092\n0.196376\n0.133721\n0.167385\n0.0922635\n\n\n4\n2.154435e-03\n0.0795713\n0.0746339\n0.173708\n0.385638\n0.229876\n0.0565718\n\n\n5\n4.641589e-02\n0.0187512\n0.0109184\n0.0710302\n0.672542\n0.218377\n0.00838142\n\n\n6\n1.000000e+00\n0.00485905\n0.00131337\n0.0266977\n0.836323\n0.129995\n0.000811631\n\n\n7\n2.154435e+01\n0.00195899\n0.000392279\n0.0188389\n0.867008\n0.111527\n0.000274337\n\n\n8\n4.641589e+02\n0.00265511\n0.000503708\n0.0206401\n0.874637\n0.101173\n0.000391244\n\n\n9\n1.000000e+04\n0.0022698\n0.000551082\n0.0158346\n0.835776\n0.145239\n0.000329257\n\n\n\n\n\n\n\n\n\nTruck\n\ntruck_table = pd.DataFrame(columns = col_names)\n\n\ntruck_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[43])\n\n\n\n\n\n\n\n\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline(LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    truck_table.loc[j,'bike'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][0]\n    truck_table.loc[j,'car'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][1]\n    truck_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][2]\n    truck_table.loc[j,'other'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][3]\n    truck_table.loc[j,'truck'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][4]\n    truck_table.loc[j,'van'] = logreg.predict_proba(X_te[43].reshape(1,-1))[0][5]\n\ntruck_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.166694\n0.166696\n0.166672\n0.166645\n0.166663\n0.16663\n\n\n1\n2.154435e-07\n0.172938\n0.172331\n0.167922\n0.161964\n0.165103\n0.159743\n\n\n2\n4.641589e-06\n0.219933\n0.214907\n0.175328\n0.126725\n0.153051\n0.110056\n\n\n3\n1.000000e-04\n0.200652\n0.211369\n0.168458\n0.132846\n0.189481\n0.0971939\n\n\n4\n2.154435e-03\n0.056008\n0.0745657\n0.0542732\n0.258366\n0.4876\n0.0691864\n\n\n5\n4.641589e-02\n0.00534244\n0.0102937\n0.0041802\n0.134445\n0.840167\n0.00557214\n\n\n6\n1.000000e+00\n0.000378198\n0.000757682\n0.000197872\n0.0389118\n0.959517\n0.000237887\n\n\n7\n2.154435e+01\n0.000166384\n0.000405344\n9.6242e-05\n0.0312516\n0.967962\n0.000118601\n\n\n8\n4.641589e+02\n0.000132615\n0.000279562\n5.80725e-05\n0.0268658\n0.972578\n8.63034e-05\n\n\n9\n1.000000e+04\n0.000108849\n0.000240686\n5.1481e-05\n0.0228083\n0.976721\n6.97044e-05\n\n\n\n\n\n\n\n\n\nVan\n\nvan_table = pd.DataFrame(columns = col_names)\n\n\nvan_table['C'] = np.logspace(-8,4,num=10)\n\n\nplt.imshow(images_te[49])\n\n\n\n\n\n\n\n\n\nfor j in range(0,len(range_C)) :\n    logreg = make_pipeline(LogisticRegression(C=range_C[j], multi_class='multinomial', solver='saga'))\n    logreg.fit(X_tr, Y_tr)\n    van_table.loc[j,'bike'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][0]\n    van_table.loc[j,'car'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][1]\n    van_table.loc[j,'motorcycle'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][2]\n    van_table.loc[j,'other'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][3]\n    van_table.loc[j,'truck'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][4]\n    van_table.loc[j,'van'] = logreg.predict_proba(X_te[49].reshape(1,-1))[0][5]\n\nvan_table\n\n\n\n\n\n\n\n\nC\nbike\ncar\nmotorcycle\nother\ntruck\nvan\n\n\n\n\n0\n1.000000e-08\n0.166689\n0.166687\n0.166669\n0.166646\n0.166665\n0.166643\n\n\n1\n2.154435e-07\n0.172933\n0.172294\n0.167892\n0.161917\n0.165156\n0.159808\n\n\n2\n4.641589e-06\n0.219668\n0.215655\n0.175138\n0.126351\n0.152427\n0.110762\n\n\n3\n1.000000e-04\n0.197421\n0.22334\n0.162916\n0.120849\n0.181751\n0.113723\n\n\n4\n2.154435e-03\n0.0603387\n0.0891248\n0.054002\n0.103107\n0.265111\n0.428316\n\n\n5\n4.641589e-02\n0.00871392\n0.0115041\n0.00540101\n0.023631\n0.104446\n0.846304\n\n\n6\n1.000000e+00\n0.000890357\n0.00103803\n0.000308159\n0.003582\n0.027256\n0.966925\n\n\n7\n2.154435e+01\n0.00056502\n0.000575758\n8.43756e-05\n0.00218135\n0.0210307\n0.975563\n\n\n8\n4.641589e+02\n0.000535682\n0.000454719\n0.000156076\n0.00228876\n0.0219212\n0.974644\n\n\n9\n1.000000e+04\n0.000320905\n0.000460301\n9.14838e-05\n0.00193528\n0.0160996\n0.981092\n\n\n\n\n\n\n\nWe can see that when C is very low (i.e. strong regularization), the probabilities tend to 0.166 (or 1/6), that is uniform distribution. The model becomes useless. However, when we increase C, the model predicts better the different categories.\n\nSource: 5. Logistic regression\n\n6. Nonlinear classifiers\n\nTry with a random Forest, does increasing the number of trees help?\n\nFirst, I load the data sets.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\nI create deep decision trees and vary the number of trees.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ndt_max_depth = RandomForestClassifier(\n    n_estimators=1, max_depth=None, random_state=0)\n\ndt_max_depth.fit(X_tr, Y_tr)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\n\n\n\nrf_100 = RandomForestClassifier(\n    n_estimators=100, max_depth=None, random_state=0)\n\nrf_100.fit(X_tr, Y_tr)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\n\n\n\nrf_1000 = RandomForestClassifier(\n    n_estimators=1000, max_depth=None, random_state=0)\n\nrf_1000.fit(X_tr, Y_tr)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\n\n\n\nfrom sklearn.model_selection import cross_validate\n\ndt_scores = cross_validate(dt_max_depth, X_tr, Y_tr, cv=10)\nprint('Decision tree - mean test {:.3f}'.format(\n    np.mean(dt_scores['test_score'])))\n\nrf_scores = cross_validate(rf_100, X_tr, Y_tr, cv=10)\nprint('Random forest - mean test {:.3f}'.format(\n    np.mean(rf_scores['test_score'])))\n\nrf_scores = cross_validate(rf_1000, X_tr, Y_tr, cv=10)\nprint('Large Random forest - mean test {:.3f}'.format(\n    np.mean(rf_scores['test_score'])))\n\nDecision tree - mean test 0.563\nRandom forest - mean test 0.861\nLarge Random forest - mean test 0.900\n\n\n\nimport pandas as pd\n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'random forest','test_accuracy':round(np.mean(rf_scores['test_score']),2)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\n\nrange_i = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\nscores_valid = []\nscores_test = []\n\nfor i in range_i:\n    rfc = RandomForestClassifier(n_estimators=i, max_depth=None, random_state=0)\n    rfc.fit(X_tr,Y_tr)\n    scores_valid.append(rfc.score(X_val,Y_val))\n    scores_test.append(rfc.score(X_te,Y_te))\n    \n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.plot(range_i, scores_test, label='Test score')\nplt.plot(range_i, scores_valid, label='Validation score')\nplt.legend()\nplt.xlabel('Number of trees')\nplt.ylabel('Mean test')\nplt.title('Performance depending of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the performance increases with the number of trees. From 20 trees, it seems to stabilize a bit.\n\nTry with SVMs - does the RBF kernel perform better than the linear one?\n\nI first try the linear SVM:\n\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\n\nlinear_svc.fit(X_tr, Y_tr)\n\nprint('SVC - val score {:.3f}'.format(linear_svc.score(X_val,Y_val)))\nprint('SVC - test score {:.3f}'.format(linear_svc.score(X_te,Y_te)))\n\nSVC - val score 0.906\nSVC - test score 0.920\n\n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'svm linear','test_accuracy':linear_svc.score(X_te,Y_te)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\nAnd then the RBF kernel:\n\nfrom sklearn.svm import SVC\n\nrbf_svc_c1 = SVC(kernel='rbf', C=1,gamma='auto')\n\nrbf_svc_c1.fit(X_tr, Y_tr)\n\nprint('SVC - val score {:.3f}'.format(rbf_svc_c1.score(X_val,Y_val)))\nprint('SVC - test score {:.3f}'.format(rbf_svc_c1.score(X_te,Y_te)))\n\nSVC - val score 0.921\nSVC - test score 0.960\n\n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'svm rbf','test_accuracy':rbf_svc_c1.score(X_te,Y_te)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\nThe performance increases with the non linear classifier SVM.\nI can try to vary the C value:\n\nrange_i = np.logspace(-8, 1, num=10)\nscores_valid = []\nscores_test = []\n\nfor i in range_i:\n    rbf_svc_c1 = SVC(kernel='rbf', C=i ,gamma='auto')\n    rbf_svc_c1.fit(X_tr, Y_tr)\n    scores_valid.append(rbf_svc_c1.score(X_val,Y_val))\n    scores_test.append(rbf_svc_c1.score(X_te,Y_te))\n    \n\n\nplt.plot(range_i, scores_test,label='Test score')\nplt.plot(range_i, scores_valid,label='Validation score')\nplt.legend()\nplt.xlabel('C')\nplt.ylabel('Mean test')\nplt.title('Performance depending of C')\nplt.show()\n\n\n\n\n\n\n\n\nSource: 6. Nonlinear classifiers\n\n7. Dense networks\n\n1-layer dense network i.e. no hidden layer, just the input and output ones\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nI create the model with 1 layer.\n\nmodel = Sequential()\nmodel.add(Dense(6, activation='softmax', input_dim=1280))\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 6)                 7686      \n=================================================================\nTotal params: 7,686\nTrainable params: 7,686\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom tensorflow.keras import optimizers\n\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['acc'])\n\nI can now load the data sets.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\n\nhistory = model.fit(\n    x=X_tr, y=Y_tr,\n    validation_data=(X_val,Y_val), batch_size=32, epochs=50, shuffle=True\n)\n\nTrain on 280 samples, validate on 139 samples\nEpoch 1/50\n280/280 [==============================] - 1s 2ms/step - loss: 1.4758 - acc: 0.4643 - val_loss: 1.0158 - val_acc: 0.7050\nEpoch 2/50\n280/280 [==============================] - 0s 67us/step - loss: 0.8911 - acc: 0.7393 - val_loss: 0.7403 - val_acc: 0.8058\nEpoch 3/50\n280/280 [==============================] - 0s 62us/step - loss: 0.6684 - acc: 0.8107 - val_loss: 0.6159 - val_acc: 0.8201\nEpoch 4/50\n280/280 [==============================] - 0s 66us/step - loss: 0.5402 - acc: 0.8714 - val_loss: 0.5278 - val_acc: 0.8561\nEpoch 5/50\n280/280 [==============================] - 0s 67us/step - loss: 0.4592 - acc: 0.9107 - val_loss: 0.4822 - val_acc: 0.8633\nEpoch 6/50\n280/280 [==============================] - 0s 71us/step - loss: 0.4017 - acc: 0.9107 - val_loss: 0.4404 - val_acc: 0.8777\nEpoch 7/50\n280/280 [==============================] - 0s 63us/step - loss: 0.3579 - acc: 0.9250 - val_loss: 0.4074 - val_acc: 0.8921\nEpoch 8/50\n280/280 [==============================] - 0s 62us/step - loss: 0.3232 - acc: 0.9286 - val_loss: 0.3888 - val_acc: 0.9137\nEpoch 9/50\n280/280 [==============================] - 0s 65us/step - loss: 0.2978 - acc: 0.9393 - val_loss: 0.3655 - val_acc: 0.8993\nEpoch 10/50\n280/280 [==============================] - 0s 62us/step - loss: 0.2755 - acc: 0.9464 - val_loss: 0.3547 - val_acc: 0.9065\nEpoch 11/50\n280/280 [==============================] - 0s 64us/step - loss: 0.2560 - acc: 0.9500 - val_loss: 0.3402 - val_acc: 0.9137\nEpoch 12/50\n280/280 [==============================] - 0s 63us/step - loss: 0.2405 - acc: 0.9500 - val_loss: 0.3341 - val_acc: 0.9065\nEpoch 13/50\n280/280 [==============================] - 0s 63us/step - loss: 0.2253 - acc: 0.9500 - val_loss: 0.3264 - val_acc: 0.9137\nEpoch 14/50\n280/280 [==============================] - 0s 64us/step - loss: 0.2132 - acc: 0.9536 - val_loss: 0.3154 - val_acc: 0.9065\nEpoch 15/50\n280/280 [==============================] - 0s 66us/step - loss: 0.2044 - acc: 0.9643 - val_loss: 0.3085 - val_acc: 0.9065\nEpoch 16/50\n280/280 [==============================] - 0s 115us/step - loss: 0.1929 - acc: 0.9643 - val_loss: 0.3079 - val_acc: 0.9065\nEpoch 17/50\n280/280 [==============================] - 0s 81us/step - loss: 0.1830 - acc: 0.9714 - val_loss: 0.2996 - val_acc: 0.9065\nEpoch 18/50\n280/280 [==============================] - 0s 82us/step - loss: 0.1743 - acc: 0.9679 - val_loss: 0.2954 - val_acc: 0.9065\nEpoch 19/50\n280/280 [==============================] - 0s 67us/step - loss: 0.1681 - acc: 0.9750 - val_loss: 0.2923 - val_acc: 0.9065\nEpoch 20/50\n280/280 [==============================] - 0s 96us/step - loss: 0.1612 - acc: 0.9786 - val_loss: 0.2897 - val_acc: 0.9065\nEpoch 21/50\n280/280 [==============================] - 0s 60us/step - loss: 0.1545 - acc: 0.9857 - val_loss: 0.2837 - val_acc: 0.9065\nEpoch 22/50\n280/280 [==============================] - 0s 60us/step - loss: 0.1487 - acc: 0.9857 - val_loss: 0.2784 - val_acc: 0.9065\nEpoch 23/50\n280/280 [==============================] - 0s 63us/step - loss: 0.1440 - acc: 0.9821 - val_loss: 0.2766 - val_acc: 0.9065\nEpoch 24/50\n280/280 [==============================] - 0s 63us/step - loss: 0.1388 - acc: 0.9857 - val_loss: 0.2764 - val_acc: 0.9065\nEpoch 25/50\n280/280 [==============================] - 0s 69us/step - loss: 0.1346 - acc: 0.9893 - val_loss: 0.2729 - val_acc: 0.9065\nEpoch 26/50\n280/280 [==============================] - 0s 61us/step - loss: 0.1295 - acc: 0.9893 - val_loss: 0.2716 - val_acc: 0.9065\nEpoch 27/50\n280/280 [==============================] - 0s 62us/step - loss: 0.1252 - acc: 0.9893 - val_loss: 0.2717 - val_acc: 0.9137\nEpoch 28/50\n280/280 [==============================] - 0s 64us/step - loss: 0.1216 - acc: 0.9929 - val_loss: 0.2709 - val_acc: 0.9065\nEpoch 29/50\n280/280 [==============================] - 0s 60us/step - loss: 0.1179 - acc: 0.9929 - val_loss: 0.2674 - val_acc: 0.9065\nEpoch 30/50\n280/280 [==============================] - 0s 62us/step - loss: 0.1149 - acc: 0.9929 - val_loss: 0.2679 - val_acc: 0.9065\nEpoch 31/50\n280/280 [==============================] - 0s 59us/step - loss: 0.1107 - acc: 0.9929 - val_loss: 0.2654 - val_acc: 0.9137\nEpoch 32/50\n280/280 [==============================] - 0s 60us/step - loss: 0.1082 - acc: 0.9929 - val_loss: 0.2624 - val_acc: 0.9065\nEpoch 33/50\n280/280 [==============================] - 0s 60us/step - loss: 0.1054 - acc: 0.9929 - val_loss: 0.2617 - val_acc: 0.9065\nEpoch 34/50\n280/280 [==============================] - 0s 65us/step - loss: 0.1024 - acc: 0.9929 - val_loss: 0.2609 - val_acc: 0.9065\nEpoch 35/50\n280/280 [==============================] - 0s 70us/step - loss: 0.1000 - acc: 0.9929 - val_loss: 0.2592 - val_acc: 0.9065\nEpoch 36/50\n280/280 [==============================] - 0s 64us/step - loss: 0.0977 - acc: 0.9929 - val_loss: 0.2586 - val_acc: 0.9137\nEpoch 37/50\n280/280 [==============================] - 0s 64us/step - loss: 0.0951 - acc: 0.9929 - val_loss: 0.2568 - val_acc: 0.9065\nEpoch 38/50\n280/280 [==============================] - 0s 64us/step - loss: 0.0930 - acc: 0.9929 - val_loss: 0.2566 - val_acc: 0.9065\nEpoch 39/50\n280/280 [==============================] - 0s 65us/step - loss: 0.0907 - acc: 0.9929 - val_loss: 0.2559 - val_acc: 0.9137\nEpoch 40/50\n280/280 [==============================] - 0s 62us/step - loss: 0.0885 - acc: 0.9929 - val_loss: 0.2567 - val_acc: 0.9137\nEpoch 41/50\n280/280 [==============================] - 0s 63us/step - loss: 0.0865 - acc: 0.9929 - val_loss: 0.2553 - val_acc: 0.9137\nEpoch 42/50\n280/280 [==============================] - 0s 62us/step - loss: 0.0846 - acc: 0.9929 - val_loss: 0.2545 - val_acc: 0.9137\nEpoch 43/50\n280/280 [==============================] - 0s 77us/step - loss: 0.0826 - acc: 0.9929 - val_loss: 0.2546 - val_acc: 0.9137\nEpoch 44/50\n280/280 [==============================] - 0s 70us/step - loss: 0.0811 - acc: 0.9929 - val_loss: 0.2552 - val_acc: 0.9137\nEpoch 45/50\n280/280 [==============================] - 0s 64us/step - loss: 0.0794 - acc: 0.9929 - val_loss: 0.2532 - val_acc: 0.9137\nEpoch 46/50\n280/280 [==============================] - 0s 69us/step - loss: 0.0778 - acc: 0.9929 - val_loss: 0.2538 - val_acc: 0.9137\nEpoch 47/50\n280/280 [==============================] - 0s 77us/step - loss: 0.0762 - acc: 0.9929 - val_loss: 0.2535 - val_acc: 0.9137\nEpoch 48/50\n280/280 [==============================] - 0s 75us/step - loss: 0.0747 - acc: 0.9929 - val_loss: 0.2518 - val_acc: 0.9137\nEpoch 49/50\n280/280 [==============================] - 0s 69us/step - loss: 0.0732 - acc: 0.9964 - val_loss: 0.2520 - val_acc: 0.9137\nEpoch 50/50\n280/280 [==============================] - 0s 64us/step - loss: 0.0718 - acc: 0.9964 - val_loss: 0.2508 - val_acc: 0.9137\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n\nax1.plot(history.history['loss'], label='train loss')\nax1.plot(history.history['val_loss'], label='val loss')\nax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_loss'][-3:]) \n))\nax1.set_xlabel('epoch')\nax1.set_ylabel('loss value')\nax1.legend()\n\nax2.plot(history.history['acc'], label='train acc')\nax2.plot(history.history['val_acc'], label='val acc')\nax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_acc'][-3:]) \n))\nax2.set_xlabel('epoch')\nax2.set_ylabel('accuracy')\nax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n(test_loss, test_accuracy) = model.evaluate(X_te, Y_te, batch_size=32)\n\nprint('Test loss: {:.2f}'.format(test_loss)) \nprint('Test accuracy: {:.2f}%'.format(100*test_accuracy)) \n\n50/50 [==============================] - 0s 112us/step\nTest loss: 0.21\nTest accuracy: 92.00%\n\n\n\nimport pandas as pd \n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'1-layer nn','test_accuracy':round(test_accuracy,2)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\n\n2-layer dense network i.e. one hidden layer\n\nI create the model with 2 layers:\n\nmodel_2 = Sequential()\nmodel_2.add(Dense(50, activation='relu', input_dim=1280))\nmodel_2.add(Dense(6, activation='softmax'))\nmodel_2.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 50)                64050     \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 306       \n=================================================================\nTotal params: 64,356\nTrainable params: 64,356\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel_2.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['acc'])\n\n\nhistory = model_2.fit(\n    x=X_tr, y=Y_tr,\n    validation_data=(X_val,Y_val), batch_size=32, epochs=50, shuffle=True\n)\n\nTrain on 280 samples, validate on 139 samples\nEpoch 1/50\n280/280 [==============================] - 1s 2ms/step - loss: 1.5272 - acc: 0.4214 - val_loss: 1.0659 - val_acc: 0.7122\nEpoch 2/50\n280/280 [==============================] - 0s 116us/step - loss: 0.9486 - acc: 0.7500 - val_loss: 0.7731 - val_acc: 0.7986\nEpoch 3/50\n280/280 [==============================] - 0s 118us/step - loss: 0.6889 - acc: 0.8179 - val_loss: 0.6228 - val_acc: 0.8201\nEpoch 4/50\n280/280 [==============================] - 0s 121us/step - loss: 0.5490 - acc: 0.8607 - val_loss: 0.5392 - val_acc: 0.8489\nEpoch 5/50\n280/280 [==============================] - 0s 120us/step - loss: 0.4590 - acc: 0.8893 - val_loss: 0.4809 - val_acc: 0.8561\nEpoch 6/50\n280/280 [==============================] - 0s 117us/step - loss: 0.3946 - acc: 0.8964 - val_loss: 0.4423 - val_acc: 0.8561\nEpoch 7/50\n280/280 [==============================] - 0s 118us/step - loss: 0.3472 - acc: 0.9250 - val_loss: 0.4130 - val_acc: 0.8849\nEpoch 8/50\n280/280 [==============================] - 0s 119us/step - loss: 0.3116 - acc: 0.9214 - val_loss: 0.3899 - val_acc: 0.8849\nEpoch 9/50\n280/280 [==============================] - 0s 120us/step - loss: 0.2860 - acc: 0.9321 - val_loss: 0.3731 - val_acc: 0.8921\nEpoch 10/50\n280/280 [==============================] - 0s 124us/step - loss: 0.2567 - acc: 0.9464 - val_loss: 0.3563 - val_acc: 0.9065\nEpoch 11/50\n280/280 [==============================] - 0s 119us/step - loss: 0.2345 - acc: 0.9500 - val_loss: 0.3431 - val_acc: 0.9065\nEpoch 12/50\n280/280 [==============================] - 0s 121us/step - loss: 0.2162 - acc: 0.9643 - val_loss: 0.3340 - val_acc: 0.9065\nEpoch 13/50\n280/280 [==============================] - 0s 118us/step - loss: 0.2006 - acc: 0.9679 - val_loss: 0.3238 - val_acc: 0.9065\nEpoch 14/50\n280/280 [==============================] - 0s 125us/step - loss: 0.1876 - acc: 0.9679 - val_loss: 0.3161 - val_acc: 0.8993\nEpoch 15/50\n280/280 [==============================] - 0s 114us/step - loss: 0.1755 - acc: 0.9714 - val_loss: 0.3101 - val_acc: 0.9065\nEpoch 16/50\n280/280 [==============================] - 0s 124us/step - loss: 0.1648 - acc: 0.9786 - val_loss: 0.3042 - val_acc: 0.9065\nEpoch 17/50\n280/280 [==============================] - 0s 122us/step - loss: 0.1557 - acc: 0.9821 - val_loss: 0.3002 - val_acc: 0.9065\nEpoch 18/50\n280/280 [==============================] - 0s 121us/step - loss: 0.1465 - acc: 0.9893 - val_loss: 0.2954 - val_acc: 0.9065\nEpoch 19/50\n280/280 [==============================] - 0s 117us/step - loss: 0.1393 - acc: 0.9857 - val_loss: 0.2928 - val_acc: 0.8993\nEpoch 20/50\n280/280 [==============================] - 0s 117us/step - loss: 0.1322 - acc: 0.9893 - val_loss: 0.2892 - val_acc: 0.8993\nEpoch 21/50\n280/280 [==============================] - 0s 134us/step - loss: 0.1253 - acc: 0.9929 - val_loss: 0.2857 - val_acc: 0.9065\nEpoch 22/50\n280/280 [==============================] - 0s 119us/step - loss: 0.1200 - acc: 0.9929 - val_loss: 0.2846 - val_acc: 0.8993\nEpoch 23/50\n280/280 [==============================] - 0s 117us/step - loss: 0.1151 - acc: 0.9893 - val_loss: 0.2826 - val_acc: 0.8993\nEpoch 24/50\n280/280 [==============================] - 0s 117us/step - loss: 0.1098 - acc: 0.9929 - val_loss: 0.2800 - val_acc: 0.8993\nEpoch 25/50\n280/280 [==============================] - 0s 116us/step - loss: 0.1048 - acc: 0.9929 - val_loss: 0.2791 - val_acc: 0.8993\nEpoch 26/50\n280/280 [==============================] - 0s 119us/step - loss: 0.1013 - acc: 0.9929 - val_loss: 0.2773 - val_acc: 0.8993\nEpoch 27/50\n280/280 [==============================] - 0s 119us/step - loss: 0.0978 - acc: 0.9929 - val_loss: 0.2780 - val_acc: 0.8993\nEpoch 28/50\n280/280 [==============================] - 0s 118us/step - loss: 0.0953 - acc: 0.9929 - val_loss: 0.2763 - val_acc: 0.8993\nEpoch 29/50\n280/280 [==============================] - 0s 133us/step - loss: 0.0901 - acc: 0.9929 - val_loss: 0.2758 - val_acc: 0.8993\nEpoch 30/50\n280/280 [==============================] - 0s 135us/step - loss: 0.0866 - acc: 0.9929 - val_loss: 0.2745 - val_acc: 0.8993\nEpoch 31/50\n280/280 [==============================] - 0s 361us/step - loss: 0.0840 - acc: 0.9929 - val_loss: 0.2719 - val_acc: 0.9065\nEpoch 32/50\n280/280 [==============================] - 0s 129us/step - loss: 0.0814 - acc: 0.9929 - val_loss: 0.2717 - val_acc: 0.9065\nEpoch 33/50\n280/280 [==============================] - 0s 121us/step - loss: 0.0784 - acc: 0.9929 - val_loss: 0.2721 - val_acc: 0.8993\nEpoch 34/50\n280/280 [==============================] - 0s 103us/step - loss: 0.0758 - acc: 0.9929 - val_loss: 0.2722 - val_acc: 0.8993\nEpoch 35/50\n280/280 [==============================] - 0s 121us/step - loss: 0.0732 - acc: 0.9929 - val_loss: 0.2702 - val_acc: 0.9065\nEpoch 36/50\n280/280 [==============================] - 0s 119us/step - loss: 0.0711 - acc: 0.9929 - val_loss: 0.2701 - val_acc: 0.9065\nEpoch 37/50\n280/280 [==============================] - 0s 118us/step - loss: 0.0695 - acc: 0.9929 - val_loss: 0.2688 - val_acc: 0.9065\nEpoch 38/50\n280/280 [==============================] - 0s 119us/step - loss: 0.0670 - acc: 0.9929 - val_loss: 0.2686 - val_acc: 0.9065\nEpoch 39/50\n280/280 [==============================] - 0s 131us/step - loss: 0.0646 - acc: 0.9964 - val_loss: 0.2692 - val_acc: 0.9065\nEpoch 40/50\n280/280 [==============================] - 0s 112us/step - loss: 0.0634 - acc: 0.9964 - val_loss: 0.2677 - val_acc: 0.9065\nEpoch 41/50\n280/280 [==============================] - 0s 108us/step - loss: 0.0607 - acc: 0.9964 - val_loss: 0.2677 - val_acc: 0.9065\nEpoch 42/50\n280/280 [==============================] - 0s 121us/step - loss: 0.0599 - acc: 0.9964 - val_loss: 0.2676 - val_acc: 0.9065\nEpoch 43/50\n280/280 [==============================] - 0s 114us/step - loss: 0.0577 - acc: 0.9964 - val_loss: 0.2676 - val_acc: 0.9065\nEpoch 44/50\n280/280 [==============================] - 0s 108us/step - loss: 0.0561 - acc: 0.9964 - val_loss: 0.2676 - val_acc: 0.9065\nEpoch 45/50\n280/280 [==============================] - 0s 124us/step - loss: 0.0549 - acc: 0.9964 - val_loss: 0.2662 - val_acc: 0.9065\nEpoch 46/50\n280/280 [==============================] - 0s 131us/step - loss: 0.0533 - acc: 0.9964 - val_loss: 0.2658 - val_acc: 0.9065\nEpoch 47/50\n280/280 [==============================] - 0s 106us/step - loss: 0.0520 - acc: 0.9964 - val_loss: 0.2662 - val_acc: 0.9065\nEpoch 48/50\n280/280 [==============================] - 0s 126us/step - loss: 0.0508 - acc: 0.9964 - val_loss: 0.2664 - val_acc: 0.9065\nEpoch 49/50\n280/280 [==============================] - 0s 113us/step - loss: 0.0495 - acc: 0.9964 - val_loss: 0.2659 - val_acc: 0.9065\nEpoch 50/50\n280/280 [==============================] - 0s 122us/step - loss: 0.0484 - acc: 0.9964 - val_loss: 0.2661 - val_acc: 0.9065\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n\nax1.plot(history.history['loss'], label='train loss')\nax1.plot(history.history['val_loss'], label='val loss')\nax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_loss'][-3:]) \n))\nax1.set_xlabel('epoch')\nax1.set_ylabel('loss value')\nax1.legend()\n\nax2.plot(history.history['acc'], label='train acc')\nax2.plot(history.history['val_acc'], label='val acc')\nax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_acc'][-3:]) \n))\nax2.set_xlabel('epoch')\nax2.set_ylabel('accuracy')\nax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n(test_loss, test_accuracy) = model_2.evaluate(X_te, Y_te, batch_size=32)\n\nprint('Test loss: {:.2f}'.format(test_loss)) \nprint('Test accuracy: {:.2f}%'.format(100*test_accuracy)) \n\n50/50 [==============================] - 0s 152us/step\nTest loss: 0.20\nTest accuracy: 94.00%\n\n\nIt improves a bit the test accuracy.\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'2-layer nn','test_accuracy':round(test_accuracy,2)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\nSource: 7. Dense networks\n\n8. Convnets\n\nWhat accuracy can you achieve?\n\nI first load the datasets.\n\nimport numpy as np\n\nwith np.load('trainset.npz', allow_pickle=False) as npz_file:\n    X_tr = npz_file['train_feature']\n    Y_tr = npz_file['train_label']\n    names_tr = npz_file['train_name']\n    images_tr = npz_file['train_image']\n    \nwith np.load('validset.npz', allow_pickle=False) as npz_file:\n    X_val = npz_file['valid_feature']\n    Y_val = npz_file['valid_label']\n    names_val = npz_file['valid_name']\n    images_val = npz_file['valid_image']\n    \nwith np.load('testset.npz', allow_pickle=False) as npz_file:\n    X_te = npz_file['test_feature']\n    Y_te = npz_file['test_label']\n    names_te = npz_file['test_name']\n    images_te = npz_file['test_image']\n\n\nimages_tr.shape\n\n(280, 224, 224, 3)\n\n\nThe shape is different now: 280 images, 224 pixels in length and width and 3 for the RBF colors.\nSo I have to work with the images directly and not with the features.\n\nimport tensorflow.keras as keras\n\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=5, strides=2,\n                              activation='relu', input_shape=(224, 224, 3)))\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,\n                              activation='relu'))\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(units=len(names_tr), activation='softmax'))\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 110, 110, 64)      4864      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 55, 55, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 53, 53, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 26, 26, 64)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 43264)             0         \n_________________________________________________________________\ndense (Dense)                (None, 6)                 259590    \n=================================================================\nTotal params: 301,382\nTrainable params: 301,382\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer=keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n\n\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n\n\nhistory = model.fit(\n  x=images_tr, y=Y_tr, validation_data=(images_val,Y_val),\n    batch_size=32, epochs=50, shuffle=False, callbacks=[early_stopping]\n)\n\nTrain on 280 samples, validate on 139 samples\nEpoch 1/50\n280/280 [==============================] - 6s 21ms/step - loss: 1.8190 - acc: 0.2500 - val_loss: 1.7580 - val_acc: 0.2302\nEpoch 2/50\n280/280 [==============================] - 4s 13ms/step - loss: 1.6767 - acc: 0.3286 - val_loss: 1.6980 - val_acc: 0.2302\nEpoch 3/50\n280/280 [==============================] - 4s 14ms/step - loss: 1.5277 - acc: 0.3500 - val_loss: 1.6350 - val_acc: 0.2734\nEpoch 4/50\n280/280 [==============================] - 4s 13ms/step - loss: 1.2265 - acc: 0.5464 - val_loss: 1.6170 - val_acc: 0.3381\nEpoch 5/50\n280/280 [==============================] - 4s 14ms/step - loss: 0.8568 - acc: 0.6679 - val_loss: 1.5506 - val_acc: 0.4101\nEpoch 6/50\n280/280 [==============================] - 4s 14ms/step - loss: 0.6211 - acc: 0.8000 - val_loss: 1.4878 - val_acc: 0.4748\nEpoch 7/50\n280/280 [==============================] - 4s 14ms/step - loss: 0.3643 - acc: 0.8821 - val_loss: 1.6023 - val_acc: 0.4604\nEpoch 8/50\n280/280 [==============================] - 4s 15ms/step - loss: 0.1790 - acc: 0.9821 - val_loss: 2.0222 - val_acc: 0.4317\nEpoch 9/50\n280/280 [==============================] - 4s 15ms/step - loss: 0.0766 - acc: 0.9929 - val_loss: 2.0663 - val_acc: 0.4245\nEpoch 10/50\n280/280 [==============================] - 4s 13ms/step - loss: 0.0337 - acc: 0.9964 - val_loss: 1.9994 - val_acc: 0.4460\nEpoch 11/50\n280/280 [==============================] - 4s 13ms/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.0683 - val_acc: 0.4460\nEpoch 12/50\n280/280 [==============================] - 4s 13ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 2.2585 - val_acc: 0.4317\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n\nax1.plot(history.history['loss'], label='train loss')\nax1.plot(history.history['val_loss'], label='val loss')\nax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_loss'][-3:]) \n))\nax1.set_xlabel('epoch')\nax1.set_ylabel('loss value')\nax1.legend()\n\nax2.plot(history.history['acc'], label='train acc')\nax2.plot(history.history['val_acc'], label='val acc')\nax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_acc'][-3:]) \n))\nax2.set_xlabel('epoch')\nax2.set_ylabel('accuracy')\nax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n(test_loss, test_accuracy) = model.evaluate(images_te, Y_te, batch_size=32)\n\nprint('Test loss: {:.2f}'.format(test_loss)) \nprint('Test accuracy: {:.2f}%'.format(100*test_accuracy)) \n\n50/50 [==============================] - 0s 6ms/step\nTest loss: 2.27\nTest accuracy: 42.00%\n\n\nI have an accuracy of 40-50% which is quite bad results.\n\nimport pandas as pd \n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nnew_row = {'model':'cnn','test_accuracy':round(test_accuracy,2)}\nResults = Results.append(new_row,ignore_index=True)\n\n\nResults.to_csv(r\"Results.csv\")\n\n\nCan you get good results? - If not, why?\n\nI could maybe have better results if I try to tune a bit the model (kernel size, padding, number of layers, etc.) but it could not be better than the other models. This is because the other models has been pre-trained with the high level features on a much bigger dataset than this one.\nSource: 8. Convnets\n\n9. Resultls\n\nimport pandas as pd \n\n\nResults = pd.read_csv('Results.csv').drop('Unnamed: 0',axis=1)\n\n\nResults\n\n\n\n\n\n\n\n\nmodel\ntest_accuracy\n\n\n\n\n0\nk-NN\n0.98\n\n\n1\ndecision tree\n0.74\n\n\n2\nlogistic\n0.92\n\n\n3\nrandom forest\n0.90\n\n\n4\nsvm linear\n0.92\n\n\n5\nsvm rbf\n0.96\n\n\n6\n1-layer nn\n0.92\n\n\n7\n2-layer nn\n0.94\n\n\n8\ncnn\n0.42\n\n\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nResults = Results.sort_values('test_accuracy')\n\nplt.figure(figsize=(20, 10))\nplt.bar(Results.model,Results.test_accuracy,color='lightsalmon')\nplt.xlabel('Model type')\nplt.ylabel('Test accuracy')\nplt.title('Performance of the different models')\nplt.show()\n\n\n\n\n\n\n\n\nSource: 9. Resultls"
  },
  {
    "objectID": "projet_3.html",
    "href": "projet_3.html",
    "title": "House prices predictions using machine learning models",
    "section": "",
    "text": "House prices predictions using machine learning models\nThe goal of this project was to build several models of varying complexity that can predict house prices. Then, we compare the results.\nThe dataset contains about 3000 observations (i.e. houses) and 82 attributes. We use the first 2500 observations to fit our models and estimate prices for the 500 others.\n\n\n\nPackages\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as  plt\n%matplotlib inline\nimport math\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nData set\n\ndata_df = pd.read_csv(\"/Users/gregoireurvoy/Documents/ML/Course 3/09. Course project/Project/house-prices.csv\")\n\n\nprint('data_df shape:', data_df.shape) \n\ndata_df shape: (2430, 82)\n\n\n\ndata_df.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n484\n528275070\n60\nRL\nNaN\n8795\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2009\nWD\nNormal\n236000\n\n\n1\n2586\n535305120\n20\nRL\n75.0\n10170\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n6\n2006\nWD\nNormal\n155000\n\n\n2\n2289\n923228250\n160\nRM\n21.0\n2001\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n1\n2007\nWD\nNormal\n75000\n\n\n3\n142\n535152150\n20\nRL\n70.0\n10552\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n165500\n\n\n4\n2042\n903475060\n190\nRM\n60.0\n10120\nPave\nNaN\nIR1\nBnk\n...\n0\nNaN\nMnPrv\nNaN\n0\n1\n2007\nWD\nNormal\n122000\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\n1. Data cleaning:\n\ndata_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2430 entries, 0 to 2429\nData columns (total 82 columns):\nOrder              2430 non-null int64\nPID                2430 non-null int64\nMS SubClass        2430 non-null int64\nMS Zoning          2430 non-null object\nLot Frontage       2010 non-null float64\nLot Area           2430 non-null int64\nStreet             2430 non-null object\nAlley              163 non-null object\nLot Shape          2430 non-null object\nLand Contour       2430 non-null object\nUtilities          2430 non-null object\nLot Config         2430 non-null object\nLand Slope         2430 non-null object\nNeighborhood       2430 non-null object\nCondition 1        2430 non-null object\nCondition 2        2430 non-null object\nBldg Type          2430 non-null object\nHouse Style        2430 non-null object\nOverall Qual       2430 non-null int64\nOverall Cond       2430 non-null int64\nYear Built         2430 non-null int64\nYear Remod/Add     2430 non-null int64\nRoof Style         2430 non-null object\nRoof Matl          2430 non-null object\nExterior 1st       2430 non-null object\nExterior 2nd       2430 non-null object\nMas Vnr Type       2410 non-null object\nMas Vnr Area       2410 non-null float64\nExter Qual         2430 non-null object\nExter Cond         2430 non-null object\nFoundation         2430 non-null object\nBsmt Qual          2359 non-null object\nBsmt Cond          2359 non-null object\nBsmt Exposure      2356 non-null object\nBsmtFin Type 1     2359 non-null object\nBsmtFin SF 1       2429 non-null float64\nBsmtFin Type 2     2358 non-null object\nBsmtFin SF 2       2429 non-null float64\nBsmt Unf SF        2429 non-null float64\nTotal Bsmt SF      2429 non-null float64\nHeating            2430 non-null object\nHeating QC         2430 non-null object\nCentral Air        2430 non-null object\nElectrical         2429 non-null object\n1st Flr SF         2430 non-null int64\n2nd Flr SF         2430 non-null int64\nLow Qual Fin SF    2430 non-null int64\nGr Liv Area        2430 non-null int64\nBsmt Full Bath     2428 non-null float64\nBsmt Half Bath     2428 non-null float64\nFull Bath          2430 non-null int64\nHalf Bath          2430 non-null int64\nBedroom AbvGr      2430 non-null int64\nKitchen AbvGr      2430 non-null int64\nKitchen Qual       2430 non-null object\nTotRms AbvGrd      2430 non-null int64\nFunctional         2430 non-null object\nFireplaces         2430 non-null int64\nFireplace Qu       1244 non-null object\nGarage Type        2294 non-null object\nGarage Yr Blt      2292 non-null float64\nGarage Finish      2292 non-null object\nGarage Cars        2429 non-null float64\nGarage Area        2429 non-null float64\nGarage Qual        2292 non-null object\nGarage Cond        2292 non-null object\nPaved Drive        2430 non-null object\nWood Deck SF       2430 non-null int64\nOpen Porch SF      2430 non-null int64\nEnclosed Porch     2430 non-null int64\n3Ssn Porch         2430 non-null int64\nScreen Porch       2430 non-null int64\nPool Area          2430 non-null int64\nPool QC            12 non-null object\nFence              489 non-null object\nMisc Feature       90 non-null object\nMisc Val           2430 non-null int64\nMo Sold            2430 non-null int64\nYr Sold            2430 non-null int64\nSale Type          2430 non-null object\nSale Condition     2430 non-null object\nSalePrice          2430 non-null int64\ndtypes: float64(11), int64(28), object(43)\nmemory usage: 1.5+ MB\n\n\nThe datatype seems correct for every columns except for the first column MS SubClass that should be an object and not an int64. To avoid any confusion, I change the labels of the column the following way:\n\nchange = { 20:\"a\", 30:\"b\", 40:\"c\", 45:\"d\", 50:\"e\", 60:\"f\", 70:\"g\", 75:\"h\", 80:\"i\", 85:\"j\", 90:\"k\", 120:\"l\", 150:\"m\", 160:\"n\", 180:\"o\", 190:\"p\"}\ndata_df['MS SubClass']= data_df['MS SubClass'].map(change)\ndata_df['MS SubClass'].head()\n\n0    f\n1    a\n2    n\n3    a\n4    p\nName: MS SubClass, dtype: object\n\n\nI delete the two columns Order and PID since they do not provide usefull informations (they are only identifiers).\n\ndata_df = data_df.drop(['Order', 'PID'], axis=1)\n\n\nDuplicated and missing values\nI check if there are duplicated values.\n\ndata_df.duplicated().sum()\n\n0\n\n\nThere is no duplicated values. I can check now the missing values.\n\ncol_NA = [col for col in data_df.columns if data_df[col].isnull().any()]\n\n\ndata_df[col_NA].isnull().sum()\n\nLot Frontage       420\nAlley             2267\nMas Vnr Type        20\nMas Vnr Area        20\nBsmt Qual           71\nBsmt Cond           71\nBsmt Exposure       74\nBsmtFin Type 1      71\nBsmtFin SF 1         1\nBsmtFin Type 2      72\nBsmtFin SF 2         1\nBsmt Unf SF          1\nTotal Bsmt SF        1\nElectrical           1\nBsmt Full Bath       2\nBsmt Half Bath       2\nFireplace Qu      1186\nGarage Type        136\nGarage Yr Blt      138\nGarage Finish      138\nGarage Cars          1\nGarage Area          1\nGarage Qual        138\nGarage Cond        138\nPool QC           2418\nFence             1941\nMisc Feature      2340\ndtype: int64\n\n\nFor some of these variables, the missing value NaN actually gives us information. For example, for the variable “Alley”, it means that the house does not have an alley. It doesn’t mean that the information is missing. Therefore, we should fill the NaN values with the string NA as indicated in the documentation. In some cases, it is maybe a true missing value, but for most of the cases it should be correct to make this hypothesis.\n\ndata_df['Alley'].unique()\n(data_df['Alley'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Bsmt Qual'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Bsmt Cond'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Bsmt Exposure'] == 'NA').sum()\n\n0\n\n\n\n(data_df['BsmtFin Type 1'] == 'NA').sum()\n\n0\n\n\n\n(data_df['BsmtFin Type 2'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Fireplace Qu'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Garage Type'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Garage Finish'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Garage Qual'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Garage Cond'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Pool QC'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Fence'] == 'NA').sum()\n\n0\n\n\n\n(data_df['Misc Feature'] == 'NA').sum()\n\n0\n\n\n\ndata_df['Alley'].fillna(value='NA', inplace=True)\ndata_df['Bsmt Qual'].fillna(value='NA', inplace=True)\ndata_df['Bsmt Cond'].fillna(value='NA', inplace=True)\ndata_df['Bsmt Exposure'].fillna(value='NA', inplace=True)\ndata_df['BsmtFin Type 1'].fillna(value='NA', inplace=True)\ndata_df['BsmtFin Type 2'].fillna(value='NA', inplace=True)\ndata_df['Fireplace Qu'].fillna(value='NA', inplace=True)\ndata_df['Garage Type'].fillna(value='NA', inplace=True)\ndata_df['Garage Finish'].fillna(value='NA', inplace=True)\ndata_df['Garage Qual'].fillna(value='NA', inplace=True)\ndata_df['Garage Cond'].fillna(value='NA', inplace=True)\ndata_df['Pool QC'].fillna(value='NA', inplace=True)\ndata_df['Fence'].fillna(value='NA', inplace=True)\ndata_df['Misc Feature'].fillna(value='NA', inplace=True)\n\n\ncol_NA2 = [col for col in data_df.columns if data_df[col].isnull().any()]\ndata_df[col_NA2].isnull().sum()\n\nLot Frontage      420\nMas Vnr Type       20\nMas Vnr Area       20\nBsmtFin SF 1        1\nBsmtFin SF 2        1\nBsmt Unf SF         1\nTotal Bsmt SF       1\nElectrical          1\nBsmt Full Bath      2\nBsmt Half Bath      2\nGarage Yr Blt     138\nGarage Cars         1\nGarage Area         1\ndtype: int64\n\n\nI decide to delete the rows where there is only 1 or 2 missing values in the variable. It should not impact the overall results of the analysis since we have more than 2000 rows.\n\ndata_df.dropna(subset=['BsmtFin SF 1', 'BsmtFin SF 2','Bsmt Unf SF','Total Bsmt SF','Electrical','Bsmt Full Bath','Bsmt Half Bath','Garage Cars','Garage Area'],inplace=True)\n\n\ncol_NA3 = [col for col in data_df.columns if data_df[col].isnull().any()]\ndata_df[col_NA3].isnull().sum()\n\nLot Frontage     420\nMas Vnr Type      20\nMas Vnr Area      20\nGarage Yr Blt    137\ndtype: int64\n\n\nFor the last 4 ones, we need to investigate the variables one by one.\n\nLot Frontage:\n\n\ndata_df['Lot Frontage'].value_counts()\n\n60.0     229\n80.0     112\n70.0     104\n50.0      98\n75.0      87\n65.0      74\n85.0      67\n63.0      43\n78.0      42\n24.0      42\n90.0      42\n68.0      40\n21.0      40\n64.0      34\n72.0      33\n74.0      31\n52.0      26\n73.0      25\n100.0     25\n62.0      24\n51.0      23\n55.0      23\n79.0      22\n82.0      22\n57.0      20\n66.0      20\n40.0      19\n76.0      19\n59.0      19\n44.0      18\n        ... \n134.0      2\n313.0      2\n115.0      2\n140.0      2\n128.0      2\n130.0      2\n121.0      2\n125.0      2\n116.0      2\n38.0       2\n122.0      2\n200.0      1\n182.0      1\n138.0      1\n195.0      1\n152.0      1\n33.0       1\n126.0      1\n168.0      1\n150.0      1\n135.0      1\n136.0      1\n22.0       1\n31.0       1\n25.0       1\n155.0      1\n113.0      1\n119.0      1\n133.0      1\n141.0      1\nName: Lot Frontage, Length: 120, dtype: int64\n\n\nThis variable is continuous and measures the linear feet of street connected to property. In order to limit the effects of potential outliers, I decided to fill the missing values by the median value of the varialbe.\n\ndata_df['Lot Frontage'].fillna(value=data_df['Lot Frontage'].median(), inplace=True)\n\n\nMas Vnr Type & Mas Vnr Area:\n\n\ndata_df['Mas Vnr Type'].value_counts()\n\nNone       1438\nBrkFace     736\nStone       210\nBrkCmn       21\nCBlock        1\nName: Mas Vnr Type, dtype: int64\n\n\n\ndata_df['Mas Vnr Area'].value_counts()\n\n0.0       1434\n120.0       14\n200.0       12\n144.0       11\n176.0       10\n196.0        9\n108.0        9\n16.0         9\n180.0        9\n210.0        9\n216.0        9\n72.0         9\n128.0        8\n164.0        7\n178.0        7\n170.0        7\n40.0         7\n256.0        7\n456.0        7\n302.0        7\n174.0        7\n340.0        7\n80.0         7\n240.0        7\n50.0         6\n420.0        6\n260.0        6\n203.0        6\n186.0        6\n132.0        6\n          ... \n528.0        1\n53.0         1\n1170.0       1\n283.0        1\n568.0        1\n263.0        1\n562.0        1\n970.0        1\n571.0        1\n221.0        1\n530.0        1\n376.0        1\n254.0        1\n291.0        1\n102.0        1\n754.0        1\n634.0        1\n630.0        1\n257.0        1\n1050.0       1\n522.0        1\n323.0        1\n428.0        1\n418.0        1\n616.0        1\n440.0        1\n664.0        1\n760.0        1\n57.0         1\n394.0        1\nName: Mas Vnr Area, Length: 408, dtype: int64\n\n\nWe can see that the most common value is None for the type of Masonry and 0 for the area. Let’s check the value of the type when there is a missing value for the Area.\n\ndata_df[data_df['Mas Vnr Area'].isnull()]['Mas Vnr Type']\n\n66      NaN\n83      NaN\n101     NaN\n159     NaN\n201     NaN\n270     NaN\n352     NaN\n447     NaN\n456     NaN\n457     NaN\n465     NaN\n624     NaN\n851     NaN\n891     NaN\n1098    NaN\n1480    NaN\n1592    NaN\n1832    NaN\n2137    NaN\n2219    NaN\nName: Mas Vnr Type, dtype: object\n\n\nWhen there is a missing value for the type, there is also a missing value for the area. Therefore I propose to replace the missing values the most common ones: None and 0.\n\ndata_df['Mas Vnr Area'].fillna(value=0, inplace=True)\n\n\ndata_df['Mas Vnr Type'].fillna(value='NA', inplace=True)\n\n\nGarage Yr Blt:\n\n\ndata_df['Garage Yr Blt'].value_counts()\n\n2005.0    114\n2007.0     90\n2006.0     89\n2004.0     84\n2003.0     79\n1977.0     58\n2008.0     54\n2002.0     46\n1998.0     46\n1976.0     45\n1999.0     44\n2000.0     44\n1968.0     42\n1993.0     42\n1950.0     39\n1978.0     38\n2001.0     37\n1958.0     36\n1997.0     35\n1954.0     34\n1967.0     33\n1974.0     33\n1965.0     31\n1956.0     31\n1920.0     31\n1959.0     30\n1979.0     30\n1994.0     30\n1969.0     30\n1960.0     29\n         ... \n1936.0      7\n1935.0      7\n1922.0      6\n1946.0      6\n1924.0      6\n1928.0      6\n1900.0      5\n1942.0      5\n1927.0      5\n1923.0      5\n1915.0      5\n1932.0      4\n1921.0      4\n1937.0      4\n2010.0      4\n1947.0      4\n1916.0      4\n1982.0      4\n1934.0      3\n1918.0      2\n1931.0      2\n1929.0      2\n1917.0      2\n1914.0      2\n2207.0      1\n1943.0      1\n1933.0      1\n1919.0      1\n1896.0      1\n1908.0      1\nName: Garage Yr Blt, Length: 101, dtype: int64\n\n\nWe can therefore replace by the median in these cases.\n\nnp.median(data_df[data_df['Garage Yr Blt'] != 'NA']['Garage Yr Blt'])\n\nnan\n\n\n\ndata_df['Garage Yr Blt'].fillna(value=1978, inplace=True)\n\n\ndata_df.isnull().sum().sum()\n\n0\n\n\nThere is no more missing values.\n\n\nInconsistencies in the variables\nTo check potential inconsistencies, I need to be sure that: - if Bsmt Qual = 0, then Bsmt Cond, Bsmt Exposure, BsmtFin Type 1, BsmtFin Type 2 = NA, and BsmtFin SF 1, BsmtFin SF 2, Bsmt Unf SF, Total Bsmt SF, Bsmt Full Bath, Bsmt Half Bath = 0. - if House style = 1Story, then 2nd Flr SF = 0. - if Fireplaces == 0, then FireplaceQu == NA. - if Garage Type = NA, then Garage Yr Blt, Garage Finish, Garage Qual, Garage Cond = NA, and Garage Cars, Garage Area = 0. - if Pool QC = NA, then Pool Area = 0 (and vice versa). - if Misc Feature = 0, then Misc Val = 0 (and vice versa). - Yr Sold &gt; Year Built - Correct numbers: Lot Frontage, Lot Area, Mas Vnr Area, BsmtFin SF 1, BsmtFin SF 2, Bsmt Unf SF, Total Bsmt SF, 1st Flr SF, 2nd Fr SF, Low Qual Fin SF, Gr Liv Area, Bsmt Full bath, Bsmt Half Bath, Full Bath, Half Bath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, Garage Cars, Garage Area, Wood Deck SF, Open Porch SF, Enclosed Porch, 3-Ssn Porch, Screen Porch, Pool Area, Misc Val, SalePrice - Correct Years: Year Built, Year Remod/Add, Garage Yr Blt, Yr Sold\n\nBasement\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Bsmt Cond']!= 'NA')][['Bsmt Qual','Bsmt Cond']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Cond\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Bsmt Exposure']!= 'NA')][['Bsmt Qual','Bsmt Exposure']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Exposure\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['BsmtFin Type 1']!= 'NA')][['Bsmt Qual','BsmtFin Type 1']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin Type 1\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['BsmtFin Type 2']!= 'NA')][['Bsmt Qual','BsmtFin Type 2']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin Type 2\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['BsmtFin SF 1']!= 0)][['Bsmt Qual','BsmtFin SF 1']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin SF 1\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['BsmtFin SF 2']!= 0)][['Bsmt Qual','BsmtFin SF 2']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin SF 2\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Bsmt Unf SF']!= 0)][['Bsmt Qual','Bsmt Unf SF']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Unf SF\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Total Bsmt SF']!= 0)][['Bsmt Qual','Total Bsmt SF']]\n\n\n\n\n\n\n\n\nBsmt Qual\nTotal Bsmt SF\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Bsmt Full Bath']!= 0)][['Bsmt Qual','Bsmt Full Bath']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Full Bath\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Bsmt Qual' ] == 'NA') & (data_df['Bsmt Half Bath']!= 0)][['Bsmt Qual','Bsmt Half Bath']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Half Bath\n\n\n\n\n\n\n\n\n\nThere is no inconsistency with the basement variables.\n\nHouse Style\n\n\ndata_df[(data_df['House Style' ] == '1Story') & (data_df['2nd Flr SF']!= 0)][['House Style','2nd Flr SF']]\n\n\n\n\n\n\n\n\nHouse Style\n2nd Flr SF\n\n\n\n\n247\n1Story\n192\n\n\n1136\n1Story\n467\n\n\n1308\n1Story\n144\n\n\n\n\n\n\n\nSince it does not make sense to have the surface of area of the 2nd Floor for an house of one story, I replace the area by 0 in these cases.\n\ndata_df.loc[247,'2nd Flr SF'] = 0\ndata_df.loc[1136,'2nd Flr SF'] = 0\ndata_df.loc[1308,'2nd Flr SF'] = 0\n\n\nFireplace\n\n\ndata_df[(data_df['Fireplaces'] == 0) & (data_df['Fireplace Qu']!= 'NA')][['Fireplaces','Fireplace Qu']]\n\n\n\n\n\n\n\n\nFireplaces\nFireplace Qu\n\n\n\n\n\n\n\n\n\nThere is no inconsistency with the fireplace variable.\n\nGarage\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Yr Blt']!= 1978)][['Garage Type','Garage Yr Blt']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Yr Blt\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Finish']!= 'NA')][['Garage Type','Garage Finish']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Finish\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Cars']!= 0)][['Garage Type','Garage Cars']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Cars\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Area']!= 0)][['Garage Type','Garage Area']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Area\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Qual']!= 'NA')][['Garage Type','Garage Qual']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Qual\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Garage Type'] == 'NA') & (data_df['Garage Cond']!= 'NA')][['Garage Type','Garage Cond']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Cond\n\n\n\n\n\n\n\n\n\nThere is no inconsistency with the garage variables.\n\nPool\n\n\ndata_df[(data_df['Pool QC' ] != 'NA') & (data_df['Pool Area']== 0)][['Pool QC','Pool Area']]\n\n\n\n\n\n\n\n\nPool QC\nPool Area\n\n\n\n\n\n\n\n\n\n\ndata_df[(data_df['Pool QC' ] == 'NA') & (data_df['Pool Area']!= 0)][['Pool QC','Pool Area']]\n\n\n\n\n\n\n\n\nPool QC\nPool Area\n\n\n\n\n\n\n\n\n\nThere is no inconsistency with the pool variable.\n\nMisc\n\n\ndata_df[(data_df['Misc Feature' ] != 'NA') & (data_df['Misc Val']== 0)][['Misc Feature','Misc Val']]\n\n\n\n\n\n\n\n\nMisc Feature\nMisc Val\n\n\n\n\n1690\nShed\n0\n\n\n2012\nShed\n0\n\n\n\n\n\n\n\n\ndata_df[(data_df['Misc Feature' ] == 'NA') & (data_df['Misc Val']!= 0)][['Misc Feature','Misc Val']]\n\n\n\n\n\n\n\n\nMisc Feature\nMisc Val\n\n\n\n\n\n\n\n\n\nIt is unlikely that a shed is worth 0, so I replace the misc feature by NA in these 2 cases.\n\ndata_df.loc[1690,'Misc Feature'] = 'NA'\ndata_df.loc[2012,'Misc Feature'] = 'NA'\n\n\nYear Built:\n\n\n(data_df['Year Built'] &gt; data_df['Yr Sold']).sum()\n\n1\n\n\n\ndata_df[(data_df['Year Built'] &gt; data_df['Yr Sold']) == True][['Year Built','Yr Sold']]\n\n\n\n\n\n\n\n\nYear Built\nYr Sold\n\n\n\n\n71\n2008\n2007\n\n\n\n\n\n\n\nSince it does not make sense in this case, I replace the year built by ‘NA’.\n\ndata_df.loc[71,'Year Built'] = 1978\n\n\nCorrect numbers:\n\n\ndata_df[['Lot Frontage',\n         'Lot Area', \n         'Mas Vnr Area',\n         'BsmtFin SF 1',\n         'BsmtFin SF 2',\n         'Bsmt Unf SF',\n         'Total Bsmt SF',\n         '1st Flr SF',\n         '2nd Flr SF',\n         'Low Qual Fin SF',\n         'Gr Liv Area',\n         'Bsmt Full Bath',\n         'Bsmt Half Bath',\n         'Full Bath',\n         'Half Bath',\n         'Bedroom AbvGr',\n         'Kitchen AbvGr',\n         'Total Bsmt SF',\n         'Fireplaces',\n         'Garage Cars',\n         'Garage Area',\n         'Wood Deck SF',\n         'Open Porch SF',\n         'Enclosed Porch',\n         '3Ssn Porch',\n         'Screen Porch',\n         'Pool Area',\n         'Misc Val',\n         'SalePrice']].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nLot Frontage\n2426.0\n68.965787\n21.342066\n21.0\n60.00\n68.0\n78.00\n313.0\n\n\nLot Area\n2426.0\n10230.875103\n8326.905693\n1470.0\n7482.00\n9500.0\n11604.50\n215245.0\n\n\nMas Vnr Area\n2426.0\n101.322754\n176.221672\n0.0\n0.00\n0.0\n164.00\n1600.0\n\n\nBsmtFin SF 1\n2426.0\n444.267519\n457.553941\n0.0\n0.00\n375.0\n737.00\n5644.0\n\n\nBsmtFin SF 2\n2426.0\n51.136851\n172.240590\n0.0\n0.00\n0.0\n0.00\n1526.0\n\n\nBsmt Unf SF\n2426.0\n556.882523\n439.734434\n0.0\n216.00\n462.0\n800.50\n2336.0\n\n\nTotal Bsmt SF\n2426.0\n1052.286892\n444.227724\n0.0\n794.25\n990.0\n1299.75\n6110.0\n\n\n1st Flr SF\n2426.0\n1159.114592\n389.545598\n334.0\n880.50\n1086.0\n1387.75\n5095.0\n\n\n2nd Flr SF\n2426.0\n326.251443\n419.651918\n0.0\n0.00\n0.0\n698.00\n2065.0\n\n\nLow Qual Fin SF\n2426.0\n4.594394\n47.254734\n0.0\n0.00\n0.0\n0.00\n1064.0\n\n\nGr Liv Area\n2426.0\n1490.291426\n494.130532\n334.0\n1120.00\n1440.0\n1740.00\n5642.0\n\n\nBsmt Full Bath\n2426.0\n0.434460\n0.528011\n0.0\n0.00\n0.0\n1.00\n3.0\n\n\nBsmt Half Bath\n2426.0\n0.060181\n0.243017\n0.0\n0.00\n0.0\n0.00\n2.0\n\n\nFull Bath\n2426.0\n1.560181\n0.547803\n0.0\n1.00\n2.0\n2.00\n4.0\n\n\nHalf Bath\n2426.0\n0.369332\n0.501165\n0.0\n0.00\n0.0\n1.00\n2.0\n\n\nBedroom AbvGr\n2426.0\n2.842127\n0.816968\n0.0\n2.00\n3.0\n3.00\n8.0\n\n\nKitchen AbvGr\n2426.0\n1.042869\n0.208620\n0.0\n1.00\n1.0\n1.00\n3.0\n\n\nTotal Bsmt SF\n2426.0\n1052.286892\n444.227724\n0.0\n794.25\n990.0\n1299.75\n6110.0\n\n\nFireplaces\n2426.0\n0.595218\n0.646164\n0.0\n0.00\n1.0\n1.00\n4.0\n\n\nGarage Cars\n2426.0\n1.762572\n0.758523\n0.0\n1.00\n2.0\n2.00\n4.0\n\n\nGarage Area\n2426.0\n472.025556\n215.824704\n0.0\n324.00\n478.0\n576.00\n1488.0\n\n\nWood Deck SF\n2426.0\n93.959604\n128.173796\n0.0\n0.00\n0.0\n168.00\n1424.0\n\n\nOpen Porch SF\n2426.0\n47.016076\n66.833834\n0.0\n0.00\n27.0\n70.00\n742.0\n\n\nEnclosed Porch\n2426.0\n22.244023\n62.313936\n0.0\n0.00\n0.0\n0.00\n1012.0\n\n\n3Ssn Porch\n2426.0\n2.475680\n24.900553\n0.0\n0.00\n0.0\n0.00\n508.0\n\n\nScreen Porch\n2426.0\n16.556472\n56.865103\n0.0\n0.00\n0.0\n0.00\n490.0\n\n\nPool Area\n2426.0\n2.405194\n36.141627\n0.0\n0.00\n0.0\n0.00\n800.0\n\n\nMisc Val\n2426.0\n56.272465\n617.165693\n0.0\n0.00\n0.0\n0.00\n17000.0\n\n\nSalePrice\n2426.0\n180215.640561\n79615.671593\n12789.0\n129000.00\n160000.0\n213099.75\n755000.0\n\n\n\n\n\n\n\nThere are no negatives numbers and no extreme values. So the data seem consistent here. I will check again the extreme values later.\n\nCorrect years:\n\n\ndata_df[['Year Built','Year Remod/Add','Yr Sold']].describe()\n\n\n\n\n\n\n\n\nYear Built\nYear Remod/Add\nYr Sold\n\n\n\n\ncount\n2426.000000\n2426.000000\n2426.000000\n\n\nmean\n1971.176010\n1984.232481\n2007.783594\n\n\nstd\n30.194448\n20.747941\n1.312806\n\n\nmin\n1875.000000\n1950.000000\n2006.000000\n\n\n25%\n1954.000000\n1966.000000\n2007.000000\n\n\n50%\n1973.000000\n1993.000000\n2008.000000\n\n\n75%\n2000.000000\n2003.000000\n2009.000000\n\n\nmax\n2010.000000\n2010.000000\n2010.000000\n\n\n\n\n\n\n\n\nGarageYrBlt = data_df['Garage Yr Blt'].convert_objects(convert_numeric=True)\nGarageYrBlt = GarageYrBlt.convert_objects(convert_numeric=True).dropna()\nGarageYrBlt.describe()\n\ncount    2426.000000\nmean     1977.877988\nstd        24.810639\nmin      1896.000000\n25%      1962.000000\n50%      1978.000000\n75%      2001.000000\nmax      2207.000000\nName: Garage Yr Blt, dtype: float64\n\n\n\ndata_df[data_df['Garage Yr Blt'] == 2207]\n\n\n\n\n\n\n\n\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\nUtilities\nLot Config\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n2219\na\nRL\n68.0\n8298\nPave\nNA\nIR1\nHLS\nAllPub\nInside\n...\n0\nNA\nNA\nNA\n0\n9\n2007\nNew\nPartial\n267300\n\n\n\n\n1 rows × 80 columns\n\n\n\nSince a year of 2207 is not possible, I actually replace this value by the median.\n\ndata_df.loc[2219,'Garage Yr Blt'] = 1978\n\n\n\nHandle outliers\nWe can start by the special notes of the documentation that specifies some outliers. The notes recommends to remove any houses with more than 4000 square feet from the data set.\n\n(data_df['Gr Liv Area'] &gt; 4000).sum()\n\n4\n\n\n\nplt.scatter(data_df['Gr Liv Area'],data_df['SalePrice'])\nplt.xlabel('Living area in square feet')\nplt.ylabel('Sale price in \n);\n\n\n\n\n\n\n\n\nIndeed, these datapoints seem to be outliers. I remove them from the dataset.\n\ndata_df = data_df[data_df['Gr Liv Area'] &lt; 4000]\nplt.scatter(data_df['Gr Liv Area'],data_df['SalePrice'])\nplt.xlabel('Living area in square feet')\nplt.ylabel('Sale price in \n);\n\n\n\n\n\n\n\n\nNow, I can check all the other variables.\n\ncols = data_df.select_dtypes(include=['float64']).columns\ncols = cols.append(data_df.select_dtypes(include=['int64']).columns)\n\nfig,axes = plt.subplots(nrows=9,ncols=4,figsize=(16,40))\n\nfor col,ax in zip(cols,axes.ravel()):\n    ax.boxplot(data_df[col],whis=2)\n    ax.set_title(col)   \n\nplt.tight_layout();\n\n\n\n\n\n\n\n\nFor some variables, it is better to visualize the boxplots without the 0 values. For example, a 0 value for the variable basement full bath does not necessarily mean no bathrooms. Indeed, it can also mean no basement. I replot these variables to better visualize the outliers.\n\nNoZerosCols = ['Mas Vnr Area','BsmtFin SF 1','BsmtFin SF 2', 'Bsmt Unf SF','Total Bsmt SF','2nd Flr SF','Bsmt Full Bath', 'Bsmt Half Bath'\n               ,'Fireplaces', 'Garage Cars', 'Garage Area','Wood Deck SF','Open Porch SF','Enclosed Porch','3Ssn Porch',\n               'Screen Porch','Pool Area','Misc Val']\nlen(NoZerosCols)\n\n18\n\n\n\nfig,axes = plt.subplots(nrows=5,ncols=4,figsize= (20,50))\n\nfor col,ax in zip(NoZerosCols,axes.ravel()):\n    ax.boxplot(data_df[data_df[col] != 0][col])\n    ax.set_title(col)   \n\nplt.tight_layout();\n\n\n\n\n\n\n\n\nFrom the boxplots, the dataset doesn’t seem to contain outliers or incorrect values. Some values are big but remain consistents with their respective distributions. I will use the Huber regressor later to take that into account. Therefore, I will work with this dataset for the next part of the project.\n\n\n\n2. Feature encoding\nWe basically have 4 types of variables: - continuous variables (e.g. Lot Area) - discrete variables (e.g. Year Built)\n- nominal variables (e.g. Type of dwelling) - ordinal variables (e.g. Utilities)\nContinuous and discrete variables can remain in this format for the analysis. However, we have a problem for the nominal and ordinal variables.\nFor the nominal variables, I decide to replace their values with the One-hot encoding transformation. It makes sense because we cannot give an order for this kind of variable. What I mean is that there is no good or bad value. Therefore, creating dummy variables seem to be a good idea for me.\nFor the ordinal variables, I decide to replace their values by a scale beginning from “1” to “Variable.nunique()”, 1 being the worst value. I will use the value “0” to indicate the absence of the variable.\nFrom the documentation, we can easily distinguish the 4 types of variables.\n\nContinuousVar = ['Lot Frontage','Lot Area','Mas Vnr Area','BsmtFin SF 1','BsmtFin SF 2','Bsmt Unf SF','Total Bsmt SF','1st Flr SF',\n                '2nd Flr SF', 'Low Qual Fin SF','Gr Liv Area','Garage Area','Wood Deck SF','Open Porch SF','Enclosed Porch','3Ssn Porch',\n                'Screen Porch','Pool Area','Misc Val']\n\nDiscreteVar = ['Year Built','Year Remod/Add','Bsmt Full Bath', 'Bsmt Half Bath' ,'Full Bath'  ,'Half Bath','Bedroom AbvGr','Kitchen AbvGr'\n              ,'TotRms AbvGrd','Fireplaces','Garage Yr Blt','Garage Cars','Mo Sold','Yr Sold']\n\nNominalVar = ['MS SubClass','MS Zoning','Street','Alley','Land Contour','Lot Config','Neighborhood','Condition 1','Condition 2','Bldg Type'\n             ,'House Style','Roof Style','Roof Matl','Exterior 1st', 'Exterior 2nd','Mas Vnr Type','Foundation','Heating','Central Air',\n             'Garage Type', 'Misc Feature','Sale Type','Sale Condition']\n\nOrdinalVar = ['Lot Shape','Utilities','Land Slope','Overall Qual','Overall Cond','Exter Qual','Exter Cond','Bsmt Qual','Bsmt Cond',\n             'Bsmt Exposure','BsmtFin Type 1','BsmtFin Type 2','Heating QC','Electrical','Kitchen Qual','Functional','Fireplace Qu',\n             'Garage Finish','Garage Qual','Garage Cond','Paved Drive','Pool QC','Fence',]\n\n\nOrdinal variables\n\ndata_df['Lot Shape'].unique()\n\narray(['IR1', 'Reg', 'IR2', 'IR3'], dtype=object)\n\n\nUnfortunately, the function unique does not order automatically the values from a feature. It takes the first unique values that comes row by row. So, I have to change the values “by hand”.\n\ndata_df[OrdinalVar[0]].replace({'Reg':4, 'IR1':3,'IR2':2,'IR3':1},inplace=True)\ndata_df[OrdinalVar[1]].replace({'AllPub':4, 'NoSewr':3,'NoSeWa':2,'ELO':1},inplace=True)\ndata_df[OrdinalVar[2]].replace({ 'Gtl':3,'Mod':2,'Sev':1},inplace=True)\n### data_df[OrdinalVar[3]].replace({},inplace=True) (already in a numerical order)\n### data_df[OrdinalVar[4]].replace({},inplace=True) (already in a numerical order)\ndata_df[OrdinalVar[5]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ndata_df[OrdinalVar[6]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ndata_df[OrdinalVar[7]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[8]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1, 'NA':0},inplace=True)\ndata_df[OrdinalVar[9]].replace({'Gd':4, 'Av':3,'Mn':2,'No':1, 'NA':0},inplace=True)\ndata_df[OrdinalVar[10]].replace({'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3,'LwQ':2,'Unf':1, 'NA':0},inplace=True)\ndata_df[OrdinalVar[11]].replace({'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3,'LwQ':2,'Unf':1, 'NA':0},inplace=True)\ndata_df[OrdinalVar[12]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ndata_df[OrdinalVar[13]].replace({'SBrkr':5, 'FuseA':4, 'FuseF':3,'FuseP':2,'Mix':1},inplace=True)\ndata_df[OrdinalVar[14]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ndata_df[OrdinalVar[15]].replace({'Typ':8,'Min1':7,'Min2':6,'Mod':5, 'Maj1':4, 'Maj2':3,'Sev':2,'Sal':1},inplace=True)\ndata_df[OrdinalVar[16]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[17]].replace({'Fin':3,'RFn':2,'Unf':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[18]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[19]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[20]].replace({'Y':3,'P':2,'N':1},inplace=True)\ndata_df[OrdinalVar[21]].replace({'Ex':4, 'Gd':3,'TA':2,'Fa':1,'NA':0},inplace=True)\ndata_df[OrdinalVar[22]].replace({'GdPrv':4, 'MnPrv':3,'GdWo':2,'MnWw':1,'NA':0},inplace=True)\n\nI can verify that everything has been changed correctly:\n\npd.options.display.max_columns = 500\n\n\ndata_df[OrdinalVar].head(5)\n\n\n\n\n\n\n\n\nLot Shape\nUtilities\nLand Slope\nOverall Qual\nOverall Cond\nExter Qual\nExter Cond\nBsmt Qual\nBsmt Cond\nBsmt Exposure\nBsmtFin Type 1\nBsmtFin Type 2\nHeating QC\nElectrical\nKitchen Qual\nFunctional\nFireplace Qu\nGarage Finish\nGarage Qual\nGarage Cond\nPaved Drive\nPool QC\nFence\n\n\n\n\n0\n3\n4\n3\n7\n5\n4\n3\n4\n3\n1\n6\n1\n5\n5\n4\n8\n3\n3\n3\n3\n3\n0\n0\n\n\n1\n4\n4\n3\n6\n6\n3\n3\n3\n3\n1\n1\n1\n3\n5\n4\n8\n4\n1\n3\n3\n3\n0\n0\n\n\n2\n4\n4\n3\n4\n5\n3\n3\n3\n3\n1\n1\n1\n2\n5\n3\n8\n0\n1\n3\n3\n3\n0\n0\n\n\n3\n3\n4\n3\n5\n5\n3\n3\n3\n3\n1\n3\n1\n4\n5\n4\n8\n4\n2\n3\n3\n3\n0\n0\n\n\n4\n3\n4\n3\n7\n4\n2\n3\n3\n3\n1\n1\n1\n3\n3\n3\n8\n4\n1\n3\n3\n1\n0\n3\n\n\n\n\n\n\n\n\ndata_df[OrdinalVar].apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n\nLot Shape         True\nUtilities         True\nLand Slope        True\nOverall Qual      True\nOverall Cond      True\nExter Qual        True\nExter Cond        True\nBsmt Qual         True\nBsmt Cond         True\nBsmt Exposure     True\nBsmtFin Type 1    True\nBsmtFin Type 2    True\nHeating QC        True\nElectrical        True\nKitchen Qual      True\nFunctional        True\nFireplace Qu      True\nGarage Finish     True\nGarage Qual       True\nGarage Cond       True\nPaved Drive       True\nPool QC           True\nFence             True\ndtype: bool\n\n\nEverything seem correct.\n\n\nNominal variables\nThe idea is to create a new variable for each possible value. And to avoid redundance, I drop the first column for each “old variable”.\n\ndata_df = pd.get_dummies(data_df, columns=NominalVar, drop_first=True)\n\n\ndata_df.head()\n\n\n\n\n\n\n\n\nLot Frontage\nLot Area\nLot Shape\nUtilities\nLand Slope\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nMas Vnr Area\nExter Qual\nExter Cond\nBsmt Qual\nBsmt Cond\nBsmt Exposure\nBsmtFin Type 1\nBsmtFin SF 1\nBsmtFin Type 2\nBsmtFin SF 2\nBsmt Unf SF\nTotal Bsmt SF\nHeating QC\nElectrical\n1st Flr SF\n2nd Flr SF\nLow Qual Fin SF\nGr Liv Area\nBsmt Full Bath\nBsmt Half Bath\nFull Bath\nHalf Bath\nBedroom AbvGr\nKitchen AbvGr\nKitchen Qual\nTotRms AbvGrd\nFunctional\nFireplaces\nFireplace Qu\nGarage Yr Blt\nGarage Finish\nGarage Cars\nGarage Area\nGarage Qual\nGarage Cond\nPaved Drive\nWood Deck SF\nOpen Porch SF\nEnclosed Porch\n3Ssn Porch\nScreen Porch\nPool Area\nPool QC\nFence\nMisc Val\nMo Sold\nYr Sold\nSalePrice\nMS SubClass_b\nMS SubClass_c\nMS SubClass_d\nMS SubClass_e\nMS SubClass_f\nMS SubClass_g\nMS SubClass_h\nMS SubClass_i\nMS SubClass_j\nMS SubClass_k\nMS SubClass_l\nMS SubClass_m\nMS SubClass_n\nMS SubClass_o\nMS SubClass_p\nMS Zoning_C (all)\nMS Zoning_FV\nMS Zoning_I (all)\nMS Zoning_RH\nMS Zoning_RL\nMS Zoning_RM\nStreet_Pave\nAlley_NA\nAlley_Pave\nLand Contour_HLS\nLand Contour_Low\nLand Contour_Lvl\nLot Config_CulDSac\nLot Config_FR2\nLot Config_FR3\nLot Config_Inside\nNeighborhood_Blueste\nNeighborhood_BrDale\nNeighborhood_BrkSide\nNeighborhood_ClearCr\nNeighborhood_CollgCr\nNeighborhood_Crawfor\nNeighborhood_Edwards\nNeighborhood_Gilbert\nNeighborhood_Greens\nNeighborhood_GrnHill\nNeighborhood_IDOTRR\nNeighborhood_Landmrk\nNeighborhood_MeadowV\nNeighborhood_Mitchel\nNeighborhood_NAmes\nNeighborhood_NPkVill\nNeighborhood_NWAmes\nNeighborhood_NoRidge\nNeighborhood_NridgHt\nNeighborhood_OldTown\nNeighborhood_SWISU\nNeighborhood_Sawyer\nNeighborhood_SawyerW\nNeighborhood_Somerst\nNeighborhood_StoneBr\nNeighborhood_Timber\nNeighborhood_Veenker\nCondition 1_Feedr\nCondition 1_Norm\nCondition 1_PosA\nCondition 1_PosN\nCondition 1_RRAe\nCondition 1_RRAn\nCondition 1_RRNe\nCondition 1_RRNn\nCondition 2_Feedr\nCondition 2_Norm\nCondition 2_PosA\nCondition 2_PosN\nCondition 2_RRAe\nCondition 2_RRAn\nCondition 2_RRNn\nBldg Type_2fmCon\nBldg Type_Duplex\nBldg Type_Twnhs\nBldg Type_TwnhsE\nHouse Style_1.5Unf\nHouse Style_1Story\nHouse Style_2.5Fin\nHouse Style_2.5Unf\nHouse Style_2Story\nHouse Style_SFoyer\nHouse Style_SLvl\nRoof Style_Gable\nRoof Style_Gambrel\nRoof Style_Hip\nRoof Style_Mansard\nRoof Style_Shed\nRoof Matl_Membran\nRoof Matl_Metal\nRoof Matl_Tar&Grv\nRoof Matl_WdShake\nRoof Matl_WdShngl\nExterior 1st_AsphShn\nExterior 1st_BrkComm\nExterior 1st_BrkFace\nExterior 1st_CBlock\nExterior 1st_CemntBd\nExterior 1st_HdBoard\nExterior 1st_ImStucc\nExterior 1st_MetalSd\nExterior 1st_Plywood\nExterior 1st_PreCast\nExterior 1st_Stone\nExterior 1st_Stucco\nExterior 1st_VinylSd\nExterior 1st_Wd Sdng\nExterior 1st_WdShing\nExterior 2nd_AsphShn\nExterior 2nd_Brk Cmn\nExterior 2nd_BrkFace\nExterior 2nd_CBlock\nExterior 2nd_CmentBd\nExterior 2nd_HdBoard\nExterior 2nd_ImStucc\nExterior 2nd_MetalSd\nExterior 2nd_Plywood\nExterior 2nd_PreCast\nExterior 2nd_Stone\nExterior 2nd_Stucco\nExterior 2nd_VinylSd\nExterior 2nd_Wd Sdng\nExterior 2nd_Wd Shng\nMas Vnr Type_BrkFace\nMas Vnr Type_CBlock\nMas Vnr Type_NA\nMas Vnr Type_None\nMas Vnr Type_Stone\nFoundation_CBlock\nFoundation_PConc\nFoundation_Slab\nFoundation_Stone\nFoundation_Wood\nHeating_GasA\nHeating_GasW\nHeating_Grav\nHeating_OthW\nHeating_Wall\nCentral Air_Y\nGarage Type_Attchd\nGarage Type_Basment\nGarage Type_BuiltIn\nGarage Type_CarPort\nGarage Type_Detchd\nGarage Type_NA\nMisc Feature_NA\nMisc Feature_Othr\nMisc Feature_Shed\nMisc Feature_TenC\nSale Type_CWD\nSale Type_Con\nSale Type_ConLD\nSale Type_ConLI\nSale Type_ConLw\nSale Type_New\nSale Type_Oth\nSale Type_VWD\nSale Type_WD\nSale Condition_AdjLand\nSale Condition_Alloca\nSale Condition_Family\nSale Condition_Normal\nSale Condition_Partial\n\n\n\n\n0\n68.0\n8795\n3\n4\n3\n7\n5\n2000\n2000\n0.0\n4\n3\n4\n3\n1\n6\n300.0\n1\n0.0\n652.0\n952.0\n5\n5\n980\n1276\n0\n2256\n0.0\n0.0\n2\n1\n4\n1\n4\n8\n8\n1\n3\n2000.0\n3\n2.0\n554.0\n3\n3\n3\n224\n54\n0\n0\n0\n0\n0\n0\n0\n4\n2009\n236000\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n1\n75.0\n10170\n4\n4\n3\n6\n6\n1951\n1951\n522.0\n3\n3\n3\n3\n1\n1\n0.0\n1\n0.0\n216.0\n216.0\n3\n5\n1575\n0\n0\n1575\n0.0\n0.0\n1\n1\n2\n1\n4\n5\n8\n1\n4\n1951.0\n1\n2.0\n400.0\n3\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n2006\n155000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n2\n21.0\n2001\n4\n4\n3\n4\n5\n1970\n1970\n80.0\n3\n3\n3\n3\n1\n1\n0.0\n1\n0.0\n546.0\n546.0\n2\n5\n546\n546\n0\n1092\n0.0\n0.0\n1\n1\n3\n1\n3\n6\n8\n0\n0\n1970.0\n1\n1.0\n286.0\n3\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2007\n75000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n3\n70.0\n10552\n3\n4\n3\n5\n5\n1959\n1959\n0.0\n3\n3\n3\n3\n1\n3\n1018.0\n1\n0.0\n380.0\n1398.0\n4\n5\n1700\n0\n0\n1700\n0.0\n1.0\n1\n1\n4\n1\n4\n6\n8\n1\n4\n1959.0\n2\n2.0\n447.0\n3\n3\n3\n0\n38\n0\n0\n0\n0\n0\n0\n0\n4\n2010\n165500\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n60.0\n10120\n3\n4\n3\n7\n4\n1910\n1950\n0.0\n2\n3\n3\n3\n1\n1\n0.0\n1\n0.0\n925.0\n925.0\n3\n3\n964\n925\n0\n1889\n0.0\n0.0\n1\n1\n4\n2\n3\n9\n8\n1\n4\n1960.0\n1\n1.0\n308.0\n3\n3\n1\n0\n0\n264\n0\n0\n0\n0\n3\n0\n1\n2007\n122000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\npd.options.display.max_rows = 400\n\n\ndata_df.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n\nLot Frontage              True\nLot Area                  True\nLot Shape                 True\nUtilities                 True\nLand Slope                True\nOverall Qual              True\nOverall Cond              True\nYear Built                True\nYear Remod/Add            True\nMas Vnr Area              True\nExter Qual                True\nExter Cond                True\nBsmt Qual                 True\nBsmt Cond                 True\nBsmt Exposure             True\nBsmtFin Type 1            True\nBsmtFin SF 1              True\nBsmtFin Type 2            True\nBsmtFin SF 2              True\nBsmt Unf SF               True\nTotal Bsmt SF             True\nHeating QC                True\nElectrical                True\n1st Flr SF                True\n2nd Flr SF                True\nLow Qual Fin SF           True\nGr Liv Area               True\nBsmt Full Bath            True\nBsmt Half Bath            True\nFull Bath                 True\nHalf Bath                 True\nBedroom AbvGr             True\nKitchen AbvGr             True\nKitchen Qual              True\nTotRms AbvGrd             True\nFunctional                True\nFireplaces                True\nFireplace Qu              True\nGarage Yr Blt             True\nGarage Finish             True\nGarage Cars               True\nGarage Area               True\nGarage Qual               True\nGarage Cond               True\nPaved Drive               True\nWood Deck SF              True\nOpen Porch SF             True\nEnclosed Porch            True\n3Ssn Porch                True\nScreen Porch              True\nPool Area                 True\nPool QC                   True\nFence                     True\nMisc Val                  True\nMo Sold                   True\nYr Sold                   True\nSalePrice                 True\nMS SubClass_b             True\nMS SubClass_c             True\nMS SubClass_d             True\nMS SubClass_e             True\nMS SubClass_f             True\nMS SubClass_g             True\nMS SubClass_h             True\nMS SubClass_i             True\nMS SubClass_j             True\nMS SubClass_k             True\nMS SubClass_l             True\nMS SubClass_m             True\nMS SubClass_n             True\nMS SubClass_o             True\nMS SubClass_p             True\nMS Zoning_C (all)         True\nMS Zoning_FV              True\nMS Zoning_I (all)         True\nMS Zoning_RH              True\nMS Zoning_RL              True\nMS Zoning_RM              True\nStreet_Pave               True\nAlley_NA                  True\nAlley_Pave                True\nLand Contour_HLS          True\nLand Contour_Low          True\nLand Contour_Lvl          True\nLot Config_CulDSac        True\nLot Config_FR2            True\nLot Config_FR3            True\nLot Config_Inside         True\nNeighborhood_Blueste      True\nNeighborhood_BrDale       True\nNeighborhood_BrkSide      True\nNeighborhood_ClearCr      True\nNeighborhood_CollgCr      True\nNeighborhood_Crawfor      True\nNeighborhood_Edwards      True\nNeighborhood_Gilbert      True\nNeighborhood_Greens       True\nNeighborhood_GrnHill      True\nNeighborhood_IDOTRR       True\nNeighborhood_Landmrk      True\nNeighborhood_MeadowV      True\nNeighborhood_Mitchel      True\nNeighborhood_NAmes        True\nNeighborhood_NPkVill      True\nNeighborhood_NWAmes       True\nNeighborhood_NoRidge      True\nNeighborhood_NridgHt      True\nNeighborhood_OldTown      True\nNeighborhood_SWISU        True\nNeighborhood_Sawyer       True\nNeighborhood_SawyerW      True\nNeighborhood_Somerst      True\nNeighborhood_StoneBr      True\nNeighborhood_Timber       True\nNeighborhood_Veenker      True\nCondition 1_Feedr         True\nCondition 1_Norm          True\nCondition 1_PosA          True\nCondition 1_PosN          True\nCondition 1_RRAe          True\nCondition 1_RRAn          True\nCondition 1_RRNe          True\nCondition 1_RRNn          True\nCondition 2_Feedr         True\nCondition 2_Norm          True\nCondition 2_PosA          True\nCondition 2_PosN          True\nCondition 2_RRAe          True\nCondition 2_RRAn          True\nCondition 2_RRNn          True\nBldg Type_2fmCon          True\nBldg Type_Duplex          True\nBldg Type_Twnhs           True\nBldg Type_TwnhsE          True\nHouse Style_1.5Unf        True\nHouse Style_1Story        True\nHouse Style_2.5Fin        True\nHouse Style_2.5Unf        True\nHouse Style_2Story        True\nHouse Style_SFoyer        True\nHouse Style_SLvl          True\nRoof Style_Gable          True\nRoof Style_Gambrel        True\nRoof Style_Hip            True\nRoof Style_Mansard        True\nRoof Style_Shed           True\nRoof Matl_Membran         True\nRoof Matl_Metal           True\nRoof Matl_Tar&Grv         True\nRoof Matl_WdShake         True\nRoof Matl_WdShngl         True\nExterior 1st_AsphShn      True\nExterior 1st_BrkComm      True\nExterior 1st_BrkFace      True\nExterior 1st_CBlock       True\nExterior 1st_CemntBd      True\nExterior 1st_HdBoard      True\nExterior 1st_ImStucc      True\nExterior 1st_MetalSd      True\nExterior 1st_Plywood      True\nExterior 1st_PreCast      True\nExterior 1st_Stone        True\nExterior 1st_Stucco       True\nExterior 1st_VinylSd      True\nExterior 1st_Wd Sdng      True\nExterior 1st_WdShing      True\nExterior 2nd_AsphShn      True\nExterior 2nd_Brk Cmn      True\nExterior 2nd_BrkFace      True\nExterior 2nd_CBlock       True\nExterior 2nd_CmentBd      True\nExterior 2nd_HdBoard      True\nExterior 2nd_ImStucc      True\nExterior 2nd_MetalSd      True\nExterior 2nd_Plywood      True\nExterior 2nd_PreCast      True\nExterior 2nd_Stone        True\nExterior 2nd_Stucco       True\nExterior 2nd_VinylSd      True\nExterior 2nd_Wd Sdng      True\nExterior 2nd_Wd Shng      True\nMas Vnr Type_BrkFace      True\nMas Vnr Type_CBlock       True\nMas Vnr Type_NA           True\nMas Vnr Type_None         True\nMas Vnr Type_Stone        True\nFoundation_CBlock         True\nFoundation_PConc          True\nFoundation_Slab           True\nFoundation_Stone          True\nFoundation_Wood           True\nHeating_GasA              True\nHeating_GasW              True\nHeating_Grav              True\nHeating_OthW              True\nHeating_Wall              True\nCentral Air_Y             True\nGarage Type_Attchd        True\nGarage Type_Basment       True\nGarage Type_BuiltIn       True\nGarage Type_CarPort       True\nGarage Type_Detchd        True\nGarage Type_NA            True\nMisc Feature_NA           True\nMisc Feature_Othr         True\nMisc Feature_Shed         True\nMisc Feature_TenC         True\nSale Type_CWD             True\nSale Type_Con             True\nSale Type_ConLD           True\nSale Type_ConLI           True\nSale Type_ConLw           True\nSale Type_New             True\nSale Type_Oth             True\nSale Type_VWD             True\nSale Type_WD              True\nSale Condition_AdjLand    True\nSale Condition_Alloca     True\nSale Condition_Family     True\nSale Condition_Normal     True\nSale Condition_Partial    True\ndtype: bool\n\n\n\n\n\n3. Feature engineering\n\nSalePrice\n\ndata_df.hist(column='SalePrice', bins=20, grid=False, xrot=45,density=True)\nmu = np.mean(data_df['SalePrice'])\nvariance = np.var(data_df['SalePrice'])\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.show()\nprint('Skewness=', stats.skew(data_df['SalePrice']))\n\n\n\n\n\n\n\n\nSkewness= 1.6047157543103137\n\n\nAs we have seen in the course before (Regularization chapter), “most of the houses in the data set have a price between 100 and 300 thousand dollars. However, there are a few expensive houses with prices well above that. In practice, this can lead to biased models that favor accurate predictions of expensive houses. To avoid building a model that is biased toward more expensive houses, we will apply the logarithm transformation and build a model that predicts the log of the sale price.”\n\nplt.hist(np.log10(data_df.SalePrice), bins=20,density=True)\nmu = np.mean(np.log10(data_df['SalePrice']))\nvariance = np.var(np.log10(data_df['SalePrice']))\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.xlabel('log10(SalePrice)')\nplt.show()\nprint('Skewness=', stats.skew(np.log(data_df['SalePrice'])))\n\n\n\n\n\n\n\n\nSkewness= -0.09232378345258598\n\n\n\ndata_df['log_SalePrice'] =np.log1p(data_df['SalePrice'])\n\n\n\nFeatures\nFirst, let’s have a look at the general shape of all the features in order to have a good overview.\n\nConitnuous variables:\n\n\nlen(ContinuousVar)\n\n19\n\n\n\nfig,axes = plt.subplots(nrows=5,ncols=4,figsize=(16,20))\n\nfor col,ax in zip(ContinuousVar,axes.ravel()):\n    if col in NoZerosCols:\n        mu = np.mean(data_df[data_df[col]!=0][col])\n        variance = np.var(data_df[data_df[col]!=0][col])\n        sigma = math.sqrt(variance)\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.hist(data_df[data_df[col]!=0][col],density=1) \n        ax.plot(x, stats.norm.pdf(x, mu, sigma))\n        ax.set_title(col +str(' / Skew= ')+ str(round(stats.skew(data_df[data_df[col]!=0][col]),1) ) )\n        \n    else:\n        mu = np.mean(data_df[col])\n        variance = np.var(data_df[col])\n        sigma = math.sqrt(variance)\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.hist(data_df[col],density=1)\n        ax.plot(x, stats.norm.pdf(x, mu, sigma))\n        ax.set_title(col +str(' / Skew= ')+ str(round(stats.skew(data_df[col]),1) ))\n    \n\nplt.tight_layout();\n\n\n\n\n\n\n\n\nWe can see that almost all continuous variables are also skewed. Therefore, I also apply a logarithmic transformation to counter this problem:\n\nfig,axes = plt.subplots(nrows=5,ncols=4,figsize=(16,20))\n\nfor col,ax in zip(ContinuousVar,axes.ravel()):\n    if col in NoZerosCols:\n        mu = np.mean(np.log(data_df[data_df[col]!=0][col]))\n        variance = np.var(np.log(data_df[data_df[col]!=0][col]))\n        sigma = math.sqrt(variance)\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.hist(np.log(data_df[data_df[col]!=0][col]),density=1) \n        ax.plot(x, stats.norm.pdf(x, mu, sigma))\n        ax.set_title(col +str(' / Skew= ')+ str(round(stats.skew(np.log(data_df[data_df[col]!=0][col])),1) ) )\n    elif col =='Low Qual Fin SF':\n        mu = np.mean(np.log1p(data_df[col]))\n        variance = np.var(np.log1p(data_df[col]))\n        sigma = math.sqrt(variance)\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.hist(np.log1p(data_df[col]),density=1)\n        ax.plot(x, stats.norm.pdf(x, mu, sigma))\n        ax.set_title(col +str(' / Skew= ')+ str(round(stats.skew(np.log1p(data_df[col])),1) ))    \n    else:\n        mu = np.mean(np.log(data_df[col]))\n        variance = np.var(np.log(data_df[col]))\n        sigma = math.sqrt(variance)\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.hist(np.log(data_df[col]),density=1)\n        ax.plot(x, stats.norm.pdf(x, mu, sigma))\n        ax.set_title(col +str(' / Skew= ')+ str(round(stats.skew(np.log(data_df[col])),1) ))\n    \n\nplt.tight_layout();\n\n\n\n\n\n\n\n\nAs mentioned in the solution of the Regularization exercise, I have to add 1 to the log for the 0 values. Otherwise, I get this error: “ValueError: supplied range of [-inf, 6.96979066990159] is not finite” for the column “Low Qual Fin SF”.\n\ndata_df[ContinuousVar]=np.log1p(data_df[ContinuousVar])\n\n\ndata_df = data_df.rename(columns={'Lot Frontage':'log_Lot Frontage',\n 'Lot Area':'log_Lot Area',\n 'Mas Vnr Area':'log_Mas Vnr Area',\n 'BsmtFin SF 1':'log_BsmtFin SF 1',\n 'BsmtFin SF 2':'log_BsmtFin SF 2',\n 'Bsmt Unf SF':'log_Bsmt Unf SF',\n 'Total Bsmt SF':'log_Total Bsmt SF',\n '1st Flr SF':'log_1st Flr SF',\n '2nd Flr SF':'log_2nd Flr SF',\n 'Low Qual Fin SF':'log_Low Qual Fin SF',\n 'Gr Liv Area':'log_Gr Liv Area',\n 'Garage Area':'log_Garage Area',\n 'Wood Deck SF':'log_Wood Deck SF',\n 'Open Porch SF':'log_Open Porch SF',\n 'Enclosed Porch':'log_Enclosed Porch',\n '3Ssn Porch':'log_3Ssn Porch',\n 'Screen Porch':'log_Screen Porch',\n 'Pool Area':'log_Pool Area',\n 'Misc Val':'log_Misc Val'})\n\n\ndata_df.head(1)\n\n\n\n\n\n\n\n\nlog_Lot Frontage\nlog_Lot Area\nLot Shape\nUtilities\nLand Slope\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nlog_Mas Vnr Area\nExter Qual\nExter Cond\nBsmt Qual\nBsmt Cond\nBsmt Exposure\nBsmtFin Type 1\nlog_BsmtFin SF 1\nBsmtFin Type 2\nlog_BsmtFin SF 2\nlog_Bsmt Unf SF\nlog_Total Bsmt SF\nHeating QC\nElectrical\nlog_1st Flr SF\nlog_2nd Flr SF\nlog_Low Qual Fin SF\nlog_Gr Liv Area\nBsmt Full Bath\nBsmt Half Bath\nFull Bath\nHalf Bath\nBedroom AbvGr\nKitchen AbvGr\nKitchen Qual\nTotRms AbvGrd\nFunctional\nFireplaces\nFireplace Qu\nGarage Yr Blt\nGarage Finish\nGarage Cars\nlog_Garage Area\nGarage Qual\nGarage Cond\nPaved Drive\nlog_Wood Deck SF\nlog_Open Porch SF\nlog_Enclosed Porch\nlog_3Ssn Porch\nlog_Screen Porch\nlog_Pool Area\nPool QC\nFence\nlog_Misc Val\nMo Sold\nYr Sold\nSalePrice\nMS SubClass_b\nMS SubClass_c\nMS SubClass_d\nMS SubClass_e\nMS SubClass_f\nMS SubClass_g\nMS SubClass_h\nMS SubClass_i\nMS SubClass_j\nMS SubClass_k\nMS SubClass_l\nMS SubClass_m\nMS SubClass_n\nMS SubClass_o\nMS SubClass_p\nMS Zoning_C (all)\nMS Zoning_FV\nMS Zoning_I (all)\nMS Zoning_RH\nMS Zoning_RL\nMS Zoning_RM\nStreet_Pave\nAlley_NA\nAlley_Pave\nLand Contour_HLS\nLand Contour_Low\nLand Contour_Lvl\nLot Config_CulDSac\nLot Config_FR2\nLot Config_FR3\nLot Config_Inside\nNeighborhood_Blueste\nNeighborhood_BrDale\nNeighborhood_BrkSide\nNeighborhood_ClearCr\nNeighborhood_CollgCr\nNeighborhood_Crawfor\nNeighborhood_Edwards\nNeighborhood_Gilbert\nNeighborhood_Greens\nNeighborhood_GrnHill\nNeighborhood_IDOTRR\nNeighborhood_Landmrk\nNeighborhood_MeadowV\nNeighborhood_Mitchel\nNeighborhood_NAmes\nNeighborhood_NPkVill\nNeighborhood_NWAmes\nNeighborhood_NoRidge\nNeighborhood_NridgHt\nNeighborhood_OldTown\nNeighborhood_SWISU\nNeighborhood_Sawyer\nNeighborhood_SawyerW\nNeighborhood_Somerst\nNeighborhood_StoneBr\nNeighborhood_Timber\nNeighborhood_Veenker\nCondition 1_Feedr\nCondition 1_Norm\nCondition 1_PosA\nCondition 1_PosN\nCondition 1_RRAe\nCondition 1_RRAn\nCondition 1_RRNe\nCondition 1_RRNn\nCondition 2_Feedr\nCondition 2_Norm\nCondition 2_PosA\nCondition 2_PosN\nCondition 2_RRAe\nCondition 2_RRAn\nCondition 2_RRNn\nBldg Type_2fmCon\nBldg Type_Duplex\nBldg Type_Twnhs\nBldg Type_TwnhsE\nHouse Style_1.5Unf\nHouse Style_1Story\nHouse Style_2.5Fin\nHouse Style_2.5Unf\nHouse Style_2Story\nHouse Style_SFoyer\nHouse Style_SLvl\nRoof Style_Gable\nRoof Style_Gambrel\nRoof Style_Hip\nRoof Style_Mansard\nRoof Style_Shed\nRoof Matl_Membran\nRoof Matl_Metal\nRoof Matl_Tar&Grv\nRoof Matl_WdShake\nRoof Matl_WdShngl\nExterior 1st_AsphShn\nExterior 1st_BrkComm\nExterior 1st_BrkFace\nExterior 1st_CBlock\nExterior 1st_CemntBd\nExterior 1st_HdBoard\nExterior 1st_ImStucc\nExterior 1st_MetalSd\nExterior 1st_Plywood\nExterior 1st_PreCast\nExterior 1st_Stone\nExterior 1st_Stucco\nExterior 1st_VinylSd\nExterior 1st_Wd Sdng\nExterior 1st_WdShing\nExterior 2nd_AsphShn\nExterior 2nd_Brk Cmn\nExterior 2nd_BrkFace\nExterior 2nd_CBlock\nExterior 2nd_CmentBd\nExterior 2nd_HdBoard\nExterior 2nd_ImStucc\nExterior 2nd_MetalSd\nExterior 2nd_Plywood\nExterior 2nd_PreCast\nExterior 2nd_Stone\nExterior 2nd_Stucco\nExterior 2nd_VinylSd\nExterior 2nd_Wd Sdng\nExterior 2nd_Wd Shng\nMas Vnr Type_BrkFace\nMas Vnr Type_CBlock\nMas Vnr Type_NA\nMas Vnr Type_None\nMas Vnr Type_Stone\nFoundation_CBlock\nFoundation_PConc\nFoundation_Slab\nFoundation_Stone\nFoundation_Wood\nHeating_GasA\nHeating_GasW\nHeating_Grav\nHeating_OthW\nHeating_Wall\nCentral Air_Y\nGarage Type_Attchd\nGarage Type_Basment\nGarage Type_BuiltIn\nGarage Type_CarPort\nGarage Type_Detchd\nGarage Type_NA\nMisc Feature_NA\nMisc Feature_Othr\nMisc Feature_Shed\nMisc Feature_TenC\nSale Type_CWD\nSale Type_Con\nSale Type_ConLD\nSale Type_ConLI\nSale Type_ConLw\nSale Type_New\nSale Type_Oth\nSale Type_VWD\nSale Type_WD\nSale Condition_AdjLand\nSale Condition_Alloca\nSale Condition_Family\nSale Condition_Normal\nSale Condition_Partial\nlog_SalePrice\n\n\n\n\n0\n4.234107\n9.082052\n3\n4\n3\n7\n5\n2000\n2000\n0.0\n4\n3\n4\n3\n1\n6\n5.70711\n1\n0.0\n6.481577\n6.859615\n5\n5\n6.888572\n7.152269\n0.0\n7.721792\n0.0\n0.0\n2\n1\n4\n1\n4\n8\n8\n1\n3\n2000.0\n3\n2.0\n6.318968\n3\n3\n3\n5.4161\n4.007333\n0.0\n0.0\n0.0\n0.0\n0\n0\n0.0\n4\n2009\n236000\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n12.371591\n\n\n\n\n\n\n\nNow let’s plot these variables with respect to the sale price.\n\nfig,axes = plt.subplots(nrows=5,ncols=4,figsize=(16,20))\n\nfor col,ax in zip(ContinuousVar,axes.ravel()):\n    ax.scatter(data_df[str('log_')+col],data_df['SalePrice'])\n    ax.set_title(col+str(' Versus SalePrice'))\n\n\n\n\n\n\n\n\nFrom what I see, we could maybe create the following dummy variables: - 0 if no basement, 1 otherwise - 0 if no pool, 1 otherwise - 0 if no masonry, 1 otherwise -( Garage is handled in next section)\n\ndata_df['Basement'] = (data_df['log_Total Bsmt SF'] !=0).astype(int)\ndata_df['Pool'] = (data_df['log_Pool Area'] !=0).astype(int)\ndata_df['Mas Vnr Area'] = (data_df['log_Mas Vnr Area'] !=0).astype(int)\n\n\nDiscrete variables:\n\n\nlen(DiscreteVar)\n\n14\n\n\nAs I did it before for continuous before, let’s plot the discrete variables to have an overview of the data.\n\nfig,axes = plt.subplots(nrows=4,ncols=4,figsize=(20,20))\n\nfor col,ax in zip(DiscreteVar,axes.ravel()):\n    ax.scatter(data_df[col],data_df['SalePrice'])\n    ax.set_title(col+str(' Versus SalePrice'))\n\n\n\n\n\n\n\n\nFrom what I see, we could maybe create the following dummy variables: - 0 if YearBuilt below threshold, 1 otherwise - 0 if Garage Cars = 0 (no garage), 1 otherwise\n\nYearBuiltBelow1980 = data_df[data_df['Year Built']&lt;1980]\nYearBuiltAbove1980 = data_df[data_df['Year Built']&gt;=1980]\n\nfig, ax = plt.subplots(figsize = (8,8))\nYearBuiltBelow1980['SalePrice'].plot(kind=\"hist\",density=True,bins=30,color='orange',alpha=0.2,label='&lt;1980')\nYearBuiltAbove1980['SalePrice'].plot(kind=\"hist\",density=True,bins=30,color='blue',alpha=0.2,label='&gt;1980')\nax.legend(loc='upper right')\nYearBuiltBelow1980['SalePrice'].plot(kind=\"kde\",color='orange')\nYearBuiltAbove1980['SalePrice'].plot(kind=\"kde\",color='blue')\nax.set_xlim(0, 600000)\nax.set_title('Sale Price distribution depending of the Sold Year')\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_df['Year Built Above 1980'] = (data_df['Year Built'] &gt;= 1980).astype(int)\ndata_df['Year Built Below 1980'] = (data_df['Year Built'] &lt; 1980).astype(int)\n\n\ndata_df[['Year Built','Year Built Above 1980','Year Built Below 1980']].head(3)\n\n\n\n\n\n\n\n\nYear Built\nYear Built Above 1980\nYear Built Below 1980\n\n\n\n\n0\n2000\n1\n0\n\n\n1\n1951\n0\n1\n\n\n2\n1970\n0\n1\n\n\n\n\n\n\n\n\nprint(str('The mean of the sale price for houses without garage is:'), round(np.mean(data_df[data_df['Garage Cars'] == 0]['SalePrice'])))\nprint(str('The mean of the sale price for houses with a garage is:'), round(np.mean(data_df[data_df['Garage Cars'] != 0]['SalePrice'])))\n\nThe mean of the sale price for houses without garage is: 105693.0\nThe mean of the sale price for houses with a garage is: 184158.0\n\n\n\ndata_df['House without Garage'] = (data_df['Garage Cars'] == 0).astype(int)\ndata_df['House with Garage'] = (data_df['Garage Cars'] != 0).astype(int)\n\n\ndata_df[['House with Garage','House without Garage']].head(3)\n\n\n\n\n\n\n\n\nHouse with Garage\nHouse without Garage\n\n\n\n\n0\n1\n0\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n\n\n\n\n\n\nNominal variables:\n\nNominal variables have already been encoded with dummy variabbles. (see section above)\n\nOrdinal variables:\n\n\nlen(OrdinalVar)\n\n23\n\n\n\nfig,axes = plt.subplots(nrows=6,ncols=4,figsize=(25,30))\nfor col,axis in zip(OrdinalVar,axes.ravel()):\n    g = sns.catplot(x=col,y='SalePrice',data=data_df,ax=axis,kind='box')\n    plt.close()\n\n\n\n\n\n\n\n\nI think these ordinal variables are fine like that. They do not need a particular encoding. However, this grid shows us big differences in the quality variables (Overall Qual, Ext Qual etc.).\n\n\n\n4. Model fitting\n\nSimple model:\n\nFor this model, I take as features the two most correlated variables with the SalePrice variable. Taking these variables make sense since it should indirectly lead to better results.\n\ndata_df.corr()['log_SalePrice'].sort_values(ascending=False).head(13)\n\nlog_SalePrice            1.000000\nSalePrice                0.947413\nOverall Qual             0.828522\nlog_Gr Liv Area          0.721594\nGarage Cars              0.670633\nKitchen Qual             0.661810\nBsmt Qual                0.638555\nlog_1st Flr SF           0.620696\nYear Built Above 1980    0.619973\nYear Built               0.619094\nGarage Finish            0.592214\nYear Remod/Add           0.582110\nFull Bath                0.569508\nName: log_SalePrice, dtype: float64\n\n\n\nX_simple = data_df[['Overall Qual','log_Gr Liv Area']]\ny_simple = data_df['log_SalePrice']\n\n\nX_tr_simple, X_te_simple, y_tr_simple, y_te_simple = train_test_split(X_simple, y_simple, test_size=0.5, random_state=0)\n\nFirst, let’s create the baseline.\n\ndef MAE(y, y_pred):\n    return np.mean(np.abs(y - y_pred))\n\n\nmae_baseline = MAE(np.exp(y_te_simple), np.exp(np.median(y_tr_simple)))\nprint('MAE Baseline: {:.5f}'.format(mae_baseline))\n\nMAE Baseline: 54856.93064\n\n\nAs stated before, I use the Huber regressor since I did not take out potential outliers.\n\nhuber = HuberRegressor(epsilon=1.45)\nhuber.fit(X_tr_simple, y_tr_simple) \ny_pred_huber_simple = huber.predict(X_te_simple)\n\nmae_huber = MAE(np.exp(y_te_simple), np.exp(y_pred_huber_simple))\nprint('MAE Huber: {:.5f}'.format(mae_huber))\n\nMAE Huber: 24722.31174\n\n\n\nplt.bar([1, 2, ], [mae_baseline, mae_huber])\nplt.xticks([1, 2,], ['MAE Baseline', 'MAE Huber'])\nplt.show()\n\n\n\n\n\n\n\n\n\nIntermediate model:\n\n\nX_intermediate = data_df[['Overall Qual','log_Gr Liv Area','Garage Cars', 'Kitchen Qual', 'Bsmt Qual','log_1st Flr SF','Year Built Above 1980','Garage Finish',\n    'Year Remod/Add','Full Bath']]\ny_intermediate = data_df['log_SalePrice']\n\n\nX_tr_intermediate, X_te_intermediate, y_tr_intermediate, y_te_intermediate = train_test_split(X_intermediate, y_intermediate, test_size=0.5, random_state=0)\n\n\nhuber = HuberRegressor(epsilon=1.45)\nhuber.fit(X_tr_intermediate, y_tr_intermediate) \ny_pred_huber_intermediate = huber.predict(X_te_intermediate)\n\nmae_huber2 = MAE(np.exp(y_te_intermediate), np.exp(y_pred_huber_intermediate))\nprint('MAE Huber: {:.5f}'.format(mae_huber2))\n\nMAE Huber: 19353.02712\n\n\n\nplt.bar([1, 2, 3], [mae_baseline, mae_huber,mae_huber2])\nplt.xticks([1, 2,3], ['MAE Baseline', 'MAE Huber 2 var','MAE Huber 10 var'])\nplt.show()\n\n\n\n\n\n\n\n\n\nComplex model:\n\n\nX_complex = data_df.drop(['SalePrice','log_SalePrice'],axis=1)\ny_complex = data_df['log_SalePrice']\n\n\nX_tr_complex, X_te_complex, y_tr_complex, y_te_complex = train_test_split(X_complex, y_complex, test_size=0.5, random_state=0)\n\n\nhuber = HuberRegressor(epsilon=1.45)\nhuber.fit(X_tr_complex, y_tr_complex) \ny_pred_huber_complex = huber.predict(X_te_complex)\n\nmae_huber3 = MAE(np.exp(y_te_complex), np.exp(y_pred_huber_complex))\nprint('MAE Huber: {:.5f}'.format(mae_huber3))\n\nMAE Huber: 38596.51255\n\n\n\nplt.figure(figsize=(10,6))\nplt.bar([1, 2, 3,4], [mae_baseline, mae_huber,mae_huber2, mae_huber3],align='center')\nplt.xticks([1, 2,3,4], ['MAE Baseline', 'MAE Huber 2 var','MAE Huber 10 var','MAE Huber All var'])\nplt.show()\n\n\n\n\n\n\n\n\nThe best model is the one with 10 variables which is slightly better than the one with only 2 variables.\n\n\n5. Regularization\nWe first standardize our data.\n\nscaler = StandardScaler()\nX_tr_rescaled = scaler.fit_transform(X_tr_complex)\nX_te_rescaled = scaler.transform(X_te_complex)\n\nThen, we can apply our ridge regression and plot the validation curve.\n\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.linear_model import Ridge\n\ngs_results = []\n\nfor alpha in np.logspace(-1, 4, num=20):\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_tr_rescaled, y_tr_complex)\n    \n    gs_results.append({\n        'model': ridge,\n        'alpha': alpha,\n        'train_mse': MSE(y_tr_complex, ridge.predict(X_tr_rescaled)),\n        'train_mae': MAE(np.exp(y_tr_complex), np.exp(ridge.predict(X_tr_rescaled))),\n        'test_mse': MSE(y_te_complex, ridge.predict(X_te_rescaled)),\n        'test_mae': MAE(np.exp(y_te_complex), np.exp(ridge.predict(X_te_rescaled))),\n    })\n\ngs_results = pd.DataFrame(gs_results)\n\nplt.plot(np.log10(gs_results['alpha']), gs_results['train_mse'], label='train curve')\nplt.plot(np.log10(gs_results['alpha']), gs_results['test_mse'], label='test curve')\n\nbest_result = gs_results.loc[gs_results.test_mse.idxmin()]\nplt.scatter(np.log10(best_result.alpha), best_result.test_mse, marker='x', c='red', zorder=10)\nplt.title('Best alpha: {:.1e} - mse: {:.4f} mae: {:,.0f}\n.format(\n    best_result.alpha, best_result.test_mse, best_result.test_mae))\n\nplt.xlabel('$log_{10}(alpha)\n)\nplt.ylabel('MSE')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nbest_result\n\nalpha                                                   78.476\nmodel        Ridge(alpha=78.47599703514607, copy_X=True, fi...\ntest_mae                                               14061.5\ntest_mse                                             0.0145549\ntrain_mae                                              12342.5\ntrain_mse                                            0.0106063\nName: 11, dtype: object\n\n\n\nplt.figure(figsize=(10,6))\nplt.bar([1, 2, 3,4,5], [mae_baseline, mae_huber,mae_huber2, mae_huber3, best_result.test_mae],align='center')\nplt.xticks([1, 2,3,4,5], ['MAE Baseline', 'MAE Huber 2 var','MAE Huber 10 var','MAE Huber All var','MAE Ridge All var'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6. Communicating the results\nAs we can see on the barplot above, the best model in terms of MAE in the complex one with the ridge regression. This model has a MAE of 14’062$, which seems to be a good result. We can really see the importance of the ridge regression. It improves a lot the model and allows to fight against overfitting of the complex model (the one including all variables).\n\n\n7. Compute predictions\n\ntest_df = pd.read_csv(\"/Users/gregoireurvoy/Documents/ML/Course 3/09. Course project/Project/house-prices-test.csv\")\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\nUtilities\nLot Config\nLand Slope\nNeighborhood\nCondition 1\nCondition 2\nBldg Type\nHouse Style\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nRoof Style\nRoof Matl\nExterior 1st\nExterior 2nd\nMas Vnr Type\nMas Vnr Area\nExter Qual\nExter Cond\nFoundation\nBsmt Qual\nBsmt Cond\nBsmt Exposure\nBsmtFin Type 1\nBsmtFin SF 1\nBsmtFin Type 2\nBsmtFin SF 2\nBsmt Unf SF\nTotal Bsmt SF\nHeating\nHeating QC\nCentral Air\nElectrical\n1st Flr SF\n2nd Flr SF\nLow Qual Fin SF\nGr Liv Area\nBsmt Full Bath\nBsmt Half Bath\nFull Bath\nHalf Bath\nBedroom AbvGr\nKitchen AbvGr\nKitchen Qual\nTotRms AbvGrd\nFunctional\nFireplaces\nFireplace Qu\nGarage Type\nGarage Yr Blt\nGarage Finish\nGarage Cars\nGarage Area\nGarage Qual\nGarage Cond\nPaved Drive\nWood Deck SF\nOpen Porch SF\nEnclosed Porch\n3Ssn Porch\nScreen Porch\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\n\n\n\n\n0\n2217\n909279080\n50\nRL\nNaN\n11275\nPave\nNaN\nIR1\nHLS\nAllPub\nCorner\nMod\nCrawfor\nNorm\nNorm\n1Fam\n1.5Fin\n6\n7\n1932\n1950\nGable\nCompShg\nMetalSd\nMetalSd\nBrkFace\n480.0\nTA\nTA\nCBlock\nTA\nTA\nMn\nRec\n297.0\nLwQ\n557.0\n0.0\n854.0\nGasA\nTA\nY\nSBrkr\n1096\n895\n0\n1991\n0.0\n0.0\n1\n1\n3\n1\nTA\n7\nTyp\n1\nGd\nDetchd\n1977.0\nUnf\n2.0\n432.0\nTA\nFa\nY\n0\n0\n19\n0\n0\n0\nNaN\nNaN\nNaN\n0\n3\n2007\nWD\nNormal\n\n\n1\n837\n907126050\n20\nRL\n65.0\n9757\nPave\nNaN\nReg\nLow\nAllPub\nInside\nMod\nCollgCr\nNorm\nNorm\n1Fam\n1Story\n5\n7\n1994\n1994\nGable\nCompShg\nVinylSd\nVinylSd\nNone\n0.0\nTA\nGd\nPConc\nTA\nTA\nNo\nALQ\n755.0\nUnf\n0.0\n235.0\n990.0\nGasA\nEx\nY\nSBrkr\n990\n0\n0\n990\n1.0\n0.0\n1\n0\n3\n1\nTA\n5\nTyp\n0\nNaN\nAttchd\n1995.0\nRFn\n1.0\n440.0\nTA\nTA\nY\n66\n0\n0\n0\n92\n0\nNaN\nNaN\nNaN\n0\n10\n2009\nWD\nNormal\n\n\n2\n2397\n528144030\n60\nRL\n86.0\n11065\nPave\nNaN\nIR1\nLvl\nAllPub\nInside\nGtl\nNridgHt\nNorm\nNorm\n1Fam\n2Story\n8\n5\n2006\n2006\nGable\nCompShg\nVinylSd\nVinylSd\nStone\n788.0\nGd\nTA\nPConc\nGd\nTA\nMn\nUnf\n0.0\nUnf\n0.0\n1085.0\n1085.0\nGasA\nEx\nY\nSBrkr\n1120\n850\n0\n1970\n0.0\n0.0\n2\n1\n3\n1\nEx\n8\nTyp\n1\nGd\nBuiltIn\n2006.0\nFin\n3.0\n753.0\nTA\nTA\nY\n177\n74\n0\n0\n0\n0\nNaN\nNaN\nNaN\n0\n10\n2006\nNew\nPartial\n\n\n3\n1963\n535452060\n20\nRL\n70.0\n7000\nPave\nNaN\nReg\nLvl\nAllPub\nInside\nGtl\nNAmes\nNorm\nNorm\n1Fam\n1Story\n5\n7\n1960\n2002\nGable\nCompShg\nWd Sdng\nWd Sdng\nBrkFace\n45.0\nTA\nTA\nCBlock\nTA\nTA\nNo\nRec\n588.0\nUnf\n0.0\n422.0\n1010.0\nGasA\nEx\nY\nSBrkr\n1134\n0\n0\n1134\n0.0\n0.0\n1\n0\n2\n1\nTA\n6\nTyp\n0\nNaN\nAttchd\n1960.0\nRFn\n1.0\n254.0\nTA\nTA\nY\n0\n16\n0\n0\n0\n0\nNaN\nMnWw\nNaN\n0\n4\n2007\nWD\nFamily\n\n\n4\n306\n911202100\n50\nC (all)\n66.0\n8712\nPave\nPave\nReg\nHLS\nAllPub\nInside\nMod\nIDOTRR\nNorm\nNorm\n1Fam\n1.5Fin\n4\n7\n1900\n1950\nGable\nCompShg\nMetalSd\nMetalSd\nNone\n0.0\nTA\nTA\nStone\nTA\nTA\nMn\nUnf\n0.0\nUnf\n0.0\n859.0\n859.0\nGasA\nGd\nY\nSBrkr\n859\n319\n0\n1178\n0.0\n0.0\n1\n0\n2\n1\nTA\n7\nTyp\n0\nNaN\nDetchd\n1964.0\nRFn\n1.0\n384.0\nTA\nTA\nN\n68\n0\n98\n0\n0\n0\nNaN\nNaN\nNaN\n0\n1\n2010\nWD\nAbnorml\n\n\n\n\n\n\n\n\ntest_PID = test_df['PID']\n\nBefore computing the predictions, I need to preprocess the data again: checking missing values, duplicated values, feature encoding, feature engineering etc. To remain consistent, I will apply the same methods than I did for the first part of the project.\n\nData cleaning:\n\nchange = { 20:\"a\", 30:\"b\", 40:\"c\", 45:\"d\", 50:\"e\", 60:\"f\", 70:\"g\", 75:\"h\", 80:\"i\", 85:\"j\", 90:\"k\", 120:\"l\", 150:\"m\", 160:\"n\", 180:\"o\", 190:\"p\"}\ntest_df['MS SubClass']= test_df['MS SubClass'].map(change)\ntest_df['MS SubClass'].head()\n\n0    e\n1    a\n2    f\n3    a\n4    e\nName: MS SubClass, dtype: object\n\n\n\ntest_df = test_df.drop(['Order', 'PID'], axis=1)\n\n\ntest_df.duplicated().sum()\n\n0\n\n\n\ncol_NA = [col for col in test_df.columns if test_df[col].isnull().any()]\n\n\ntest_df[col_NA].isnull().sum()\n\nLot Frontage       70\nAlley             465\nMas Vnr Type        3\nMas Vnr Area        3\nBsmt Qual           9\nBsmt Cond           9\nBsmt Exposure       9\nBsmtFin Type 1      9\nBsmtFin Type 2      9\nFireplace Qu      236\nGarage Type        21\nGarage Yr Blt      21\nGarage Finish      21\nGarage Qual        21\nGarage Cond        21\nPool QC           499\nFence             417\nMisc Feature      484\ndtype: int64\n\n\n\ntest_df['Alley'].fillna(value='NA', inplace=True)\ntest_df['Bsmt Qual'].fillna(value='NA', inplace=True)\ntest_df['Bsmt Cond'].fillna(value='NA', inplace=True)\ntest_df['Bsmt Exposure'].fillna(value='NA', inplace=True)\ntest_df['BsmtFin Type 1'].fillna(value='NA', inplace=True)\ntest_df['BsmtFin Type 2'].fillna(value='NA', inplace=True)\ntest_df['Fireplace Qu'].fillna(value='NA', inplace=True)\ntest_df['Garage Type'].fillna(value='NA', inplace=True)\ntest_df['Garage Finish'].fillna(value='NA', inplace=True)\ntest_df['Garage Qual'].fillna(value='NA', inplace=True)\ntest_df['Garage Cond'].fillna(value='NA', inplace=True)\ntest_df['Pool QC'].fillna(value='NA', inplace=True)\ntest_df['Fence'].fillna(value='NA', inplace=True)\ntest_df['Misc Feature'].fillna(value='NA', inplace=True)\n\n\ncol_NA2 = [col for col in test_df.columns if test_df[col].isnull().any()]\ntest_df[col_NA2].isnull().sum()\n\nLot Frontage     70\nMas Vnr Type      3\nMas Vnr Area      3\nGarage Yr Blt    21\ndtype: int64\n\n\n\ntest_df['Lot Frontage'].fillna(value=test_df['Lot Frontage'].median(), inplace=True)\n\n\ntest_df['Mas Vnr Area'].fillna(value=0, inplace=True)\ntest_df['Mas Vnr Type'].fillna(value='NA', inplace=True)\n\n\ntest_df['Garage Yr Blt'].fillna(value=1978, inplace=True)\n\n\ntest_df.isnull().sum().sum()\n\n0\n\n\n\nPotential inconsistencies:\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Bsmt Cond']!= 'NA')][['Bsmt Qual','Bsmt Cond']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Cond\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Bsmt Exposure']!= 'NA')][['Bsmt Qual','Bsmt Exposure']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Exposure\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['BsmtFin Type 1']!= 'NA')][['Bsmt Qual','BsmtFin Type 1']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin Type 1\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['BsmtFin Type 2']!= 'NA')][['Bsmt Qual','BsmtFin Type 2']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin Type 2\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['BsmtFin SF 1']!= 0)][['Bsmt Qual','BsmtFin SF 1']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin SF 1\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['BsmtFin SF 2']!= 0)][['Bsmt Qual','BsmtFin SF 2']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmtFin SF 2\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Bsmt Unf SF']!= 0)][['Bsmt Qual','Bsmt Unf SF']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Unf SF\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Total Bsmt SF']!= 0)][['Bsmt Qual','Total Bsmt SF']]\n\n\n\n\n\n\n\n\nBsmt Qual\nTotal Bsmt SF\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Bsmt Full Bath']!= 0)][['Bsmt Qual','Bsmt Full Bath']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Full Bath\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Bsmt Qual' ] == 'NA') & (test_df['Bsmt Half Bath']!= 0)][['Bsmt Qual','Bsmt Half Bath']]\n\n\n\n\n\n\n\n\nBsmt Qual\nBsmt Half Bath\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['House Style' ] == '1Story') & (test_df['2nd Flr SF']!= 0)][['House Style','2nd Flr SF']]\n\n\n\n\n\n\n\n\nHouse Style\n2nd Flr SF\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Fireplaces'] == 0) & (test_df['Fireplace Qu']!= 'NA')][['Fireplaces','Fireplace Qu']]\n\n\n\n\n\n\n\n\nFireplaces\nFireplace Qu\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Yr Blt']!= 'NA')][['Garage Type','Garage Yr Blt']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Yr Blt\n\n\n\n\n6\nNA\n1978.0\n\n\n58\nNA\n1978.0\n\n\n62\nNA\n1978.0\n\n\n63\nNA\n1978.0\n\n\n81\nNA\n1978.0\n\n\n101\nNA\n1978.0\n\n\n102\nNA\n1978.0\n\n\n184\nNA\n1978.0\n\n\n199\nNA\n1978.0\n\n\n212\nNA\n1978.0\n\n\n233\nNA\n1978.0\n\n\n237\nNA\n1978.0\n\n\n301\nNA\n1978.0\n\n\n303\nNA\n1978.0\n\n\n321\nNA\n1978.0\n\n\n337\nNA\n1978.0\n\n\n448\nNA\n1978.0\n\n\n453\nNA\n1978.0\n\n\n457\nNA\n1978.0\n\n\n465\nNA\n1978.0\n\n\n482\nNA\n1978.0\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Finish']!= 'NA')][['Garage Type','Garage Finish']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Finish\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Cars']!= 0)][['Garage Type','Garage Cars']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Cars\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Area']!= 0)][['Garage Type','Garage Area']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Area\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Qual']!= 'NA')][['Garage Type','Garage Qual']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Qual\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Garage Type'] == 'NA') & (test_df['Garage Cond']!= 'NA')][['Garage Type','Garage Cond']]\n\n\n\n\n\n\n\n\nGarage Type\nGarage Cond\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Pool QC' ] != 'NA') & (test_df['Pool Area']== 0)][['Pool QC','Pool Area']]\n\n\n\n\n\n\n\n\nPool QC\nPool Area\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Pool QC' ] == 'NA') & (test_df['Pool Area']!= 0)][['Pool QC','Pool Area']]\n\n\n\n\n\n\n\n\nPool QC\nPool Area\n\n\n\n\n\n\n\n\n\n\ntest_df[(test_df['Misc Feature' ] != 'NA') & (test_df['Misc Val']== 0)][['Misc Feature','Misc Val']]\n\n\n\n\n\n\n\n\nMisc Feature\nMisc Val\n\n\n\n\n451\nOthr\n0\n\n\n\n\n\n\n\nI replace the misc feature by NA since its misc value is equal to 0.\n\ntest_df.loc[451,'Misc Feature'] = 'NA'\n\n\ntest_df[(test_df['Misc Feature' ] == 'NA') & (test_df['Misc Val']!= 0)][['Misc Feature','Misc Val']]\n\n\n\n\n\n\n\n\nMisc Feature\nMisc Val\n\n\n\n\n\n\n\n\n\n\n(test_df['Year Built'] &gt; test_df['Yr Sold']).sum()\n\n0\n\n\n\ntest_df[['Lot Frontage',\n         'Lot Area', \n         'Mas Vnr Area',\n         'BsmtFin SF 1',\n         'BsmtFin SF 2',\n         'Bsmt Unf SF',\n         'Total Bsmt SF',\n         '1st Flr SF',\n         '2nd Flr SF',\n         'Low Qual Fin SF',\n         'Gr Liv Area',\n         'Bsmt Full Bath',\n         'Bsmt Half Bath',\n         'Full Bath',\n         'Half Bath',\n         'Bedroom AbvGr',\n         'Kitchen AbvGr',\n         'Total Bsmt SF',\n         'Fireplaces',\n         'Garage Cars',\n         'Garage Area',\n         'Wood Deck SF',\n         'Open Porch SF',\n         'Enclosed Porch',\n         '3Ssn Porch',\n         'Screen Porch',\n         'Pool Area',\n         'Misc Val']].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nLot Frontage\n500.0\n69.284\n21.154251\n21.0\n60.00\n69.0\n79.00\n153.0\n\n\nLot Area\n500.0\n9683.132\n4950.630454\n1300.0\n7329.00\n9150.5\n11312.75\n53504.0\n\n\nMas Vnr Area\n500.0\n100.810\n190.548649\n0.0\n0.00\n0.0\n146.50\n1378.0\n\n\nBsmtFin SF 1\n500.0\n436.242\n446.826476\n0.0\n0.00\n342.5\n706.25\n2260.0\n\n\nBsmtFin SF 2\n500.0\n43.158\n153.891202\n0.0\n0.00\n0.0\n0.00\n1474.0\n\n\nBsmt Unf SF\n500.0\n572.776\n438.924115\n0.0\n242.75\n478.0\n811.50\n1969.0\n\n\nTotal Bsmt SF\n500.0\n1052.176\n420.686171\n0.0\n784.00\n990.0\n1303.00\n3200.0\n\n\n1st Flr SF\n500.0\n1158.160\n386.358440\n494.0\n864.00\n1077.5\n1382.25\n3228.0\n\n\n2nd Flr SF\n500.0\n378.142\n467.378415\n0.0\n0.00\n0.0\n731.25\n1796.0\n\n\nLow Qual Fin SF\n500.0\n5.114\n41.670299\n0.0\n0.00\n0.0\n0.00\n514.0\n\n\nGr Liv Area\n500.0\n1541.416\n547.214249\n540.0\n1173.50\n1454.5\n1769.50\n4676.0\n\n\nBsmt Full Bath\n500.0\n0.418\n0.509701\n0.0\n0.00\n0.0\n1.00\n2.0\n\n\nBsmt Half Bath\n500.0\n0.066\n0.256467\n0.0\n0.00\n0.0\n0.00\n2.0\n\n\nFull Bath\n500.0\n1.594\n0.574303\n0.0\n1.00\n2.0\n2.00\n4.0\n\n\nHalf Bath\n500.0\n0.428\n0.507278\n0.0\n0.00\n0.0\n1.00\n2.0\n\n\nBedroom AbvGr\n500.0\n2.910\n0.873743\n1.0\n2.00\n3.0\n3.00\n6.0\n\n\nKitchen AbvGr\n500.0\n1.052\n0.239606\n0.0\n1.00\n1.0\n1.00\n3.0\n\n\nTotal Bsmt SF\n500.0\n1052.176\n420.686171\n0.0\n784.00\n990.0\n1303.00\n3200.0\n\n\nFireplaces\n500.0\n0.620\n0.654566\n0.0\n0.00\n1.0\n1.00\n3.0\n\n\nGarage Cars\n500.0\n1.788\n0.772171\n0.0\n1.00\n2.0\n2.00\n5.0\n\n\nGarage Area\n500.0\n476.902\n211.744251\n0.0\n312.75\n480.0\n576.25\n1220.0\n\n\nWood Deck SF\n500.0\n92.946\n117.628585\n0.0\n0.00\n0.0\n171.25\n728.0\n\n\nOpen Porch SF\n500.0\n49.680\n69.232651\n0.0\n0.00\n26.0\n72.00\n444.0\n\n\nEnclosed Porch\n500.0\n26.496\n72.019689\n0.0\n0.00\n0.0\n0.00\n584.0\n\n\n3Ssn Porch\n500.0\n3.180\n26.391450\n0.0\n0.00\n0.0\n0.00\n360.0\n\n\nScreen Porch\n500.0\n13.440\n52.347674\n0.0\n0.00\n0.0\n0.00\n576.0\n\n\nPool Area\n500.0\n1.476\n33.004363\n0.0\n0.00\n0.0\n0.00\n738.0\n\n\nMisc Val\n500.0\n23.688\n175.375094\n0.0\n0.00\n0.0\n0.00\n2500.0\n\n\n\n\n\n\n\n\ntest_df[['Year Built','Year Remod/Add','Yr Sold']].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nYear Built\n500.0\n1972.274\n30.440305\n1872.0\n1953.0\n1975.0\n2001.25\n2010.0\n\n\nYear Remod/Add\n500.0\n1984.402\n21.403378\n1950.0\n1964.0\n1994.0\n2004.00\n2010.0\n\n\nYr Sold\n500.0\n2007.824\n1.340380\n2006.0\n2007.0\n2008.0\n2009.00\n2010.0\n\n\n\n\n\n\n\nEverything seems correct.\n\n\nFeature Encoding\n\nOrdinal variales:\n\n\ntest_df[OrdinalVar[0]].replace({'Reg':4, 'IR1':3,'IR2':2,'IR3':1},inplace=True)\ntest_df[OrdinalVar[1]].replace({'AllPub':4, 'NoSewr':3,'NoSeWa':2,'ELO':1},inplace=True)\ntest_df[OrdinalVar[2]].replace({ 'Gtl':3,'Mod':2,'Sev':1},inplace=True)\n### data_df[OrdinalVar[3]].replace({},inplace=True) (already in a numerical order)\n### data_df[OrdinalVar[4]].replace({},inplace=True) (already in a numerical order)\ntest_df[OrdinalVar[5]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ntest_df[OrdinalVar[6]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ntest_df[OrdinalVar[7]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[8]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1, 'NA':0},inplace=True)\ntest_df[OrdinalVar[9]].replace({'Gd':4, 'Av':3,'Mn':2,'No':1, 'NA':0},inplace=True)\ntest_df[OrdinalVar[10]].replace({'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3,'LwQ':2,'Unf':1, 'NA':0},inplace=True)\ntest_df[OrdinalVar[11]].replace({'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3,'LwQ':2,'Unf':1, 'NA':0},inplace=True)\ntest_df[OrdinalVar[12]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ntest_df[OrdinalVar[13]].replace({'SBrkr':5, 'FuseA':4, 'FuseF':3,'FuseP':2,'Mix':1},inplace=True)\ntest_df[OrdinalVar[14]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1},inplace=True)\ntest_df[OrdinalVar[15]].replace({'Typ':8,'Min1':7,'Min2':6,'Mod':5, 'Maj1':4, 'Maj2':3,'Sev':2,'Sal':1},inplace=True)\ntest_df[OrdinalVar[16]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[17]].replace({'Fin':3,'RFn':2,'Unf':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[18]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[19]].replace({'Ex':5, 'Gd':4, 'TA':3,'Fa':2,'Po':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[20]].replace({'Y':3,'P':2,'N':1},inplace=True)\ntest_df[OrdinalVar[21]].replace({'Ex':4, 'Gd':3,'TA':2,'Fa':1,'NA':0},inplace=True)\ntest_df[OrdinalVar[22]].replace({'GdPrv':4, 'MnPrv':3,'GdWo':2,'MnWw':1,'NA':0},inplace=True)\n\n\ntest_df[OrdinalVar].apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n\nLot Shape         True\nUtilities         True\nLand Slope        True\nOverall Qual      True\nOverall Cond      True\nExter Qual        True\nExter Cond        True\nBsmt Qual         True\nBsmt Cond         True\nBsmt Exposure     True\nBsmtFin Type 1    True\nBsmtFin Type 2    True\nHeating QC        True\nElectrical        True\nKitchen Qual      True\nFunctional        True\nFireplace Qu      True\nGarage Finish     True\nGarage Qual       True\nGarage Cond       True\nPaved Drive       True\nPool QC           True\nFence             True\ndtype: bool\n\n\n\nNominal variables:\n\n\ntest_df = pd.get_dummies(test_df, columns=NominalVar, drop_first=True)\n\n\ntest_df.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n\nLot Frontage              True\nLot Area                  True\nLot Shape                 True\nUtilities                 True\nLand Slope                True\nOverall Qual              True\nOverall Cond              True\nYear Built                True\nYear Remod/Add            True\nMas Vnr Area              True\nExter Qual                True\nExter Cond                True\nBsmt Qual                 True\nBsmt Cond                 True\nBsmt Exposure             True\nBsmtFin Type 1            True\nBsmtFin SF 1              True\nBsmtFin Type 2            True\nBsmtFin SF 2              True\nBsmt Unf SF               True\nTotal Bsmt SF             True\nHeating QC                True\nElectrical                True\n1st Flr SF                True\n2nd Flr SF                True\nLow Qual Fin SF           True\nGr Liv Area               True\nBsmt Full Bath            True\nBsmt Half Bath            True\nFull Bath                 True\nHalf Bath                 True\nBedroom AbvGr             True\nKitchen AbvGr             True\nKitchen Qual              True\nTotRms AbvGrd             True\nFunctional                True\nFireplaces                True\nFireplace Qu              True\nGarage Yr Blt             True\nGarage Finish             True\nGarage Cars               True\nGarage Area               True\nGarage Qual               True\nGarage Cond               True\nPaved Drive               True\nWood Deck SF              True\nOpen Porch SF             True\nEnclosed Porch            True\n3Ssn Porch                True\nScreen Porch              True\nPool Area                 True\nPool QC                   True\nFence                     True\nMisc Val                  True\nMo Sold                   True\nYr Sold                   True\nMS SubClass_b             True\nMS SubClass_c             True\nMS SubClass_d             True\nMS SubClass_e             True\nMS SubClass_f             True\nMS SubClass_g             True\nMS SubClass_h             True\nMS SubClass_i             True\nMS SubClass_j             True\nMS SubClass_k             True\nMS SubClass_l             True\nMS SubClass_n             True\nMS SubClass_o             True\nMS SubClass_p             True\nMS Zoning_FV              True\nMS Zoning_RH              True\nMS Zoning_RL              True\nMS Zoning_RM              True\nStreet_Pave               True\nAlley_NA                  True\nAlley_Pave                True\nLand Contour_HLS          True\nLand Contour_Low          True\nLand Contour_Lvl          True\nLot Config_CulDSac        True\nLot Config_FR2            True\nLot Config_FR3            True\nLot Config_Inside         True\nNeighborhood_Blueste      True\nNeighborhood_BrDale       True\nNeighborhood_BrkSide      True\nNeighborhood_ClearCr      True\nNeighborhood_CollgCr      True\nNeighborhood_Crawfor      True\nNeighborhood_Edwards      True\nNeighborhood_Gilbert      True\nNeighborhood_IDOTRR       True\nNeighborhood_MeadowV      True\nNeighborhood_Mitchel      True\nNeighborhood_NAmes        True\nNeighborhood_NPkVill      True\nNeighborhood_NWAmes       True\nNeighborhood_NoRidge      True\nNeighborhood_NridgHt      True\nNeighborhood_OldTown      True\nNeighborhood_SWISU        True\nNeighborhood_Sawyer       True\nNeighborhood_SawyerW      True\nNeighborhood_Somerst      True\nNeighborhood_StoneBr      True\nNeighborhood_Timber       True\nNeighborhood_Veenker      True\nCondition 1_Feedr         True\nCondition 1_Norm          True\nCondition 1_PosA          True\nCondition 1_PosN          True\nCondition 1_RRAe          True\nCondition 1_RRAn          True\nCondition 1_RRNe          True\nCondition 1_RRNn          True\nCondition 2_Norm          True\nCondition 2_PosN          True\nBldg Type_2fmCon          True\nBldg Type_Duplex          True\nBldg Type_Twnhs           True\nBldg Type_TwnhsE          True\nHouse Style_1.5Unf        True\nHouse Style_1Story        True\nHouse Style_2.5Fin        True\nHouse Style_2.5Unf        True\nHouse Style_2Story        True\nHouse Style_SFoyer        True\nHouse Style_SLvl          True\nRoof Style_Gable          True\nRoof Style_Gambrel        True\nRoof Style_Hip            True\nRoof Style_Mansard        True\nRoof Matl_Roll            True\nRoof Matl_Tar&Grv         True\nRoof Matl_WdShake         True\nExterior 1st_BrkFace      True\nExterior 1st_CemntBd      True\nExterior 1st_HdBoard      True\nExterior 1st_MetalSd      True\nExterior 1st_Plywood      True\nExterior 1st_Stucco       True\nExterior 1st_VinylSd      True\nExterior 1st_Wd Sdng      True\nExterior 1st_WdShing      True\nExterior 2nd_Brk Cmn      True\nExterior 2nd_BrkFace      True\nExterior 2nd_CBlock       True\nExterior 2nd_CmentBd      True\nExterior 2nd_HdBoard      True\nExterior 2nd_ImStucc      True\nExterior 2nd_MetalSd      True\nExterior 2nd_Other        True\nExterior 2nd_Plywood      True\nExterior 2nd_Stone        True\nExterior 2nd_Stucco       True\nExterior 2nd_VinylSd      True\nExterior 2nd_Wd Sdng      True\nExterior 2nd_Wd Shng      True\nMas Vnr Type_BrkFace      True\nMas Vnr Type_NA           True\nMas Vnr Type_None         True\nMas Vnr Type_Stone        True\nFoundation_CBlock         True\nFoundation_PConc          True\nFoundation_Slab           True\nFoundation_Stone          True\nHeating_GasW              True\nHeating_Grav              True\nHeating_OthW              True\nHeating_Wall              True\nCentral Air_Y             True\nGarage Type_Attchd        True\nGarage Type_Basment       True\nGarage Type_BuiltIn       True\nGarage Type_CarPort       True\nGarage Type_Detchd        True\nGarage Type_NA            True\nMisc Feature_Shed         True\nSale Type_CWD             True\nSale Type_ConLD           True\nSale Type_ConLI           True\nSale Type_ConLw           True\nSale Type_New             True\nSale Type_Oth             True\nSale Type_WD              True\nSale Condition_AdjLand    True\nSale Condition_Alloca     True\nSale Condition_Family     True\nSale Condition_Normal     True\nSale Condition_Partial    True\ndtype: bool\n\n\n\n\nFeature engineering\n\nContinuous Variables:\n\n\ntest_df[ContinuousVar]=np.log1p(test_df[ContinuousVar])\n\n\ntest_df = test_df.rename(columns={'Lot Frontage':'log_Lot Frontage',\n 'Lot Area':'log_Lot Area',\n 'Mas Vnr Area':'log_Mas Vnr Area',\n 'BsmtFin SF 1':'log_BsmtFin SF 1',\n 'BsmtFin SF 2':'log_BsmtFin SF 2',\n 'Bsmt Unf SF':'log_Bsmt Unf SF',\n 'Total Bsmt SF':'log_Total Bsmt SF',\n '1st Flr SF':'log_1st Flr SF',\n '2nd Flr SF':'log_2nd Flr SF',\n 'Low Qual Fin SF':'log_Low Qual Fin SF',\n 'Gr Liv Area':'log_Gr Liv Area',\n 'Garage Area':'log_Garage Area',\n 'Wood Deck SF':'log_Wood Deck SF',\n 'Open Porch SF':'log_Open Porch SF',\n 'Enclosed Porch':'log_Enclosed Porch',\n '3Ssn Porch':'log_3Ssn Porch',\n 'Screen Porch':'log_Screen Porch',\n 'Pool Area':'log_Pool Area',\n 'Misc Val':'log_Misc Val'})\n\n\ntest_df['Basement'] = (test_df['log_Total Bsmt SF'] !=0).astype(int)\ntest_df['Pool'] = (test_df['log_Pool Area'] !=0).astype(int)\ntest_df['Mas Vnr Area'] = (test_df['log_Mas Vnr Area'] !=0).astype(int)\n\n\nDiscrete Variables:\n\n\ntest_df['Year Built Above 1980'] = (test_df['Year Built'] &gt;= 1980).astype(int)\ntest_df['Year Built Below 1980'] = (test_df['Year Built'] &lt; 1980).astype(int)\n\n\ntest_df['House without Garage'] = (test_df['Garage Cars'] == 0).astype(int)\ntest_df['House with Garage'] = (test_df['Garage Cars'] != 0).astype(int)\n\nNow that I have the same encoding and engineering for all the data, I can start to make the model predictions.\n\n\nSimple model\n\ntest_X_simple = test_df[['Overall Qual','log_Gr Liv Area']]\n\n\nhuber = HuberRegressor(epsilon=1.45)\nhuber.fit(X_tr_simple, y_tr_simple) \ny_pred_huber_simple_test = huber.predict(test_X_simple)\n\n\ny_pred_simple = np.exp(y_pred_huber_simple_test)\n\n\npd.DataFrame({'PID':test_PID,'SalePrice':y_pred_simple}).to_csv('predictions-simple-model.csv',index=False)\n\n\n\nIntermediate model\n\ntest_X_intermediate = test_df[['Overall Qual','log_Gr Liv Area','Garage Cars', 'Kitchen Qual', 'Bsmt Qual','log_1st Flr SF','Year Built Above 1980','Garage Finish',\n    'Year Remod/Add','Full Bath']]\n\n\nhuber = HuberRegressor(epsilon=1.45)\nhuber.fit(X_tr_intermediate, y_tr_intermediate) \ny_pred_huber_intermediate_test = huber.predict(test_X_intermediate)\n\n\ny_pred_intermediate = np.exp(y_pred_huber_intermediate_test)\n\n\npd.DataFrame({'PID':test_PID,'SalePrice':y_pred_intermediate}).to_csv('predictions-intermediate-model.csv',index=False)\n\n\n\nComplex model\nI had a problem with the dimensions of the dataset. Indeed, the first dataset had more variables (due to the encoding/dummies). I had to tranform the new dataset and fill the columns with 0 to hanndle this problem.\n\ncolumns_data = data_df.columns.drop(['SalePrice','log_SalePrice'])\n\n\ncolumns_test = test_df.columns\n\n\ncolumns_add = columns_data[~columns_data.isin(columns_test)]\n\n\nfor col in columns_add:\n    test_df[col] = 0\n\n\ntest_df = test_df.reindex(columns=data_df.columns).drop(['SalePrice','log_SalePrice'],axis=1)\n\n\ntest_X_rescaled = scaler.transform(test_df)\n\ny_pred_complex = np.exp(best_result.model.predict(test_X_rescaled))\n\n\npd.DataFrame({'PID':test_PID,'SalePrice':y_pred_complex}).to_csv('predictions-complex-model.csv',index=False)\n\n\n\nSource: Packages"
  },
  {
    "objectID": "manchester.html",
    "href": "manchester.html",
    "title": "Gregoire Urvoy",
    "section": "",
    "text": "Manchester\n\n\nBlue or Red?\n\n \n\nEnter the realm of football’s fierce symphony, where the streets of Manchester echo with the thunderous beats of rivalry – the storied clash between Manchester City and Manchester United. This is no ordinary derby; it’s a spectacle that transcends the boundaries of sport, a saga of passion, pride, and undying loyalty. Since the inception of this epic duel, the two giants, City and United, have woven a narrative of contention and glory that resonates far beyond the city’s limits. The rivalry encapsulates the spirit of Manchester itself, where blue and red stand as vibrant symbols in a ceaseless struggle for supremacy. From the hallowed Old Trafford to the Etihad Stadium, each encounter becomes a chapter in the folklore of the Manchester Derby, where the roar of the crowd and the clash of titans create a footballing spectacle that captures the essence of competition at its most exhilarating. Join us as we delve into the history of the Manchester Derby – a timeless clash that resounds with the beating heart and soul of English football. Manchester, Blue or Red?\n\nIN PROGRESS"
  },
  {
    "objectID": "premiere_league.html",
    "href": "premiere_league.html",
    "title": "Gregoire Urvoy",
    "section": "",
    "text": "Premier league\n\n\nSimply the best\n\n \n\nEnvision a grand football spectacle known as the Premier League – a distinguished gathering of 20 premier teams, akin to formidable titans engaged in epic on-field duels. Its inaugural chapter unfolded in 1992, swiftly evolving into a global phenomenon that commands fervent admiration. The stadiums, hallowed grounds of sporting prowess, echo with the cacophony of ardent fans, while the players, drawn from diverse corners of the globe, infuse the league with an international flair. Beyond mere competition, this is a vibrant tapestry where each goal and remarkable save etches a compelling narrative into the annals of footballing history. Join this transcendent odyssey into the heart of the Premier League, where the beautiful game metamorphoses into a canvas upon which legends are painted. Simply the best.\n\n\nCurrent Table\n\n\n\n\n\nRank\n\n\n\n\nTeam\n\n\nPl\n\n\nW\n\n\nD\n\n\nL\n\n\nF\n\n\nA\n\n\nGD\n\n\nPts\n\n\n\n\n\n\n1\n\n\n\n\n\nManchester City\n\n\n12\n\n\n9\n\n\n1\n\n\n2\n\n\n32\n\n\n12\n\n\n20\n\n\n28\n\n\n\n\n2\n\n\n\n\n\nLiverpool\n\n\n12\n\n\n8\n\n\n3\n\n\n1\n\n\n27\n\n\n10\n\n\n17\n\n\n27\n\n\n\n\n3\n\n\n\n\n\nArsenal\n\n\n12\n\n\n8\n\n\n3\n\n\n1\n\n\n26\n\n\n10\n\n\n16\n\n\n27\n\n\n\n\n4\n\n\n\n\n\nTottenham Hotspur\n\n\n12\n\n\n8\n\n\n2\n\n\n2\n\n\n24\n\n\n15\n\n\n9\n\n\n26\n\n\n\n\n5\n\n\n\n\n\nAston Villa\n\n\n12\n\n\n8\n\n\n1\n\n\n3\n\n\n29\n\n\n17\n\n\n12\n\n\n25\n\n\n\n\n6\n\n\n\n\n\nManchester United\n\n\n12\n\n\n7\n\n\n0\n\n\n5\n\n\n13\n\n\n16\n\n\n-3\n\n\n21\n\n\n\n\n7\n\n\n\n\n\nNewcastle United\n\n\n12\n\n\n6\n\n\n2\n\n\n4\n\n\n27\n\n\n13\n\n\n14\n\n\n20\n\n\n\n\n8\n\n\n\n\n\nBrighton and Hove Albion\n\n\n12\n\n\n5\n\n\n4\n\n\n3\n\n\n25\n\n\n21\n\n\n4\n\n\n19\n\n\n\n\n9\n\n\n\n\n\nWest Ham United\n\n\n12\n\n\n5\n\n\n2\n\n\n5\n\n\n21\n\n\n22\n\n\n-1\n\n\n17\n\n\n\n\n10\n\n\n\n\n\nChelsea\n\n\n12\n\n\n4\n\n\n4\n\n\n4\n\n\n21\n\n\n16\n\n\n5\n\n\n16\n\n\n\n\n11\n\n\n\n\n\nBrentford\n\n\n12\n\n\n4\n\n\n4\n\n\n4\n\n\n19\n\n\n17\n\n\n2\n\n\n16\n\n\n\n\n12\n\n\n\n\n\nWolverhampton Wanderers\n\n\n12\n\n\n4\n\n\n3\n\n\n5\n\n\n16\n\n\n20\n\n\n-4\n\n\n15\n\n\n\n\n13\n\n\n\n\n\nCrystal Palace\n\n\n12\n\n\n4\n\n\n3\n\n\n5\n\n\n12\n\n\n16\n\n\n-4\n\n\n15\n\n\n\n\n14\n\n\n\n\n\nNottingham Forest\n\n\n12\n\n\n3\n\n\n4\n\n\n5\n\n\n14\n\n\n18\n\n\n-4\n\n\n13\n\n\n\n\n15\n\n\n\n\n\nFulham\n\n\n12\n\n\n3\n\n\n3\n\n\n6\n\n\n10\n\n\n20\n\n\n-10\n\n\n12\n\n\n\n\n16\n\n\n\n\n\nBournemouth\n\n\n12\n\n\n2\n\n\n3\n\n\n7\n\n\n11\n\n\n27\n\n\n-16\n\n\n9\n\n\n\n\n17\n\n\n\n\n\nLuton Town\n\n\n12\n\n\n1\n\n\n3\n\n\n8\n\n\n10\n\n\n22\n\n\n-12\n\n\n6\n\n\n\n\n18\n\n\n\n\n\nSheffield United\n\n\n12\n\n\n1\n\n\n2\n\n\n9\n\n\n10\n\n\n31\n\n\n-21\n\n\n5\n\n\n\n\n19\n\n\n\n\n\nEverton *\n\n\n12\n\n\n4\n\n\n2\n\n\n6\n\n\n14\n\n\n17\n\n\n-3\n\n\n4\n\n\n\n\n20\n\n\n\n\n\nBurnley\n\n\n12\n\n\n1\n\n\n1\n\n\n10\n\n\n9\n\n\n30\n\n\n-21\n\n\n4\n\n\n\n\n\n\nSeason 2023-2024\n\n\n\n\n\n\n\n\n\nAll for the show!\n\n\n\n\n\n\n\n\nAlways more money\n\n\n\n\n\n\n\n\n\nClub value in US$ million by league in 2023\n\n\n\n\n\n\n\n\nA Few Iconic Players\n\n\n\n \n\nWayne ROONEY\n\n\nEngland , Everton , Manchester United   Appareances: 491  Goals: 208  Assists: 103  5 x Premier League champions  1 x Player of the season\n\n\n \n\nSteven GERRARD\n\n\nEngland , Liverpool   Appareances: 504  Goals: 120  Assists: 92  Liverpool legend  8 x Premier League Team of the year\n\n\n \n\nRyan GIGGS\n\n\nWales , Manchester United   Appareances: 632  Goals: 109  Assists: 162  13 x Premier League Champion  Most appareances for a Manchester United player\n\n\n \n\nAlan SHEARER\n\n\nEngland , Blackburn Rovers , Newcastle United   Appareances: 441  Goals: 260  3 x Golden boot  1 x Premier League champions  Premier League’s All time record goalscorer\n\n\n \n\nErlind HAALAND\n\n\nNorway , Manchester city   Appareances: 47  Goals: 49  Season 22-23: Premier League Champion, Young Player of the Season, Player of the Season, Golden Boot  Most goals scored by a player in a single season\n\n\n\nFun facts about the Premier League\n\n\n38\n\nThe Invincibles refers to the 2003-2004 Arsenal football team, that went unbeaten throughout the entire Premier League season, a remarkable achievement in modern English football history. They secured the league title with 26 wins and 12 draws, remaining undefeated in all 38 matches.\n\n\n6\n\nArsenal, Chelsea, Everton, Liverpool, Manchester United, and Tottenham Hotspur are the indomitable six, steadfast against the specter of relegation in the rich history of the Premier League.\n\n\n90+\n\nWe say ‘Fergie time’ in reference to Sir Alex Ferguson, the legendary manager of Manchester United, because his teams were known for scoring crucial goals during added time, leading to dramatic late victories, creating a perception that the match clock was extended in their favor.\n\n\n[11]{.big-number_2 style=“font-size:72px; color:#432060; font-family: Garamond, serif”&gt;\n\nWhile being the all-time top scorer of the Premier League, Alan Shearer has also the most missed penalties in the league (11).\n\n\n\nA Few Iconic Managers\n\n\n\n\nSir Alex FERGUSON\n\n\nScotland , Manchester United   Matchs: 810  Wins: 528 (65%)  27 x Manager of the months  13 x Premier League champions  11 x Manager of the season\n\n\n\n\nJose MOURINHO\n\n\nPortugal , Chelsea , Manchester United , Tottenham Hotspur   Matchs: 363  Wins: 217 (60%)  Goals for: 625 (1.7 per game)  3 x Premier League champions  3 x Manager of the season\n\n\n\n\nArsene WENGER\n\n\nFrance , Arsenal   Matchs: 828 (record)  Wins: 476 (57%)  Goals for: 1561 (1.9 per game)  3 x Premier League champions  3 x Manager of the season\n\n\n\n\nPep GUARDIOLA\n\n\nSpain , Manchester City   Matchs: 278  Wins: 206 (74%)  Average possession percentage: 68%  5 x Premier League champions  4 x Manager of the season\n\n\nMy Premier League XI\n\n\n\n\n\n\n\n\n  \n\n“Everybody wants the Hollywood glamour of the Premier League.”\n\nOle Gunnar Solskjaer \n \n\nhttps://www.premierleague.com/home  https://fbref.com/  https://theathletic.com/4240951/2023/08/25/premier-league-tv-rights-how-work-cost/  https://www.forbes.com/lists/soccer-valuations/?sh=14a8e816198b"
  },
  {
    "objectID": "projet_2.html",
    "href": "projet_2.html",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "",
    "text": "One of the main challenges of asset management in finance is to develop models that can predict stock prices, or in other words, the market capitalization of a company (numbers of stocks x stock prices). Market capitalisation can be seen as the overall value of the company. Usually, investors use a method called DCF (Discounted Cash Flow) to predict the value of the company. In a few words, this method consists in predicting the future revenues of the company and discount it to get the present value of the company. This method is very demanding and need many assumptions concerning the company, the different rates you want to use, etc.\n\nThe goal of my project is to try to develop a machine learning model to estimate the current market capitalisation using only the latest statistics of the company."
  },
  {
    "objectID": "projet_2.html#import-packages",
    "href": "projet_2.html#import-packages",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Import packages",
    "text": "Import packages\n\n# Import packages\nfrom selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nimport pandas as pd\nimport time\npd.options.display.float_format = '{:.0f}'.format"
  },
  {
    "objectID": "projet_2.html#ticker-list",
    "href": "projet_2.html#ticker-list",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Ticker list",
    "text": "Ticker list\n\n# Import data\nticker = pd.read_csv(r'TCK.csv')\n\n\nticker = ticker['Ticker'].tolist()\n\n\nticker[0:10]\n\n\nlen(ticker)\n\nExample: https://finance.yahoo.com/quote/AAPL/balance-sheet?p=AAPL"
  },
  {
    "objectID": "projet_2.html#bs",
    "href": "projet_2.html#bs",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "BS",
    "text": "BS\n\n# List of features to extract\nlist_BS = ['Total Assets',\n 'Current Assets',\n 'Cash, Cash Equivalents & Short Term Investments',\n 'Cash And Cash Equivalents',\n 'Cash',\n 'Cash Equivalents',\n 'Other Short Term Investments',\n 'Receivables',\n 'Accounts receivable',\n 'Gross Accounts Receivable',\n 'Allowance For Doubtful Accounts Receivable',\n 'Other Receivables',\n 'Inventory',\n 'Other Current Assets',\n 'Total non-current assets',\n 'Net PPE',\n 'Gross PPE',\n 'Properties',\n 'Land And Improvements',\n 'Machinery Furniture Equipment',\n 'Leases',\n 'Accumulated Depreciation',\n 'Goodwill And Other Intangible Assets',\n 'Goodwill',\n 'Other Intangible Assets',\n 'Investments And Advances',\n 'Investment in Financial Assets',\n 'Available for Sale Securities',\n 'Other Non Current Assets',\n 'Total Liabilities Net Minority Interest',\n 'Current Liabilities',\n 'Payables And Accrued Expenses',\n 'Payables',\n 'Accounts Payable',\n 'Current Accrued Expenses',\n 'Current Debt And Capital Lease Obligation',\n 'Current Debt',\n 'Commercial Paper',\n 'Other Current Borrowings',\n 'Current Deferred Liabilities',\n 'Current Deferred Revenue',\n 'Other Current Liabilities',\n 'Total Non Current Liabilities Net Minority Interest',\n 'Long Term Debt And Capital Lease Obligation',\n 'Long Term Debt',\n 'Non Current Deferred Liabilities',\n 'Non Current Deferred Taxes Liabilities',\n 'Non Current Deferred Revenue',\n 'Tradeand Other Payables Non Current',\n 'Other Non Current Liabilities',\n 'Total Equity Gross Minority Interest',\n \"Stockholders' Equity\",\n 'Capital Stock',\n 'Common Stock',\n 'Retained Earnings',\n 'Gains Losses Not Affecting Retained Earnings',\n 'Total Capitalization',\n 'Common Stock Equity',\n 'Net Tangible Assets',\n 'Working Capital',\n 'Invested Capital',\n 'Tangible Book Value',\n 'Total Debt',\n 'Net Debt',\n 'Share Issued',\n 'Ordinary Shares Number']\n\nlist_BS_asdf = pd.DataFrame(list_BS)\n\n\n# Create empty dataframe\nBS = pd.DataFrame(index=list_BS,columns=ticker)\n\n\nfor i in range(0,1):\n    try:\n        # I had to create a temporary dataframe to store the values. Otherwise, impossible to merge 3000 companies with different features. \n        Temporary_BS = pd.DataFrame(index=range(150),columns=range(2))\n        driver = webdriver.Chrome(ChromeDriverManager().install())\n        # Change the URL \n        is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/balance-sheet?p=' + ticker[i]\n        driver.get(is_link)\n        time.sleep(1)\n        # Click on the expand button\n        link1 = driver.find_element_by_xpath('//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[2]/button/div')\n        link1.click()\n        time.sleep(1)\n        html = driver.execute_script('return document.body.innerHTML;')\n        soup = BeautifulSoup(html,'lxml')\n        # Get features\n        features_BS = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(tbc)')\n        # Get values\n        features_BS_bis = soup.find_all('span', class_='Va(m)')\n        # To manage the ads \n        if features_BS_bis[0].text != 'Total Assets':\n            features_BS_bis = features_BS_bis[1:]\n            if len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 5 :\n                j = 0\n                for j in range(0,len(features_BS),3):\n                    n=int(j/3)\n                    Temporary_BS.iloc[n,1] = features_BS[j].text\n                t = 0\n                for t in range(0,len(features_BS_bis)):\n                    Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n                Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n                Temporary_BS.set_index(0,inplace=True)\n                BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values\n            elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 3 or len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n                j = 0\n                for j in range(0,len(features_BS),2):\n                    n=int(j/2)\n                    Temporary_BS.iloc[n,1] = features_BS[j].text\n                t = 0\n                for t in range(0,len(features_BS_bis)):\n                    Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n                Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n                Temporary_BS.set_index(0,inplace=True)\n                BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values\n            elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 1 or len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 2  :\n                j = 0\n                for j in range(0,len(features_BS),1):\n                    n=int(j/1)\n                    Temporary_BS.iloc[n,1] = features_BS[j].text\n                t = 0\n                for t in range(0,len(features_BS_bis)):\n                    Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n                Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n                Temporary_BS.set_index(0,inplace=True)\n                BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values   \n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 5 :\n            j = 0\n            for j in range(0,len(features_BS),3):\n                n=int(j/3)\n                Temporary_BS.iloc[n,1] = features_BS[j].text\n            t = 0\n            for t in range(0,len(features_BS_bis)):\n                Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n            Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n            Temporary_BS.set_index(0,inplace=True)\n            BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 3 or len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n            j = 0\n            for j in range(0,len(features_BS),2):\n                n=int(j/2)\n                Temporary_BS.iloc[n,1] = features_BS[j].text\n            t = 0\n            for t in range(0,len(features_BS_bis)):\n                Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n            Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n            Temporary_BS.set_index(0,inplace=True)\n            BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 1 or len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 2  :\n            j = 0\n            for j in range(0,len(features_BS),1):\n                n=int(j/1)\n                Temporary_BS.iloc[n,1] = features_BS[j].text\n            t = 0\n            for t in range(0,len(features_BS_bis)):\n                Temporary_BS.iloc[t,0] = features_BS_bis[t].text\n            Temporary_BS = Temporary_BS.merge(list_BS_asdf,how='right')\n            Temporary_BS.set_index(0,inplace=True)\n            BS.loc[:,ticker[i]] = Temporary_BS.loc[sorted(Temporary_BS.index.to_list(), key=BS.index.to_list().index),:][1].values\n    except:\n        continue\n    driver.close()\n\n\nBS\n\n\n#BS.to_csv(r'BS.csv')"
  },
  {
    "objectID": "projet_2.html#cash-flow",
    "href": "projet_2.html#cash-flow",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Cash Flow",
    "text": "Cash Flow\n\nlist_CF = ['Operating Cash Flow',\n 'Cash Flow from Continuing Operating Activities',\n 'Net Income from Continuing Operations',\n 'Depreciation Amortization Depletion',\n 'Depreciation & amortization',\n 'Deferred Tax',\n 'Deferred Income Tax',\n 'Stock based compensation',\n 'Other non-cash items',\n 'Change in working capital',\n 'Change in Receivables',\n 'Changes in Account Receivables',\n 'Change in Inventory',\n 'Change in Payables And Accrued Expense',\n 'Change in Payable',\n 'Change in Account Payable',\n 'Change in Other Current Assets',\n 'Change in Other Current Liabilities',\n 'Change in Other Working Capital',\n 'Investing Cash Flow',\n 'Cash Flow from Continuing Investing Activities',\n 'Net PPE Purchase And Sale',\n 'Purchase of PPE',\n 'Net Intangibles Purchase And Sale',\n 'Purchase of Intangibles',\n 'Net Business Purchase And Sale',\n 'Purchase of Business',\n 'Net Investment Purchase And Sale',\n 'Purchase of Investment',\n 'Sale of Investment',\n 'Net Other Investing Changes',\n 'Financing Cash Flow',\n 'Cash Flow from Continuing Financing Activities',\n 'Net Issuance Payments of Debt',\n 'Net Long Term Debt Issuance',\n 'Long Term Debt Issuance',\n 'Long Term Debt Payments',\n 'Net Short Term Debt Issuance',\n 'Short Term Debt Payments',\n 'Net Common Stock Issuance',\n 'Common Stock Issuance',\n 'Common Stock Payments',\n 'Cash Dividends Paid',\n 'Common Stock Dividend Paid',\n 'Net Other Financing Charges',\n 'End Cash Position',\n 'Changes in Cash',\n 'Beginning Cash Position',\n 'Income Tax Paid Supplemental Data',\n 'Interest Paid Supplemental Data',\n 'Capital Expenditure',\n 'Issuance of Capital Stock',\n 'Issuance of Debt',\n 'Repayment of Debt',\n 'Repurchase of Capital Stock',\n 'Free Cash Flow']\nlist_CF_asdf = pd.DataFrame(list_CF)\n\n\nCF = pd.DataFrame(index=list_CF,columns=ticker)\n\n\nfor i in range(0,1):\n    try:\n        Temporary_CF = pd.DataFrame(index=range(150),columns=range(2))\n        driver = webdriver.Chrome(ChromeDriverManager().install())\n        is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/cash-flow?p=' + ticker[i]\n        driver.get(is_link)\n        time.sleep(1)\n        link1 = driver.find_element_by_xpath('//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[2]/button/div')\n        link1.click()\n        time.sleep(1)\n        html = driver.execute_script('return document.body.innerHTML;')\n        soup = BeautifulSoup(html,'lxml')\n        features_CF = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg Bgc($lv1BgColor) fi-row:h_Bgc($hoverBgColor) D(tbc)')\n        features_CF_bis = soup.find_all('span', class_='Va(m)')\n        if features_CF_bis[0].text != 'Operating Cash Flow':\n            features_CF_bis = features_CF_bis[1:]\n            if len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n                j = 0\n                for j in range(0,len(features_CF),3):\n                    n=int(j/3)\n                    Temporary_CF.iloc[n,1] = features_CF[j].text\n                t = 0\n                for t in range(0,len(features_CF_bis)):\n                    Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n                Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n                Temporary_CF.set_index(0,inplace=True)\n                CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n            elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == (2 or 3):\n                j = 0\n                for j in range(0,len(features_CF),2):\n                    n=int(j/2)\n                    Temporary_CF.iloc[n,1] = features_CF[j].text\n                t = 0\n                for t in range(0,len(features_CF_bis)):\n                    Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n                Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n                Temporary_CF.set_index(0,inplace=True)\n                CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n            elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == (1 or 0):\n                j = 0\n                for j in range(0,len(features_CF),1):\n                    n=int(j/1)\n                    Temporary_CF.iloc[n,1] = features_CF[j].text\n                t = 0\n                for t in range(0,len(features_CF_bis)):\n                    Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n                Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n                Temporary_CF.set_index(0,inplace=True)\n                CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n            j = 0\n            for j in range(0,len(features_CF),3):\n                n=int(j/3)\n                Temporary_CF.iloc[n,1] = features_CF[j].text\n            t = 0\n            for t in range(0,len(features_CF_bis)):\n                Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n            Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n            Temporary_CF.set_index(0,inplace=True)\n            CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == (2 or 3):\n            j = 0\n            for j in range(0,len(features_CF),2):\n                n=int(j/2)\n                Temporary_CF.iloc[n,1] = features_CF[j].text\n            t = 0\n            for t in range(0,len(features_CF_bis)):\n                Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n            Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n            Temporary_CF.set_index(0,inplace=True)\n            CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == (1 or 0):\n            j = 0\n            for j in range(0,len(features_CF),1):\n                n=int(j/1)\n                Temporary_CF.iloc[n,1] = features_CF[j].text\n            t = 0\n            for t in range(0,len(features_CF_bis)):\n                Temporary_CF.iloc[t,0] = features_CF_bis[t].text\n            Temporary_CF = Temporary_CF.merge(list_CF_asdf,how='right')\n            Temporary_CF.set_index(0,inplace=True)\n            CF.loc[:,ticker[i]] = Temporary_CF.loc[sorted(Temporary_CF.index.to_list(), key=CF.index.to_list().index),:][1].values\n    except:\n        continue\n    driver.close()\n\n\nCF\n\n\n#CF.to_csv(r'CF.csv')"
  },
  {
    "objectID": "projet_2.html#esg",
    "href": "projet_2.html#esg",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "ESG",
    "text": "ESG\n\nESG =  pd.DataFrame(index=range(5),columns=ticker)\n\n\nfor i in range(0,len(ticker)):\n    driver = webdriver.Chrome(ChromeDriverManager().install())\n    is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/sustainability?p=' + ticker[i]\n    driver.get(is_link)\n    time.sleep(1)\n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')\n    try:\n        ESG.iloc[0,i] = soup.find('div',class_='Fz(36px) Fw(600) D(ib) Mend(5px)').text\n        ESG.iloc[1,i] = soup.find_all('div',class_='D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)')[0].text\n        ESG.iloc[2,i] = soup.find_all('div',class_='D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)')[1].text\n        ESG.iloc[3,i] = soup.find_all('div',class_='D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)')[2].text\n        ESG.iloc[4,i] = soup.find('div',class_='D(ib) Fz(36px) Fw(500)').text\n        ESG = ESG.rename(columns={i: ticker[i]})\n        driver.close()\n    except:\n        driver.close()\n        continue\n    \nESG = ESG.rename( index={0: 'ESG',1: 'E',2: 'S',3: 'G',4: 'Controverse'})\n\n\n#ESG.to_csv(r\"ESG.csv\")"
  },
  {
    "objectID": "projet_2.html#income-statement",
    "href": "projet_2.html#income-statement",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Income statement",
    "text": "Income statement\n\nlist_IS = ['Total Revenue',\n 'Operating Revenue',\n 'Cost of Revenue',\n 'Gross Profit',\n 'Operating Expense',\n 'Selling General and Administrative',\n 'Research & Development',\n 'Operating Income',\n 'Net Non Operating Interest Income Expense',\n 'Interest Income Non Operating',\n 'Interest Expense Non Operating',\n 'Other Income Expense',\n 'Other Non Operating Income Expenses',\n 'Pretax Income',\n 'Tax Provision',\n 'Net Income Common Stockholders',\n 'Net Income',\n 'Net Income Including Non-Controlling Interests',\n 'Net Income Continuous Operations',\n 'Diluted NI Available to Com Stockholders',\n 'Basic EPS',\n 'Diluted EPS',\n 'Basic Average Shares',\n 'Diluted Average Shares',\n 'Total Operating Income as Reported',\n 'Total Expenses',\n 'Net Income from Continuing & Discontinued Operation',\n 'Normalized Income',\n 'Interest Income',\n 'Interest Expense',\n 'Net Interest Income',\n 'EBIT',\n 'EBITDA',\n 'Reconciled Cost of Revenue',\n 'Reconciled Depreciation',\n 'Net Income from Continuing Operation Net Minority Interest',\n 'Normalized EBITDA',\n 'Tax Rate for Calcs',\n 'Tax Effect of Unusual Items']\n    \n    \nlist_IS_asdf = pd.DataFrame(list_IS)\n\n\nIS = pd.DataFrame(index=list_IS,columns=ticker)\n\n\nfor i in range(0,len(ticker)):\n    try:\n        Temporary_IS = pd.DataFrame(index=range(150),columns=range(2))\n        driver = webdriver.Chrome(ChromeDriverManager().install())\n        is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/financials?p=' + ticker[i]\n        driver.get(is_link)\n        time.sleep(1)\n        link1 = driver.find_element_by_xpath('//*[@id=\"Col1-1-Financials-Proxy\"]/section/div[2]/button/div')\n        link1.click()\n        time.sleep(1)\n        html = driver.execute_script('return document.body.innerHTML;')\n        soup = BeautifulSoup(html,'lxml')\n        features_IS_bis = soup.find_all('span', class_='Va(m)')\n        if features_IS_bis[0].text != 'Total Revenue':\n            features_IS_bis = features_IS_bis[1:]\n            if len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 3:\n                features_IS = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg Bgc($lv1BgColor) fi-row:h_Bgc($hoverBgColor) D(tbc)')\n                features_IS_bis = soup.find_all('span', class_='Va(m)')\n                j = 0\n                for j in range(0,len(features_IS),2):\n                    m=int(j/2)\n                    Temporary_IS.iloc[m,1] = features_IS[j].text\n                t = 0\n                for t in range(0,len(features_IS_bis)):\n                    Temporary_IS.iloc[t,0] = features_IS_bis[t].text\n                Temporary_IS = Temporary_IS.merge(list_IS_asdf,how='right')\n                Temporary_IS.set_index(0,inplace=True)\n                IS.loc[:,ticker[i]] = Temporary_IS.loc[sorted(Temporary_IS.index.to_list(), key=IS.index.to_list().index),:][1].values\n            elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n                features_IS = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg Bgc($lv1BgColor) fi-row:h_Bgc($hoverBgColor) D(tbc)')\n                features_IS_bis = soup.find_all('span', class_='Va(m)')\n                j = 0\n                for j in range(0,len(features_IS),3):\n                    m=int(j/3)\n                    Temporary_IS.iloc[m,1] = features_IS[j].text\n                t = 0\n                for t in range(0,len(features_IS_bis)):\n                    Temporary_IS.iloc[t,0] = features_IS_bis[t].text\n                Temporary_IS = Temporary_IS.merge(list_IS_asdf,how='right')\n                Temporary_IS.set_index(0,inplace=True)\n                IS.loc[:,ticker[i]] = Temporary_IS.loc[sorted(Temporary_IS.index.to_list(), key=IS.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 3:\n            features_IS = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg Bgc($lv1BgColor) fi-row:h_Bgc($hoverBgColor) D(tbc)')\n            features_IS_bis = soup.find_all('span', class_='Va(m)')\n            j = 0\n            for j in range(0,len(features_IS),2):\n                m=int(j/2)\n                Temporary_IS.iloc[m,1] = features_IS[j].text\n            t = 0\n            for t in range(0,len(features_IS_bis)):\n                Temporary_IS.iloc[t,0] = features_IS_bis[t].text\n            Temporary_IS = Temporary_IS.merge(list_IS_asdf,how='right')\n            Temporary_IS.set_index(0,inplace=True)\n            IS.loc[:,ticker[i]] = Temporary_IS.loc[sorted(Temporary_IS.index.to_list(), key=IS.index.to_list().index),:][1].values\n        elif len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b)')) +  len(soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg D(ib) Fw(b) Bgc($lv1BgColor)')) == 4:\n            features_IS = soup.find_all('div', class_='Ta(c) Py(6px) Bxz(bb) BdB Bdc($seperatorColor) Miw(120px) Miw(140px)--pnclg Bgc($lv1BgColor) fi-row:h_Bgc($hoverBgColor) D(tbc)')\n            features_IS_bis = soup.find_all('span', class_='Va(m)')\n            j = 0\n            for j in range(0,len(features_IS),3):\n                m=int(j/3)\n                Temporary_IS.iloc[m,1] = features_IS[j].text\n            t = 0\n            for t in range(0,len(features_IS_bis)):\n                Temporary_IS.iloc[t,0] = features_IS_bis[t].text\n            Temporary_IS = Temporary_IS.merge(list_IS_asdf,how='right')\n            Temporary_IS.set_index(0,inplace=True)\n            IS.loc[:,ticker[i]] = Temporary_IS.loc[sorted(Temporary_IS.index.to_list(), key=IS.index.to_list().index),:][1].values\n    except:\n        continue\n    driver.close()\n\n\n#IS.to_csv(r'IS.csv')"
  },
  {
    "objectID": "projet_2.html#statistics",
    "href": "projet_2.html#statistics",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Statistics",
    "text": "Statistics\n\nlist_i=[]\ni=0\ndriver = webdriver.Chrome(ChromeDriverManager().install())\nis_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/key-statistics?p=' + ticker[i]  \ndriver.get(is_link)\nhtml = driver.execute_script('return document.body.innerHTML;')    \nsoup = BeautifulSoup(html,'lxml')\nfeatures_stats = soup.find_all('td',class_='Pos(st)')\nt = 0\nfor t in range(0,len(features_stats)):\n    list_i.append(features_stats[t].text )\n\n\nStats = pd.DataFrame(index=list_i, columns=ticker)\n\n\ni = 0\nfor i in range(0,len(ticker)):\n    try:\n        driver = webdriver.Chrome(ChromeDriverManager().install())\n        is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/key-statistics?p=' + ticker[i]       \n        driver.get(is_link)\n        time.sleep(1)\n        html = driver.execute_script('return document.body.innerHTML;')\n        soup = BeautifulSoup(html,'lxml')\n        features_stats = soup.find_all('td',class_='Fw(500) Ta(end) Pstart(10px) Miw(60px)')\n        j = 0\n        for j in range(0,len(features_stats)):\n            Stats.iloc[j,i] = features_stats[j].text\n    except:\n        continue\n    driver.close()\n\n\n#Stats.to_csv(r'Stats.csv')"
  },
  {
    "objectID": "projet_2.html#profile",
    "href": "projet_2.html#profile",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "Profile",
    "text": "Profile\n\nProfile =  pd.DataFrame(index=range(3),columns=ticker)\n\n\ni = 0\nfor i in range(0,len(ticker)):\n    driver = webdriver.Chrome(ChromeDriverManager().install())\n    is_link = 'https://finance.yahoo.com/quote/' + ticker[i] +'/profile?p=' + ticker[i]\n    driver.get(is_link)\n    time.sleep(1)\n    html = driver.execute_script('return document.body.innerHTML;')\n    soup = BeautifulSoup(html,'lxml')\n    try:\n        Profile.iloc[0,i] = soup.find_all('span',class_='Fw(600)')[0].text\n        Profile.iloc[2,i] = soup.find_all('span',class_='Fw(600)')[2].text\n        Profile.iloc[3,i] = soup.find_all('td',class_='Ta(end)')[0].text\n        Profile= Profile.rename(columns={i: ticker[i]})\n    except:\n        continue\n    driver.close()\n    \nProfile = Profile.rename( index={0: 'Sector',1: 'Employee',2: 'Salary'})\n\n\n#Profile.to_csv(r'Profile.csv')"
  },
  {
    "objectID": "projet_2.html#a-data-cleaning",
    "href": "projet_2.html#a-data-cleaning",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "a) Data Cleaning",
    "text": "a) Data Cleaning\nDuring the proposal, I found easier and less messy to clean my data datasets by datasets.\n\nFeatures variables:\n\n\nBalance sheet\n\n# Import package\n# import pandas as pd\n\n\n# Import data, reset and rename index\nBS = pd.read_csv('BS.csv')\nBS.set_index('Unnamed: 0',inplace=True)\nBS.index.rename('Index',inplace=True)\n\nI also replace commas and ‘-’ symbol by nothing. It is very important to distinguish the case where ‘-’ is used to indicate ‘no value available’ and the case where ‘-’ is simply a negative value. In order to do that, I use the following regex expression [-]$.\nI need to replace the nan values by nothing.\n\n# NaN values to ''\nBS = BS.astype(str).replace(to_replace='nan', value='',regex=True)\n\nI apply the function created above to my dataframe.\n\nfor column in BS.columns:\n    BS[column] = convert_to_numeric(BS[column].astype(str))\n\n\n# NaN values to ''\nBS = BS.astype(str).replace(to_replace='nan', value='',regex=True)\n\nHere a small sample of the balance sheet dataframe:\n\nBS.head(2)\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Assets\n323888000.0\n301311000.0\n321195000.0\n553905000.0\n319616000.0\n319616000.0\n159316000.0\n1333425000.0\n1333425000.0\n52148000.0\n...\n581204000.0\n585615.0\n8343900.0\n353556.0\n3043897.0\n\n2598518.0\n6092074.0\n\n825463.0\n\n\nCurrent Assets\n143713000.0\n181915000.0\n132733000.0\n\n174296000.0\n174296000.0\n75670000.0\n317647000.0\n317647000.0\n26717000.0\n...\n292258000.0\n249323.0\n2118800.0\n314196.0\n607821.0\n\n708367.0\n\n\n509031.0\n\n\n\n\n2 rows × 3000 columns\n\n\n\n\n\nIncome statement\n\n# Import data, reset and rename index\nProfile = pd.read_csv('Profile.csv')\nProfile.set_index('Unnamed: 0',inplace=True)\nProfile.index.rename('Index',inplace=True)\n\nI need to delete commas for the Employee variable and be sure that this variable contains only numerical character. In order to do that, I use the following regex expression [^0-9].\n\n# Replace ',' and check numerical character\nfor i in range(0,Profile.shape[1]):\n    try:\n        Profile.iloc[1,i] = Profile.iloc[1,i].replace(',','')\n        Profile.iloc[1,i] = str(Profile.iloc[1,i])\n        Profile.iloc[1,i] = re.sub('[^0-9]','',Profile.iloc[1,i])        \n    except:\n        Profile.iloc[1,i] = ''\n\nI need to replace the nan values by nothing.\n\n# NaN values to ''\nBS = BS.astype(str).replace(to_replace='nan', value='',regex=True)\n\n\n# Import package \n# import re\n\nI apply the function created above to my dataframe.\n\nfor column in BS.columns:\n    BS[column] = convert_to_numeric(BS[column].astype(str))\n\n\n# NaN values to ''\nBS = BS.astype(str).replace(to_replace='nan', value='',regex=True)\n\nHere a small sample of the balance sheet dataframe:\n\nBS.head(2)\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Assets\n323888000.0\n301311000.0\n321195000.0\n553905000.0\n319616000.0\n319616000.0\n159316000.0\n1333425000.0\n1333425000.0\n52148000.0\n...\n581204000.0\n585615.0\n8343900.0\n353556.0\n3043897.0\n\n2598518.0\n6092074.0\n\n825463.0\n\n\nCurrent Assets\n143713000.0\n181915000.0\n132733000.0\n\n174296000.0\n174296000.0\n75670000.0\n317647000.0\n317647000.0\n26717000.0\n...\n292258000.0\n249323.0\n2118800.0\n314196.0\n607821.0\n\n708367.0\n\n\n509031.0\n\n\n\n\n2 rows × 3000 columns\n\n\n\n\n\nIncome statement\n\n# Import data, reset and rename index\nProfile = pd.read_csv('Profile.csv')\nProfile.set_index('Unnamed: 0',inplace=True)\nProfile.index.rename('Index',inplace=True)\n\nI need to delete commas for the Employee variable and be sure that this variable contains only numerical character. In order to do that, I use the following regex expression [^0-9].\n\n# Replace ',' and check numerical character\nfor i in range(0,Profile.shape[1]):\n    try:\n        Profile.iloc[1,i] = Profile.iloc[1,i].replace(',','')\n        Profile.iloc[1,i] = str(Profile.iloc[1,i])\n        Profile.iloc[1,i] = re.sub('[^0-9]','',Profile.iloc[1,i])        \n    except:\n        Profile.iloc[1,i] = ''\n\nAs before, I have to replace ‘M’ and ‘k’ by their numeric conversions (x1’000’000 and x1’000).\n\n# NaN values to ''\nProfile = Profile.astype(str).replace(to_replace='nan', value='',regex=True)\n\n\n# Numeric conversion \nfor i in range(0,Profile.shape[1]):\n    if '' in Profile.iloc[2,i]:\n        pass\n    if 'M' in Profile.iloc[2,i]:\n        Profile.iloc[2,i] = Profile.iloc[2,i].replace('M','') \n        Profile.iloc[2,i] = float(Profile.iloc[2,i])\n        Profile.iloc[2,i] = Profile.iloc[2,i]*1000000\n    elif 'k' in Profile.iloc[2,i]:\n        Profile.iloc[2,i] = Profile.iloc[2,i].replace('k','') \n        Profile.iloc[2,i] = float(Profile.iloc[2,i])\n        Profile.iloc[2,i] = Profile.iloc[2,i]*1000\n    else:\n        pass\n\nHere a small sample of the profile dataframe:\n\nProfile\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSector\nTechnology\nTechnology\nConsumer Cyclical\n\nCommunication Services\nCommunication Services\nCommunication Services\nCommunication Services\nCommunication Services\nConsumer Cyclical\n...\nIndustrials\nConsumer Defensive\nIndustrials\nHealthcare\nConsumer Cyclical\nLawyer: How Jen Shah's 'Real Housewives' money...\nIndustrials\nFicial Services\nFicial Services\nHealthcare\n\n\nEmployee\n147000\n163000\n1298000\n\n135301\n135301\n58604\n85858\n85858\n70757\n...\n21725\n1451\n112394\n223\n3924\n\n4362\n1038\n\n1325\n\n\nSalary\n1.477e+07\n1.36e+07\n1.68e+06\n\n4.01e+06\n4.01e+06\n2.342e+07\n7.1e+06\n7.1e+06\n23760\n...\n\n906030\n1.02e+06\n954110\n\n0.00\n982410\n1.86e+06\n969520\n1.36e+06\n\n\n\n\n3 rows × 3000 columns\n\n\n\n\n\nStatistics\n\n# Import data, reset and rename index\nStats = pd.read_csv('Stats.csv')\nStats.set_index('Unnamed: 0',inplace=True)\nStats.index.rename('Index',inplace=True)\n\nI check the features in order to do a first subset features selection:\n\nStats.AAPL\n\nIndex\nMarket Cap (intraday) 5                                  2.12T\nEnterprise Value 3                                       2.15T\nTrailing P/E                                             34.23\nForward P/E 1                                            26.85\nPEG Ratio (5 yr expected) 1                               1.84\nPrice/Sales (ttm)                                         7.20\nPrice/Book (mrq)                                         32.07\nEnterprise Value/Revenue 3                                7.31\nEnterprise Value/EBITDA 6                                25.23\nBeta (5Y Monthly)                                         1.22\n52-Week Change 3                                           NaN\nS&P500 52-Week Change 3                                    NaN\n52 Week High 3                                          145.09\n52 Week Low 3                                            64.75\n50-Day Moving Average 3                                 123.31\n200-Day Moving Average 3                                123.35\nAvg Vol (3 month) 3                                    109.22M\nAvg Vol (10 day) 3                                      89.94M\nShares Outstanding 5                                    16.79B\nFloat                                                   16.77B\n% Held by Insiders 1                                     0.08%\n% Held by Institutions 1                                59.79%\nShares Short (Mar 14, 2021) 4                          107.01M\nShort Ratio (Mar 14, 2021) 4                              0.89\nShort % of Float (Mar 14, 2021) 4                        0.64%\nShort % of Shares Outstanding (Mar 14, 2021) 4           0.64%\nShares Short (prior month Feb 11, 2021) 4               88.33M\nForward Annual Dividend Rate 4                            0.82\nForward Annual Dividend Yield 4                          0.65%\nTrailing Annual Dividend Rate 3                           0.81\nTrailing Annual Dividend Yield 3                         0.64%\n5 Year Average Dividend Yield 4                           1.39\nPayout Ratio 4                                          21.77%\nDividend Date 3                                   Feb 10, 2021\nEx-Dividend Date 4                                Feb 04, 2021\nLast Split Factor 2                                        4:1\nLast Split Date 3                                 Aug 30, 2020\nFiscal Year Ends                                  Sep 25, 2020\nMost Recent Quarter (mrq)                         Dec 25, 2020\nProfit Margin                                           21.74%\nOperating Margin (ttm)                                  25.25%\nReturn on Assets (ttm)                                  13.36%\nReturn on Equity (ttm)                                  82.09%\nRevenue (ttm)                                          294.14B\nRevenue Per Share (ttm)                                  17.13\nQuarterly Revenue Growth (yoy)                          21.40%\nGross Profit (ttm)                                     104.96B\nEBITDA                                                  85.16B\nNet Income Avi to Common (ttm)                          63.93B\nDiluted EPS (ttm)                                         3.69\nQuarterly Earnings Growth (yoy)                         29.30%\nTotal Cash (mrq)                                        76.83B\nTotal Cash Per Share (mrq)                                4.58\nTotal Debt (mrq)                                       112.04B\nTotal Debt/Equity (mrq)                                 169.19\nCurrent Ratio (mrq)                                       1.16\nBook Value Per Share (mrq)                                3.94\nOperating Cash Flow (ttm)                               88.92B\nLevered Free Cash Flow (ttm)                            66.89B\nName: AAPL, dtype: object\n\n\nI am interested in the following features:\n\n# Subset of features\nStats = Stats.loc[['Trailing P/E ' ,\n                   'PEG Ratio (5 yr expected) 1',\n                   'Quarterly Revenue Growth (yoy)',\n                   'Diluted EPS (ttm)',\n                   'Quarterly Earnings Growth (yoy)',\n                   'Payout Ratio 4',\n                   'Trailing Annual Dividend Yield 3',\n                   'Beta (5Y Monthly) '],:]\n\nI rename corectly the variables.\n\n# Rename features\nStats.rename({'Trailing P/E ':'Trailing P/E',\n              'PEG Ratio (5 yr expected) 1':'PEG Ratio',\n              'Payout Ratio 4':'Payout Ratio',\n              'Trailing Annual Dividend Yield 3':'Trailing Annual Dividend Yield',\n              'Beta (5Y Monthly) ':'Beta'},inplace=True)\n\n\nStats\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrailing P/E\n34.23\n36.96\n77.07\n267.81\n37.69\n37.96\n30.35\n33.60\n33.66\n1,080.66\n...\n8.92\n19.60\nNaN\nNaN\nNaN\nNaN\nNaN\n14.47\n10.87\n97.60\n\n\nPEG Ratio\n1.84\n1.87\n1.71\nNaN\n1.72\n1.73\n1.15\n48.59\n58.93\n4.82\n...\nNaN\n12.17\nNaN\n-0.16\nNaN\nNaN\n1.76\nNaN\nNaN\n2.49\n\n\nQuarterly Revenue Growth (yoy)\n21.40%\n16.70%\n43.60%\nNaN\n23.50%\n23.50%\n33.20%\n26.40%\n26.40%\n45.50%\n...\n-9.90%\n-30.50%\n-12.20%\n-41.30%\n-85.90%\nNaN\n22.70%\n6.70%\n16.80%\n23.10%\n\n\nDiluted EPS (ttm)\n3.69\n6.71\n41.83\nNaN\n58.61\n58.61\n10.09\n2.52\n2.52\n0.64\n...\n1.48\n0.75\n-0.2940\n-2.50\n-4.41\nNaN\n-0.34\n3.22\n1,103.90\n0.32\n\n\nQuarterly Earnings Growth (yoy)\n29.30%\n32.70%\n121.00%\nNaN\n42.70%\n42.70%\n52.70%\n174.80%\n174.80%\n157.10%\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n119.70%\n33.10%\n187.90%\n-92.90%\n\n\nPayout Ratio\n21.77%\n31.15%\n0.00%\nNaN\n0.00%\n0.00%\n0.00%\n0.00%\n6.70%\n0.00%\n...\nNaN\n0.00%\n0.00%\n0.00%\n0.00%\nNaN\nNaN\n6.21%\nNaN\n115.63%\n\n\nTrailing Annual Dividend Yield\n0.64%\n0.86%\nNaN\n0.12%\nNaN\nNaN\nNaN\n0.25%\n0.25%\nNaN\n...\n2.07%\nNaN\nNaN\nNaN\nNaN\nNaN\n5.96%\n0.43%\n1.75%\n1.16%\n\n\nBeta\n1.22\n0.79\n1.12\nNaN\n1.00\n1.00\n1.29\n0.50\n0.50\n2.01\n...\n1.20\n2.41\n2.01\n1.35\n-0.91\nNaN\n1.77\n0.93\n0.43\n0.53\n\n\n\n\n8 rows × 3000 columns\n\n\n\nI have to transform the ‘%’ by its numeric conversion (/100).\n\nStats = Stats.astype(str).replace(to_replace='nan', value='',regex=True)\n\n\n# Numeric conversion\nfor j in range(0,Stats.shape[0]):\n    for i in range(0,Stats.shape[1]):\n        if '%' in Stats.iloc[j,i]:\n            Stats.iloc[j,i] = Stats.iloc[j,i].replace(',','') \n            Stats.iloc[j,i] = Stats.iloc[j,i].replace('%','') \n            Stats.iloc[j,i] = float(Stats.iloc[j,i])\n            Stats.iloc[j,i] = Stats.iloc[j,i]/100\n        elif '∞' in Stats.iloc[j,i]:\n            Stats.iloc[j,i]=''\n\nI also replace commas and ‘-’.\n\nfor column in Stats.columns:\n    Stats[column] = convert_to_numeric(Stats[column].astype(str))\n\n\n# NaN values to ''\nStats = Stats.astype(str).replace(to_replace='nan', value='',regex=True)\n\nHere a small sample of the basic statistics dataframe:\n\nStats.head(5)\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrailing P/E\n34.23\n36.96\n77.07\n267.81\n37.69\n37.96\n30.35\n33.6\n33.66\n1080.66\n...\n8.92\n19.6\n\n\n\n\n\n14.47\n10.87\n97.6\n\n\nPEG Ratio\n1.84\n1.87\n1.71\n\n1.72\n1.73\n1.15\n48.59\n58.93\n4.82\n...\n\n12.17\n\n-0.16\n\n\n1.76\n\n\n2.49\n\n\nQuarterly Revenue Growth (yoy)\n0.214\n0.16699999999999998\n0.436\n\n0.235\n0.235\n0.332\n0.264\n0.264\n0.455\n...\n-0.099\n-0.305\n-0.122\n-0.413\n-0.8590000000000001\n\n0.227\n0.067\n0.168\n0.231\n\n\nDiluted EPS (ttm)\n3.69\n6.71\n41.83\n\n58.61\n58.61\n10.09\n2.52\n2.52\n0.64\n...\n1.48\n0.75\n-0.294\n-2.5\n-4.41\n\n-0.34\n3.22\n1103.9\n0.32\n\n\nQuarterly Earnings Growth (yoy)\n0.293\n0.327\n1.21\n\n0.4270000000000001\n0.4270000000000001\n0.527\n1.7480000000000002\n1.7480000000000002\n1.571\n...\n\n\n\n\n\n\n1.197\n0.331\n1.879\n-0.929\n\n\n\n\n5 rows × 3000 columns\n\n\n\n\n\nESG\n\n# Import data, reset and rename index\nESG = pd.read_csv('ESG.csv')\nESG.set_index('Unnamed: 0',inplace=True)\nESG.index.rename('Index',inplace=True)\n\n\n# NaN values to ''\nESG = ESG.astype(str).replace(to_replace='nan', value='',regex=True)\n\nHere a small sample of the ESG dataframe:\n\nESG.head(5)\n\n\n\n\n\n\n\n\nAAPL\nMSFT\nAMZN\nUSB-PA\nGOOGL\nGOOG\nFB\nTCTZF\nTCEHY\nTSLA\n...\nNSHBY\nSTKL\nFGROF\nHRTX\nMSC\nMTZXF\nCODI\nCASH\nFINN\nLMNX\n\n\nIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nESG\n17.0\n15.0\n27.0\n\n22.0\n\n32.0\n25.0\n\n31.0\n...\n\n\n\n\n\n\n37\n\n\n38\n\n\nE\n0.1\n0.5\n5.1\n\n0.5\n\n1.4\n2.6\n\n3.0\n...\n\n\n\n\n\n\n11\n\n\n12\n\n\nS\n7.7\n9.4\n12.5\n\n9.9\n\n17.9\n13.2\n\n17.3\n...\n\n\n\n\n\n\n16\n\n\n19\n\n\nG\n8.9\n4.9\n9.8\n\n11.9\n\n12.3\n9.1\n\n11.0\n...\n\n\n\n\n\n\n9\n\n\n8\n\n\nControverse\n3.0\n3.0\n3.0\n\n4.0\n\n4.0\n2.0\n\n3.0\n...\n\n\n\n\n\n\n2\n\n\n2\n\n\n\n\n5 rows × 3000 columns\n\n\n\n\n\nConcatenate\nNow that my dataframes are more or less cleaned, I concatenate the 6 dataframes. Then, I transpose the dataframe to have the datapoints in the index and the features in the columns.\n\n# Concatenate and transpore \nframes = [BS,IS ,CF,Profile,Stats,ESG]\ndf = pd.concat(frames)\ndf = df.T\n\nLet’s have a first look of the concatenated dataframe:\n\ndf.head(5)\n\n\n\n\n\n\n\nIndex\nTotal Assets\nCurrent Assets\nCash, Cash Equivalents & Short Term Investments\nCash And Cash Equivalents\nCash\nCash Equivalents\nOther Short Term Investments\nReceivables\nAccounts receivable\nGross Accounts Receivable\n...\nDiluted EPS (ttm)\nQuarterly Earnings Growth (yoy)\nPayout Ratio\nTrailing Annual Dividend Yield\nBeta\nESG\nE\nS\nG\nControverse\n\n\n\n\nAAPL\n323888000.0\n143713000.0\n90943000.0\n38016000.0\n17773000.0\n20243000.0\n52927000.0\n37445000.0\n16120000.0\n\n...\n3.69\n0.293\n0.2177\n0.0064\n1.22\n17.0\n0.1\n7.7\n8.9\n3.0\n\n\nMSFT\n301311000.0\n181915000.0\n136527000.0\n13576000.0\n\n\n122951000.0\n32011000.0\n32011000.0\n32799000.0\n...\n6.71\n0.327\n0.3115\n0.0086\n0.79\n15.0\n0.5\n9.4\n4.9\n3.0\n\n\nAMZN\n321195000.0\n132733000.0\n84396000.0\n42122000.0\n\n\n42274000.0\n24542000.0\n24542000.0\n25542000.0\n...\n41.83\n1.21\n0.0\n\n1.12\n27.0\n5.1\n12.5\n9.8\n3.0\n\n\nUSB-PA\n553905000.0\n\n\n62580000.0\n\n\n136438000.0\n13706000.0\n\n\n...\n\n\n\n0.0012\n\n\n\n\n\n\n\n\nGOOGL\n319616000.0\n174296000.0\n136694000.0\n26465000.0\n\n\n110229000.0\n31384000.0\n30930000.0\n31683000.0\n...\n58.61\n0.4270000000000001\n0.0\n\n1.0\n22.0\n0.5\n9.9\n11.9\n4.0\n\n\n\n\n5 rows × 177 columns\n\n\n\n\n\nDuplicated values\nI have to delete duplicated values.\n\n# Import package\n# import numpy as np\n\n\ndf.replace('',np.nan,inplace=True)\n\nI compute how many duplicated rows I have:\n\ndf.duplicated().sum()\n\n109\n\n\nI have 109 duplicated rows. Let’s have a look at these duplicated rows:\n\ndf[df.duplicated()].head(10)\n\n\n\n\n\n\n\nIndex\nTotal Assets\nCurrent Assets\nCash, Cash Equivalents & Short Term Investments\nCash And Cash Equivalents\nCash\nCash Equivalents\nOther Short Term Investments\nReceivables\nAccounts receivable\nGross Accounts Receivable\n...\nDiluted EPS (ttm)\nQuarterly Earnings Growth (yoy)\nPayout Ratio\nTrailing Annual Dividend Yield\nBeta\nESG\nE\nS\nG\nControverse\n\n\n\n\nPVTA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nRBS-PL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nHORI\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNWSKF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAEH\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAED\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAEB\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMWATF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSLYQF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nTLPH\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 177 columns\n\n\n\nWe can see that the duplicated rows are mainly NaN rows.\n\n# Drop duplicated rows \ndf.drop_duplicates(inplace=True)\n\nWe still have a duplicated value with Google, BRK and Tencent. I remove these rows:\n\ndf.drop('GOOG',axis=0,inplace=True)\ndf.drop('BRK-B',axis=0,inplace=True)\ndf.drop('TCEHY',axis=0,inplace=True)\n\nI verify that I have no deplicated values anymore:\n\ndf.duplicated().sum()\n\n0\n\n\nWe can have a look at a random company:\n\n# Sample datapoint\ndf.iloc[608,:]\n\nIndex\nTotal Assets                                                20696000.0\nCurrent Assets                                               5050800.0\nCash, Cash Equivalents & Short Term Investments              1416700.0\nCash And Cash Equivalents                                    1416700.0\nCash                                                               NaN\nCash Equivalents                                                   NaN\nOther Short Term Investments                                       NaN\nReceivables                                                        NaN\nAccounts receivable                                                NaN\nGross Accounts Receivable                                          NaN\nAllowance For Doubtful Accounts Receivable                         NaN\nOther Receivables                                                  NaN\nInventory                                                    3427000.0\nOther Current Assets                                          207100.0\nTotal non-current assets                                    15645200.0\nNet PPE                                                     10440400.0\nGross PPE                                                   15205400.0\nProperties                                                         0.0\nLand And Improvements                                         238700.0\nMachinery Furniture Equipment                                4229400.0\nLeases                                                       2631700.0\nAccumulated Depreciation                                    -4765000.0\nGoodwill And Other Intangible Assets                         5084400.0\nGoodwill                                                     1984400.0\nOther Intangible Assets                                      3100000.0\nInvestments And Advances                                           NaN\nInvestment in Financial Assets                                     NaN\nAvailable for Sale Securities                                      NaN\nOther Non Current Assets                                       97200.0\nTotal Liabilities Net Minority Interest                     13410700.0\n                                                          ...         \nCash Dividends Paid                                                NaN\nCommon Stock Dividend Paid                                         NaN\nNet Other Financing Charges                                   -16900.0\nEnd Cash Position                                            1463600.0\nChanges in Cash                                               876700.0\nBeginning Cash Position                                       586000.0\nIncome Tax Paid Supplemental Data                             357700.0\nInterest Paid Supplemental Data                               152900.0\nCapital Expenditure                                          -898800.0\nIssuance of Capital Stock                                          NaN\nIssuance of Debt                                              750000.0\nRepayment of Debt                                           -1300000.0\nRepurchase of Capital Stock                                  -400000.0\nFree Cash Flow                                               1817500.0\nSector                                              Consumer Defensive\nEmployee                                                         60217\nSalary                                                        1.63e+06\nTrailing P/E                                                     20.82\nPEG Ratio                                                         1.74\nQuarterly Revenue Growth (yoy)                     0.07200000000000001\nDiluted EPS (ttm)                                                 5.65\nQuarterly Earnings Growth (yoy)                                  3.088\nPayout Ratio                                                       0.0\nTrailing Annual Dividend Yield                                     NaN\nBeta                                                              0.88\nESG                                                                 18\nE                                                                    3\nS                                                                    9\nG                                                                    6\nControverse                                                          2\nName: DLTR, Length: 177, dtype: object\n\n\nThe company’s ticker is DLTR. The company is therefore ‘Dollar Tree’. We can see its sector ‘Consumer Defensive’. The consumer defensive sector includes companies that manufacture food, beverages, household and personal products, packaging, or tobacco. Dollar Tree is a large company with 60’217 employees.\n\n\nStore dataframe:\nI can finally store the cleaned dataframe:\n\n#df.to_csv(r'df.csv')\n\n\nTarget variable:\n\n\n# Import data, reset and rename index\ntarget = pd.read_excel(r'Target.xlsx')\ntarget.set_index('Unnamed: 0',inplace=True)\ntarget.index.rename('Index',inplace=True)\ntarget.columns = ['MC']\n\nHere a small sample of the target variable:\n\ntarget.head(5)\n\n\n\n\n\n\n\n\nMC\n\n\nIndex\n\n\n\n\n\nAAPL\n2113621852160\n\n\nMSFT\n1878540681216\n\n\nAMZN\n1624868257792\n\n\nUSB-PA\n1578203611136\n\n\nGOOGL\n1498601226240\n\n\n\n\n\n\n\nI divide by 1’000 to get the result in thousands of dollars. Therefore, it will remain consistent with the features variables metrics.\n\n# Numeric conversion \ntarget = target/1000\n\nI select only the subset of companies that have been selected before.\n\ntarget = target.loc[df.index,:]\n\n\ntarget.head(5)\n\n\n\n\n\n\n\n\nMC\n\n\n\n\nAAPL\n2.113622e+09\n\n\nMSFT\n1.878541e+09\n\n\nAMZN\n1.624868e+09\n\n\nUSB-PA\n1.578204e+09\n\n\nGOOGL\n1.498601e+09"
  },
  {
    "objectID": "projet_2.html#b-variable-pre-selection",
    "href": "projet_2.html#b-variable-pre-selection",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "b) Variable pre-selection:",
    "text": "b) Variable pre-selection:\nDuring the first proposal, we saw that I had webscrapped too many features. Thefore, I first select only the variabbles I am really interested in.\n\n# Subset selection\nBalanceSheet_variables =['Total Assets', \n             'Current Assets',\n             'Cash And Cash Equivalents',\n             'Receivables', \n             'Inventory',  \n             'Total non-current assets',\n             'Net PPE',\n             'Properties',\n             'Land And Improvements',\n             'Accumulated Depreciation',\n             'Goodwill And Other Intangible Assets', \n             'Total Liabilities Net Minority Interest',\n             'Current Liabilities',\n             'Current Debt And Capital Lease Obligation',\n             'Current Debt',\n             'Total Equity Gross Minority Interest',\n             \"Stockholders' Equity\",\n             'Capital Stock',\n             'Common Stock',\n             'Retained Earnings', \n             'Total Capitalization',\n             'Net Tangible Assets',\n             'Working Capital',\n             'Invested Capital',\n             'Tangible Book Value',\n             'Total Debt',\n             'Net Debt', \n             'Share Issued',\n             'Ordinary Shares Number']\n\n\n# Subset selection\nIncomeStatement_variables= ['Total Revenue', \n             'Operating Revenue', \n             'Cost of Revenue', \n             'Gross Profit',\n             'Operating Expense', \n             'Selling General and Administrative',\n             'Research & Development', \n             'Operating Income',\n             'Pretax Income',\n             'Tax Provision', \n             'Net Income',\n             'Net Income Including Non-Controlling Interests',\n             'Net Income Continuous Operations',\n             'Diluted NI Available to Com Stockholders', \n             'Total Expenses',\n             'Net Income from Continuing & Discontinued Operation',\n             'Interest Income',\n             'EBIT',\n             'EBITDA']\n\n\n# Subset selection\nCashFlow_variables = ['Capital Expenditure', \n       'Cash Dividends Paid',\n       'Cash Flow from Continuing Financing Activities',\n       'Cash Flow from Continuing Investing Activities',\n       'Cash Flow from Continuing Operating Activities', \n       'Depreciation & amortization',\n       'Financing Cash Flow', \n       'Free Cash Flow',\n       'Investing Cash Flow', \n       'Issuance of Capital Stock', \n       'Issuance of Debt',\n       'Operating Cash Flow',  \n       'Purchase of Business',\n       'Purchase of Intangibles', \n       'Purchase of Investment', \n       'Purchase of PPE',\n       'Stock based compensation']\n\n\n# Concatenate\nselection = BalanceSheet_variables + IncomeStatement_variables + CashFlow_variables\ndf = pd.concat([df.loc[:,selection], df.iloc[:,-16:]],axis=1)\n\n\ndf.shape\n\n(2888, 81)\n\n\nThe new dataframe includes 2888 companies and 81 features."
  },
  {
    "objectID": "projet_2.html#c-missing-values",
    "href": "projet_2.html#c-missing-values",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "c) Missing values",
    "text": "c) Missing values\nI also do a second selection based on the number of missing values per columns:\nI delete the columns and rows in several steps:\n\nFirst, I delete the features that have more than 1000 missing values.\n\n\nclean1 = df[df.columns[df.isnull().sum(axis=0) &lt; 1000]]\n\n\nSecond, I delete the companies with more than 30 missing values.\n\n\nclean2 = clean1[clean1.isnull().sum(axis=1) &lt;30]\n\n\nThen, I delete the features with more than 450 missing values.\n\n\nclean3 = clean2[clean2.columns[clean2.isnull().sum(axis=0) &lt; 450]]\n\n\nFinally, I delete the companies with more than 10 missing values.\n\n\nclean4 = clean3[clean3.isnull().sum(axis=1) &lt;10]\n\n\ndf = clean4.copy()\n\n\ndf.shape\n\n(2369, 42)\n\n\nThis is the list of my 42 remaining variables:\n\ndf.columns\n\nIndex(['Total Assets', 'Cash And Cash Equivalents', 'Net PPE',\n       'Goodwill And Other Intangible Assets',\n       'Total Liabilities Net Minority Interest',\n       'Total Equity Gross Minority Interest', 'Stockholders' Equity',\n       'Capital Stock', 'Common Stock', 'Retained Earnings',\n       'Total Capitalization', 'Net Tangible Assets', 'Invested Capital',\n       'Tangible Book Value', 'Total Debt', 'Share Issued',\n       'Ordinary Shares Number', 'Total Revenue', 'Operating Revenue',\n       'Operating Expense', 'Selling General and Administrative',\n       'Operating Income', 'Pretax Income', 'Tax Provision', 'Net Income',\n       'Net Income Including Non-Controlling Interests',\n       'Net Income Continuous Operations',\n       'Diluted NI Available to Com Stockholders', 'Total Expenses',\n       'Net Income from Continuing & Discontinued Operation', 'EBIT', 'EBITDA',\n       'Financing Cash Flow', 'Free Cash Flow', 'Investing Cash Flow',\n       'Sector', 'Employee', 'Salary', 'Quarterly Revenue Growth (yoy)',\n       'Diluted EPS (ttm)', 'Payout Ratio', 'Beta'],\n      dtype='object', name='Index')\n\n\nI apply the selection to the target variable:\n\ntarget = target.loc[df.index,:]\n\nI want to fill the missing values by the median of the variable grouped by Sector. Therefore, I need to clean this variable first.\nI check the different values from the Sector variable:\n\ndf['Sector'].value_counts()\n\nIndustrials               360\nFicial Services           348\nConsumer Cyclical         282\nTechnology                281\nHealthcare                250\nBasic Materials           156\nConsumer Defensive        155\nReal Estate               152\nCommunication Services    134\nEnergy                    121\nUtilities                  98\n1.0                         9\nName: Sector, dtype: int64\n\n\nI can first replace ‘Ficial Services’ by ‘Financial Services’.\n\ndf.loc[df['Sector'] =='Ficial Services','Sector'] = 'Financial Services'\n\nI replace the incorrect sectors by a missing value.\n\nlist_sector = ['Financial Services', 'Industrials','Technology', 'Consumer Cyclical', 'Healthcare','Basic Materials','Consumer Defensive', 'Real Estate', 'Communication Services' ,'Energy','Utilities']\n\n\ndf.loc[~df['Sector'].isin(list_sector),'Sector'] = np.nan\n\nHere is the list of the companies where the sector is unknown.\n\ndf[df['Sector'].isnull()]['Sector']\n\nJGSMY     NaN\nBSX       NaN\nCEO       NaN\nGD        NaN\nMGDDY     NaN\nEBAYL     NaN\nKKR-PA    NaN\nCPPCY     NaN\nCHA       NaN\nSGTPY     NaN\nCHU       NaN\nACGL      NaN\nNTXFY     NaN\nCRL       NaN\nSAM       NaN\nVNO-PK    NaN\nASH       NaN\nPAEKY     NaN\nVGGOF     NaN\nNINOY     NaN\nWTM       NaN\nSBFFF     NaN\nCCOHF     NaN\nMNHFY     NaN\nWELPP     NaN\nFEEXF     NaN\nBAFYY     NaN\nHOVNP     NaN\nFUN       NaN\nSKYW      NaN\nMTTRY     NaN\nNPSHY     NaN\nName: Sector, dtype: object\n\n\nSince I use the sector to fill the missing values, it is important to find the correct sector for each company. So I searched the sector company by company. And I replace the missing value by the company’s sector.\n\n# Fill the sector variable manually \ndf.loc['ACGL','Sector'] =  'Financial Services'\ndf.loc['ASH','Sector'] = 'Basic Materials'\ndf.loc['BAFYY','Sector'] = 'Industrials'\ndf.loc['BSX','Sector'] = 'Healthcare'\ndf.loc['CCOHF','Sector'] = 'Utilities'\ndf.loc['CEO','Sector'] = 'Energy'\ndf.loc['CHA','Sector'] = 'Communication Services'\ndf.loc['CHU','Sector'] = 'Communication Services'\ndf.loc['CPPCY','Sector'] = 'Consumer Cyclical'\ndf.loc['CRL','Sector'] = 'Healthcare'\ndf.loc['EBAYL','Sector'] = 'Consumer Cyclical'\ndf.loc['FEEXF','Sector'] = 'Energy'\ndf.loc['FUN','Sector'] = 'Consumer Defensive'\ndf.loc['GD','Sector'] = 'Industrials'\ndf.loc['HOVNP','Sector'] = 'Real Estate'\ndf.loc['JGSMY','Sector'] = 'Industrials'\ndf.loc['KKR-PA','Sector'] = 'Financial Services'\ndf.loc['MGDDY','Sector'] = 'Industrials'\ndf.loc['MNHFY','Sector'] = 'Industrials'\ndf.loc['MTTRY','Sector'] =  'Consumer Defensive'\ndf.loc['NINOY','Sector'] = 'Technology'\ndf.loc['NPSHY','Sector'] = 'Basic Materials'\ndf.loc['NTXFY','Sector'] = 'Financial Services'\ndf.loc['PAEKY','Sector'] = 'Energy'\ndf.loc['SAM','Sector'] =  'Consumer Defensive'\ndf.loc['SBFFF','Sector'] = 'Energy'\ndf.loc['SGTPY','Sector'] = 'Energy'\ndf.loc['SKYW','Sector'] = 'Technology'\ndf.loc['VGGOF','Sector'] = 'Technology'\ndf.loc['VNO-PK','Sector'] = 'Real Estate'\ndf.loc['WELPP','Sector'] = 'Energy'\ndf.loc['WTM','Sector'] = 'Financial Services'\n\n\ndf[df['Sector'].isnull()]['Sector']\n\nSeries([], Name: Sector, dtype: object)\n\n\nNow I can fill the missing values.\nLet’s first check how many missing values I have per column:\n\n# Check number of missing values\ndf.isnull().sum(axis=0)\n\nIndex\nTotal Assets                                             0\nCash And Cash Equivalents                                7\nNet PPE                                                 87\nGoodwill And Other Intangible Assets                   223\nTotal Liabilities Net Minority Interest                  0\nTotal Equity Gross Minority Interest                     0\nStockholders' Equity                                     0\nCapital Stock                                           41\nCommon Stock                                            41\nRetained Earnings                                      165\nTotal Capitalization                                     2\nNet Tangible Assets                                      0\nInvested Capital                                         2\nTangible Book Value                                      0\nTotal Debt                                              37\nShare Issued                                             8\nOrdinary Shares Number                                   8\nTotal Revenue                                            2\nOperating Revenue                                      311\nOperating Expense                                      323\nSelling General and Administrative                     307\nOperating Income                                       309\nPretax Income                                            0\nTax Provision                                           73\nNet Income                                               0\nNet Income Including Non-Controlling Interests           0\nNet Income Continuous Operations                         0\nDiluted NI Available to Com Stockholders                61\nTotal Expenses                                         257\nNet Income from Continuing & Discontinued Operation      0\nEBIT                                                   227\nEBITDA                                                 309\nFinancing Cash Flow                                    333\nFree Cash Flow                                         332\nInvesting Cash Flow                                    336\nSector                                                   0\nEmployee                                               202\nSalary                                                 364\nQuarterly Revenue Growth (yoy)                          63\nDiluted EPS (ttm)                                       51\nPayout Ratio                                           335\nBeta                                                    44\ndtype: int64\n\n\nWe can also see it as a ratio:\n\n# in %\ndf.isnull().sum(axis=0)/df.shape[0]\n\nIndex\nTotal Assets                                           0.000000\nCash And Cash Equivalents                              0.002955\nNet PPE                                                0.036724\nGoodwill And Other Intangible Assets                   0.094133\nTotal Liabilities Net Minority Interest                0.000000\nTotal Equity Gross Minority Interest                   0.000000\nStockholders' Equity                                   0.000000\nCapital Stock                                          0.017307\nCommon Stock                                           0.017307\nRetained Earnings                                      0.069650\nTotal Capitalization                                   0.000844\nNet Tangible Assets                                    0.000000\nInvested Capital                                       0.000844\nTangible Book Value                                    0.000000\nTotal Debt                                             0.015618\nShare Issued                                           0.003377\nOrdinary Shares Number                                 0.003377\nTotal Revenue                                          0.000844\nOperating Revenue                                      0.131279\nOperating Expense                                      0.136344\nSelling General and Administrative                     0.129591\nOperating Income                                       0.130435\nPretax Income                                          0.000000\nTax Provision                                          0.030815\nNet Income                                             0.000000\nNet Income Including Non-Controlling Interests         0.000000\nNet Income Continuous Operations                       0.000000\nDiluted NI Available to Com Stockholders               0.025749\nTotal Expenses                                         0.108485\nNet Income from Continuing & Discontinued Operation    0.000000\nEBIT                                                   0.095821\nEBITDA                                                 0.130435\nFinancing Cash Flow                                    0.140566\nFree Cash Flow                                         0.140144\nInvesting Cash Flow                                    0.141832\nSector                                                 0.000000\nEmployee                                               0.085268\nSalary                                                 0.153651\nQuarterly Revenue Growth (yoy)                         0.026593\nDiluted EPS (ttm)                                      0.021528\nPayout Ratio                                           0.141410\nBeta                                                   0.018573\ndtype: float64\n\n\nWe can compute the average:\n\n# Missing values mean\nnp.mean(df.isnull().sum(axis=0)/df.shape[0])\n\n0.04884520291865163\n\n\nSo I have about 4.88% of missing values per column in average.\nThen, I replace the missing values by the median of the variable grouped by sector.\n\n# Transform the dataframe to numeric values in order to apply the transformation\ncategorical = pd.DataFrame()\ncategorical['Sector'] = df['Sector']\ndf.drop('Sector',axis=1,inplace=True)\ndf = df.apply(pd.to_numeric)\ndf['Sector']= categorical['Sector']\n\n\n# Fill NA by sector median\ndf = df.fillna(df.groupby('Sector').transform('median'))\n\n\ndf.isnull().sum().sum()\n\n0\n\n\n\ndf.head(10)\n\n\n\n\n\n\n\nIndex\nTotal Assets\nCash And Cash Equivalents\nNet PPE\nGoodwill And Other Intangible Assets\nTotal Liabilities Net Minority Interest\nTotal Equity Gross Minority Interest\nStockholders' Equity\nCapital Stock\nCommon Stock\nRetained Earnings\n...\nFinancing Cash Flow\nFree Cash Flow\nInvesting Cash Flow\nEmployee\nSalary\nQuarterly Revenue Growth (yoy)\nDiluted EPS (ttm)\nPayout Ratio\nBeta\nSector\n\n\n\n\nAAPL\n3.238880e+08\n38016000.0\n3.676600e+07\n1115000.0\n258549000.0\n6.533900e+07\n6.533900e+07\n50779000.0\n50779000.0\n1.496600e+07\n...\n-93662000.0\n80219000.0\n795000.0\n147000.0\n14770000.0\n0.214\n3.69\n0.2177\n1.22\nTechnology\n\n\nMSFT\n3.013110e+08\n13576000.0\n5.290400e+07\n50389000.0\n183007000.0\n1.183040e+08\n1.183040e+08\n80552000.0\n80552000.0\n3.456600e+07\n...\n-50830000.0\n50436000.0\n-11451000.0\n163000.0\n13600000.0\n0.167\n6.71\n0.3115\n0.79\nTechnology\n\n\nAMZN\n3.211950e+08\n42122000.0\n1.506670e+08\n19998000.0\n227791000.0\n9.340400e+07\n9.340400e+07\n5000.0\n5000.0\n5.255100e+07\n...\n-1104000.0\n25924000.0\n-59611000.0\n1298000.0\n1680000.0\n0.436\n41.83\n0.0000\n1.12\nConsumer Cyclical\n\n\nGOOGL\n3.196160e+08\n26465000.0\n9.696000e+07\n22620000.0\n97072000.0\n2.225440e+08\n2.225440e+08\n58510000.0\n58510000.0\n1.634010e+08\n...\n-24408000.0\n42843000.0\n-32773000.0\n135301.0\n4010000.0\n0.235\n58.61\n0.0000\n1.00\nCommunication Services\n\n\nFB\n1.593160e+08\n17576000.0\n5.498100e+07\n19673000.0\n31026000.0\n1.282900e+08\n1.282900e+08\n0.0\n0.0\n7.734500e+07\n...\n-10292000.0\n23632000.0\n-30059000.0\n58604.0\n23420000.0\n0.332\n10.09\n0.0000\n1.29\nCommunication Services\n\n\nTCTZF\n1.333425e+09\n152798000.0\n7.771100e+07\n175528000.0\n555382000.0\n7.780430e+08\n7.039840e+08\n0.0\n0.0\n5.384640e+08\n...\n13647000.0\n127520000.0\n-181955000.0\n85858.0\n7100000.0\n0.264\n2.52\n0.0000\n0.50\nCommunication Services\n\n\nTSLA\n5.214800e+07\n19384000.0\n2.337500e+07\n520000.0\n29073000.0\n2.307500e+07\n2.222500e+07\n1000.0\n1000.0\n-5.399000e+06\n...\n9973000.0\n2701000.0\n-3132000.0\n70757.0\n23760.0\n0.455\n0.64\n0.0000\n2.01\nConsumer Cyclical\n\n\nBABA\n1.312985e+09\n330503000.0\n1.380470e+08\n337729000.0\n442437000.0\n8.705480e+08\n7.554010e+08\n1000.0\n1000.0\n4.062870e+08\n...\n-34485.0\n408000.0\n-166943.5\n252084.0\n2140000.0\n0.369\n8.82\n0.0000\n0.82\nConsumer Cyclical\n\n\nBRK-A\n8.737290e+08\n47990000.0\n1.870170e+08\n104418000.0\n422393000.0\n4.513360e+08\n4.431640e+08\n8000.0\n8000.0\n4.446260e+08\n...\n-18344000.0\n26761000.0\n-37757000.0\n360000.0\n380330.0\n-0.015\n26667.81\n0.0000\n0.90\nFinancial Services\n\n\nTSM\n2.760600e+09\n660170600.0\n1.583318e+09\n25768100.0\n924836700.0\n1.835764e+09\n1.834811e+09\n259303800.0\n259303800.0\n1.573840e+09\n...\n-88615087.0\n305885103.0\n-505781714.0\n7000.0\n10270000.0\n0.140\n3.49\n0.5010\n1.00\nTechnology\n\n\n\n\n10 rows × 42 columns"
  },
  {
    "objectID": "projet_2.html#d-features-engineering",
    "href": "projet_2.html#d-features-engineering",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "d) Features engineering:",
    "text": "d) Features engineering:\n\nFeatures variables:\n\n\n# Import packages \n# from scipy.stats import norm\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\nI drop the numerical variable:\n\ndf.drop('Sector',axis=1,inplace=True)\n\n\ndf.shape\n\n(2369, 41)\n\n\nI have 2369 remaining datapoints and 41 features.\nI plot the distribution of the numerical variables:\n\n# Plot\nfig, axes = plt.subplots(nrows=14,ncols=3,figsize=(20,80))\nfor i,axis in zip(np.arange(df.shape[1]),axes.ravel()):\n    sns.distplot(df.iloc[:,i],fit=norm,ax=axis)\n\n\n\n\n\n\n\n\nAlmost all my variables are skewed (except the Beta variable that is very close to a normal distribution). Thefore, I need to apply a log-transformation. Since I have columns containing null values or negative values, I will need three types of log-transformation: - classic log-transformation for positive features - log(1+x) transformation for positive/null features - log(1+x+min(abs(x))) for positive/null/negative features\nI drop the Beta feature since it does not need any transformation.\n\nbeta = df['Beta']\ndf.drop('Beta',axis=1,inplace=True)\n\nThen I first need to split my data between: - only positive datapoints\n\ndf_positive = df[df.columns[(df &gt; 0).all()]]\nlog_df_positive = np.log(df_positive)\n\n\npositive and null datapoints\n\n\nlist_difference = [item for item in df.columns[(df &gt;= 0).all()] if item not in df.columns[(df &gt; 0).all()]]\ndf_null = df[list_difference]\nlog_df_null = np.log1p(df_null)\n\n\nall other data (negative/null/positive)\n\n\nlist_difference = [item for item in df.columns if item not in df.columns[(df &gt;= 0).all()]]\ndf_negative = df[list_difference]\nlog_df_negative = df_negative.copy()\nfor i in range(0,df_negative.shape[1]):\n    log_df_negative.iloc[:,i] = np.log1p(df_negative.iloc[:,i] + abs(df_negative.iloc[:,i].min()))\n\nThen I can plot the graphs of the log variables:\n\nlog_df = pd.concat([log_df_positive, log_df_null, log_df_negative],axis=1)\n\n\n# Plot \nfig, axes = plt.subplots(nrows=14,ncols=3,figsize=(20,80))\nfor i,axis in zip(np.arange(log_df.shape[1]),axes.ravel()):\n    sns.distplot(log_df.iloc[:,i],fit=norm,ax=axis)\n\n\n\n\n\n\n\n\nIt already looks much better: the data are kind of symmetric and are much closer to a normal distribution.\nI add back the beta variable:\n\ndf['Beta'] = beta\nlog_df['Beta'] = beta\n\nHere a small sample of the log data:\n\nlog_df.head()\n\n\n\n\n\n\n\nIndex\nTotal Assets\nCash And Cash Equivalents\nNet PPE\nTotal Liabilities Net Minority Interest\nShare Issued\nOrdinary Shares Number\nSelling General and Administrative\nEmployee\nSalary\nCapital Stock\n...\nTotal Expenses\nNet Income from Continuing & Discontinued Operation\nEBIT\nEBITDA\nFinancing Cash Flow\nFree Cash Flow\nInvesting Cash Flow\nQuarterly Revenue Growth (yoy)\nDiluted EPS (ttm)\nBeta\n\n\n\n\nAAPL\n19.595908\n17.453518\n17.420084\n19.370596\n16.647356\n16.647356\n16.828591\n11.898188\n16.508109\n17.742993\n...\n19.209878\n20.856170\n20.786357\n20.127432\n23.519003\n23.227046\n24.351264\n0.792087\n4.226542\n1.22\n\n\nMSFT\n19.523654\n16.423814\n17.783990\n19.025035\n15.839836\n15.839836\n17.022030\n12.001505\n16.425580\n18.204414\n...\n18.352506\n20.845059\n20.772848\n20.103245\n23.521616\n23.224608\n24.350938\n0.770571\n4.269697\n0.79\n\n\nAMZN\n19.587559\n17.556081\n18.830583\n19.243939\n13.174956\n13.128345\n17.171571\n14.076335\n14.334304\n8.517393\n...\n19.711141\n20.818159\n20.736647\n20.057335\n23.524640\n23.222596\n24.349658\n0.887891\n4.669271\n1.12\n\n\nGOOGL\n19.582631\n17.091334\n18.389809\n18.390964\n13.422797\n13.422797\n17.182737\n11.815257\n15.204302\n17.884708\n...\n18.768402\n20.835236\n20.758501\n20.078210\n23.523224\n23.223985\n24.350372\n0.801553\n4.815431\n1.00\n\n\nFB\n18.886400\n16.682045\n17.822498\n17.250336\n14.862479\n14.862479\n16.714457\n10.978558\n16.969101\n0.000000\n...\n17.796588\n20.825241\n20.743379\n20.034613\n23.524082\n23.222408\n24.350444\n0.844150\n4.315887\n1.29\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nTarget variable:\n\nLet’s have a look on the target variable main descriptive statistics:\n\n# Descriptive statistics\ntarget['MC'].describe()\n\ncount    2.369000e+03\nmean     2.939738e+07\nstd      9.246432e+07\nmin      1.515630e+06\n25%      3.590120e+06\n50%      8.958590e+06\n75%      2.465150e+07\nmax      2.113622e+09\nName: MC, dtype: float64\n\n\nThe market capitalisation goes from 1.515630e+06 to 2.113622e+09 thousands of US dollars. It has a mean greater than the median. Therefore, the market capitalisation distribution should be right skewed.\n\n# Import packages\n# import math\n# import matplotlib.pyplot as plt\n# from scipy import stats\n# from matplotlib.pyplot import figure\n\nWe can also plot the distribution:\n\n# Plot target\nfigure(figsize=(10, 6), dpi=80)\nplt.hist(target['MC'], bins=100, density=True)\nmu = np.mean(target['MC'])\nvariance = np.var(target['MC'])\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.xlabel('Market capitalisation')\nplt.ylabel('Density')\nplt.title('Market cap distribution')\nplt.show()\nprint('Skewness=', stats.skew(target['MC']))\n\n\n\n\n\n\n\n\nSkewness= 13.611971974367075\n\n\nWe can see that the distribution is not symmetric and is exclusively positive. Moreover, the distribution has a very important skewness. Therefore, I apply the log transform in order to get a distribution closer to normal.\n\n# Plot log target\nfigure(figsize=(10, 6), dpi=80)\nplt.hist(np.log(target['MC']), bins=100,density=True)\nmu = np.mean(np.log(target['MC']))\nvariance = np.var(np.log(target['MC']))\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.xlabel('Market capitalisation (log)')\nplt.ylabel('Density')\nplt.title('Market cap distribution (in log)')\nplt.show()\nprint('Skewness=', stats.skew(np.log(target['MC'])))\n\n\n\n\n\n\n\n\nSkewness= 0.6519188373695136\n\n\nThe results are already much better and the skewness coeficient is closer to 0.\n\nlog_target = np.log(target['MC'])\n\n\nCreation of binary and categorical variables:\n\nWe can also create some binary variables and check the potential differences.\n\nAssets VS liabilities: the goal is to see if the company has more assets than liabilities.\n\n\nlog_df['Assets VS Liabilities'] = 0\n\n\nlog_df.loc[(log_df['Total Assets'] &lt; log_df['Total Liabilities Net Minority Interest']),'Assets VS Liabilities'] = 1\n\n\nCash flow: the goal is to see if the company has positive cash flows.\n\n\nlog_df['Positive Free Cash Flow'] = 0\n\n\nlog_df.loc[df['Free Cash Flow'] &gt; 0,'Positive Free Cash Flow'] = 1\n\n\nEPS: the goal is to see if the company has a positive Earnings per share.\n\n\nlog_df['Positive EPS'] = 0\n\n\nlog_df.loc[df['Diluted EPS (ttm)'] &gt; 0,'Positive EPS'] = 1 \n\n\nBeta: the goal is to see if the company has a Beta bigger than 1.\n\n\nlog_df['Beta &gt; 1'] = 0\n\n\nlog_df.loc[df['Beta'] &gt; 1,'Beta &gt; 1'] = 1 \n\n\nRevenue growth: the goal is to see if the company has a positive revenue growth.\n\n\nlog_df['Positive Revenue Growth'] = 0\n\n\nlog_df.loc[df['Quarterly Revenue Growth (yoy)'] &gt;0,'Positive Revenue Growth'] = 1 \n\n\nSize: During the first proposal, we came to the point that I could split the data into 3 categories:\n\nlarge cap\nmid cap\n‘small’ cap\n\n\nI first create a column ‘Size’ that will take the three following values: ‘large’, ‘mid’ and ‘small’.\n\ncategorical['Size'] = 0\n\n\n# Create categorical variables\nfor i in range(0,df.shape[0]):\n    if target.iloc[i,0] &gt; 17000000:\n        categorical.iloc[i,1] = 'large'\n    elif 5300000 &lt; target.iloc[i,0] &lt; 17000000 :\n        categorical.iloc[i,1] = 'mid'\n    elif target.iloc[i,0] &lt; 5300000:\n        categorical.iloc[i,1] = 'small'\n\nWe can check how many values I have in each category:\n\ndata = {'index':['large','mid','small'],'Number':[(categorical.iloc[:,-1] =='large').sum(),(categorical.iloc[:,-1] =='mid').sum(),(categorical.iloc[:,-1] =='small').sum()],'%':[round(100*(categorical.iloc[:,-1] =='large').sum()/categorical.shape[0],2),round(100*(categorical.iloc[:,-1] =='mid').sum()/categorical.shape[0],2),round(100*(categorical.iloc[:,-1] =='small').sum()/categorical.shape[0],2)]}\n\n\n# Compute proportion in each class\nsize_table= pd.DataFrame(data).set_index('index')\nsize_table.index.rename('',inplace=True)\nsize_table\n\n\n\n\n\n\n\n\nNumber\n%\n\n\n\n\n\n\n\n\n\nlarge\n782\n33.01\n\n\nmid\n786\n33.18\n\n\nsmall\n801\n33.81\n\n\n\n\n\n\n\nI have approximately the same numbers in each category."
  },
  {
    "objectID": "projet_2.html#e-features-encoding",
    "href": "projet_2.html#e-features-encoding",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "e) Features encoding",
    "text": "e) Features encoding\nI need to use one hot encoding for the string variable ‘Sector’.\n\n# Create binary variables\nSector_onehot = pd.get_dummies(categorical['Sector'], columns=['Sector'], drop_first=True)\n\n\nSector_onehot\n\n\n\n\n\n\n\n\nCommunication Services\nConsumer Cyclical\nConsumer Defensive\nEnergy\nFinancial Services\nHealthcare\nIndustrials\nReal Estate\nTechnology\nUtilities\n\n\n\n\nAAPL\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nMSFT\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nAMZN\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nGOOGL\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFB\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTCTZF\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTSLA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nBABA\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nBRK-A\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nTSM\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nV\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nJPM\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nJNJ\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nWMT\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nMA\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nUNH\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nNVDA\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nBAC\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nDIS\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nHD\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nLVMHF\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nLVMUY\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nPG\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nNSRGY\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nNSRGF\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nPYPL\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nNXCLF\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nRHHBY\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nRHHBF\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nRHHVF\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVUZI\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nFARO\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nCDXS\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nRKNEF\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nQURE\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nAHL-PC\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nSGHIY\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nHTLD\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nRILY\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nGES\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSMIZF\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSUPN\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGBX\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nMEGEF\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nCBD\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nGFF\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nEFSC\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nWDR\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nSKT\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nWMK\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nSWM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nWARFY\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nNSHBY\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nSTKL\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nFGROF\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nHRTX\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nMSC\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nCODI\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nCASH\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nLMNX\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n2369 rows × 10 columns\n\n\n\n\nlog_df = pd.concat([log_df, Sector_onehot,categorical],axis=1)"
  },
  {
    "objectID": "projet_2.html#f-correlation-matrix",
    "href": "projet_2.html#f-correlation-matrix",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "f) Correlation matrix",
    "text": "f) Correlation matrix\nWe plot the correlation matrix of the features variables:\n\n# plot \nfigure(figsize=(20, 20))\ncorrMatrix = df.corr()\nsns.heatmap(corrMatrix,cmap=\"RdBu\",center=0)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, many variables are strongly positively correlated. For example, the four ‘Income’ variable have a correlation close to 1. I drop the variables that seem too correlated:\n\n# Drop too correlated variables\ndf.drop(['Capital Stock','Ordinary Shares Number','Net Income Including Non-Controlling Interests',\"Stockholders' Equity\",'Diluted NI Available to Com Stockholders',\n       'Net Income Continuous Operations','Net Income from Continuing & Discontinued Operation'],axis=1,inplace=True)\n\nI can plot the same correlation matrix:\n\n# Plot \nfigure(figsize=(20, 20))\ncorrMatrix = df.corr()\nsns.heatmap(corrMatrix,cmap=\"RdBu\",center=0)\nplt.show()"
  },
  {
    "objectID": "projet_2.html#g-data-split",
    "href": "projet_2.html#g-data-split",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "g) Data split",
    "text": "g) Data split\nBefore I fill the missing values and check for potential outliers, I split the dataset since I do not want to remove outliers from the Test Set. I use the train_test_split from sklearn to split my data set into a train, a validation and a test set. The training set will have 80% of the companies, the validation set 10% and the test set also 10%.\n\nlog_df = log_df.iloc[:,:-2]\n\n\n# Split data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_te, y_train, y_te = train_test_split(log_df, log_target, test_size=0.2, random_state=0)\nX_valid, X_test, y_valid, y_test = train_test_split(X_te, y_te, test_size=0.5, random_state=0)\nprint('Train:', X_train.shape, y_train.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)\nprint('Test:', X_test.shape, y_test.shape)\n\nTrain: (1895, 56) (1895,)\nValidation: (237, 56) (237,)\nTest: (237, 56) (237,)\n\n\n\n# Store in dataframe\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test)\ny_valid = pd.DataFrame(y_valid)"
  },
  {
    "objectID": "projet_2.html#h-outliers",
    "href": "projet_2.html#h-outliers",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "h) Outliers",
    "text": "h) Outliers\n\n# Concat dataframes for the outliers\ntrainset = pd.DataFrame()\ntrainset = pd.concat([y_train, X_train],axis=1,sort='False')\n\nFor outliers I can first plot boxplots.\n\n# Boxplots\nfig, axes = plt.subplots(nrows=11,ncols=4,figsize=(20,55))\nfor col,ax in zip(trainset.columns[:-15],axes.ravel()):\n    ax.boxplot(trainset[col])\n    ax.set_title(col)\n\n\n\n\n\n\n\n\nFrom the boxplots, we can see that we have a few very extreme values. These datapoints are correct but represent very rare cases. For the variables that are close to a normal disitrbution, I decide to remove them using z-scores. For the others, I use a specific threshold.\n\nVariables close to normal distribution:\n\nI first create a subset of variables close to normal.\n\n# Variables close to normal\nnormal_var = ['Total Assets', \n              'Cash And Cash Equivalents', \n              'Net PPE',\n              'Total Liabilities Net Minority Interest', \n              'Share Issued',\n              'Ordinary Shares Number', \n              'Selling General and Administrative',\n              'Employee', \n              'Salary', \n              'Capital Stock', \n              'Common Stock', \n              'Total Debt',\n              'Goodwill And Other Intangible Assets',\n              'Operating Revenue',\n              'Beta']\n\nI compute the z score:\n\nzscores = (trainset[normal_var] - trainset[normal_var].mean()) / (trainset[normal_var].std())\n\nI filter when z scores are bigger than 4.\n\nzfilter = np.abs(zscores) &gt; 4\n\n\nzfilter_any  = zfilter.any(axis=1)\n\n\noutliers = trainset[normal_var].loc[zfilter_any, :]\noutliers.shape\n\n(44, 15)\n\n\nWe need to remove 44 datapoints.\n\n# Drop outliers\ntrainset = trainset.drop(outliers.index, axis=0)\n\nLet’s plot again the boxplots.\n\n# Boxplots \nfig, axes = plt.subplots(nrows=4,ncols=4,figsize=(20,20))\nfor col,ax in zip(normal_var,axes.ravel()):\n    ax.boxplot(trainset[col])\n    ax.set_title(col)\n\n\n\n\n\n\n\n\nIt looks much better now. I do not have outliers anymore.\n\nOther variables:\n\nI select all the other variables (that are not close to a normal distribution and that need outliers removal):\n\n# Variables not close to normal distribution\nother_var = ['Total Equity Gross Minority Interest',\"Stockholders' Equity\",\n       'Retained Earnings', 'Total Capitalization', \n       'Invested Capital', 'Total Revenue', 'Operating Expense', \n       'Pretax Income', 'Tax Provision', 'Net Income',\n       'Net Income Including Non-Controlling Interests',\n       'Net Income Continuous Operations',\n       'Diluted NI Available to Com Stockholders','Total Expenses',\n       'Net Income from Continuing & Discontinued Operation', 'EBIT', 'EBITDA',\n       'Free Cash Flow', \n       'Diluted EPS (ttm)']\n\n\n# Boxplots\nfig, axes = plt.subplots(nrows=5,ncols=4,figsize=(20,25))\nfor col,ax in zip(other_var,axes.ravel()):\n    ax.boxplot(trainset[col])\n    ax.set_title(col)\n\n\n\n\n\n\n\n\nWe can see from above that for many columns, there are often only one datapoint close to 0. I remove them.\nI delete all the datapoints that are below 1. I choose this arbitrary threshold because we can see on the plots above that it is a good threshold to remove the extreme low values.\n\nofilter = trainset[other_var] &lt;1\n\n\nofilter_any = ofilter.any(axis=1)\n\n\noutliers = trainset[other_var].loc[ofilter_any, :]\noutliers.shape\n\n(11, 19)\n\n\nThen, I need to remove 11 datapoints.\n\n# Drop outliers\ntrainset = trainset.drop(outliers.index, axis=0)\n\nLet’s see the final boxplots.\n\n# Boxplots without outliers\nfig, axes = plt.subplots(nrows=11,ncols=4,figsize=(20,55))\nfor col,ax in zip(trainset.columns[:-15],axes.ravel()):\n    ax.boxplot(trainset[col])\n    ax.set_title(col)\n\n\n\n\n\n\n\n\nIn my opinion, there are no more very extreme values that should be deleted. I will use the huber loss function in order to take into account the values that are still a bit far from the median.\n\ny_train = pd.DataFrame(trainset.loc[:,'MC'])\n\n\nX_train = pd.DataFrame(trainset.iloc[:,1:])"
  },
  {
    "objectID": "projet_2.html#i-check-for-inconsistencies",
    "href": "projet_2.html#i-check-for-inconsistencies",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "i) Check for inconsistencies",
    "text": "i) Check for inconsistencies\nWe can check that we have: Asset = Equity + Liability.\n\n# Compute most important accounting equation for all datapoints\ndf[df['Total Equity Gross Minority Interest']&gt;0]['Total Assets'] - df[df['Total Equity Gross Minority Interest']&gt;0]['Total Liabilities Net Minority Interest']- df[df['Total Equity Gross Minority Interest']&gt;0]['Total Equity Gross Minority Interest']\n\nAAPL      0.0\nMSFT      0.0\nAMZN      0.0\nGOOGL     0.0\nFB        0.0\nTCTZF     0.0\nTSLA      0.0\nBABA      0.0\nBRK-A     0.0\nTSM       0.0\nV         0.0\nJPM       0.0\nJNJ       0.0\nWMT       0.0\nMA        0.0\nUNH       0.0\nNVDA      0.0\nBAC       0.0\nDIS       0.0\nHD        0.0\nLVMHF     0.0\nLVMUY     0.0\nPG        0.0\nNSRGY     0.0\nNSRGF     0.0\nPYPL      0.0\nNXCLF     0.0\nRHHBY     0.0\nRHHBF     0.0\nRHHVF     0.0\n         ... \nSSYS      0.0\nVUZI      0.0\nFARO      0.0\nCDXS      0.0\nQURE      0.0\nAHL-PC    0.0\nSGHIY     0.0\nHTLD      0.0\nRILY      0.0\nGES       0.0\nSMIZF     0.0\nSUPN      0.0\nGBX       0.0\nMEGEF     0.0\nCBD       0.0\nGFF       0.0\nEFSC      0.0\nWDR       0.0\nSKT       0.0\nWMK       0.0\nSWM       0.0\nWARFY     0.0\nNSHBY     0.0\nSTKL      0.0\nFGROF     0.0\nHRTX      0.0\nMSC       0.0\nCODI      0.0\nCASH      0.0\nLMNX      0.0\nLength: 2293, dtype: float64\n\n\nWe sum this column:\n\nround((df[df['Total Equity Gross Minority Interest']&gt;0]['Total Assets'] - df[df['Total Equity Gross Minority Interest']&gt;0]['Total Liabilities Net Minority Interest']- df[df['Total Equity Gross Minority Interest']&gt;0]['Total Equity Gross Minority Interest']).sum(),-1)\n\n-0.0\n\n\nIt is indeed close to 0.\nI can now store the train, validation and test set.\n\n# Store cleaned datasets\n#X_train.to_csv(r'X_train.csv')\n#y_train.to_csv(r'y_train.csv')\n#X_valid.to_csv(r'X_valid.csv')\n#y_valid.to_csv(r'y_valid.csv')\n#X_test.to_csv(r'X_test.csv')\n#y_test.to_csv(r'y_test.csv')\n#categorical.to_csv(r'categorical.csv')"
  },
  {
    "objectID": "projet_2.html#a-sector",
    "href": "projet_2.html#a-sector",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "a) Sector",
    "text": "a) Sector\nWe can have a look at the distribution of the different industries.\n\n# Count the number of companies by sector\ncategorical['Sector'].value_counts()\n\nIndustrials               365\nFinancial Services        352\nConsumer Cyclical         284\nTechnology                284\nHealthcare                252\nConsumer Defensive        158\nBasic Materials           158\nReal Estate               154\nCommunication Services    136\nEnergy                    127\nUtilities                  99\nName: Sector, dtype: int64\n\n\n\n# Import packages\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport numpy as np\nimport seaborn as sns\n\n\n# Creation of palettes to synchronize colors\npalette ={'Energy':'#cb334d', 'Utilities':'#e95c47', 'Basic Materials':'#f98e52','Industrials':'#fdbf6f','Consumer Cyclical':'#fee593','Consumer Defensive':'#ffffbe','Healthcare':'#eaf79e','Technology':'#bfe5a0', 'Communication Services':'#86cfa5','Financial Services':'#54aead','Real Estate':'#3a7eb8'}\npalette1 = {10:'#cb334d', 11:'#e95c47', 7:'#f98e52',1:'#fdbf6f',4:'#fee593',6:'#ffffbe',5:'#eaf79e',3:'#bfe5a0', 9:'#86cfa5',2:'#54aead',8:'#3a7eb8'}\n\n# Plot \nfigure(figsize=(20, 6), dpi=80)\nx_cor= np.array([1,2,3,4,5,6,7,8,9,10,11])\nax = sns.barplot(x_cor,categorical['Sector'].value_counts(),palette=palette1)\nax.set_xticklabels(categorical['Sector'].value_counts().index);\nax.set_title('Number of companies by sector')\nax.set(ylabel='');\n\n\n\n\n\n\n\n\nWe can see that the two most important sectors are Industrials and Financial services. After that we find the Technology, Healthcare, Consumers goods sectors.\nFinally we can check the difference between the industries:\n\n# Plot\nfigure(figsize=(20, 6), dpi=80)\nsns.boxplot(x=categorical['Sector'], y=target['MC'],palette=palette, order=['Industrials','Financial Services','Technology','Consumer Cyclical','Healthcare','Consumer Defensive','Basic Materials','Real Estate','Communication Services','Energy', 'Utilities' ]);\n\n\n\n\n\n\n\n\nEvery sectors have a market capitalisation median close to 8 billion of US dollars (exp(16)*1000). The communication sector has the largest median in terms of market capitalisation. Surprinsigly, the real estate sector has the lowest market capitalisation median.\nI also plot the same features but with the Sector as the hue.\n\n# Create a smallest dataframe\nsector2 = pd.concat([features[['Total Assets','Employee', 'Salary','Payout Ratio','Beta']],categorical['Sector']],axis=1,sort=False)\n\n\nsector2.head()\n\n\n\n\n\n\n\n\nTotal Assets\nEmployee\nSalary\nPayout Ratio\nBeta\nSector\n\n\n\n\nAA\n16.514184\n9.464983\n14.827111\n0.000000\n2.72\nBasic Materials\n\n\nAAL\n17.942774\n11.539567\n11.062097\n0.203186\n1.93\nIndustrials\n\n\nAAON\n13.014796\n7.726654\n13.232598\n0.227136\n0.57\nIndustrials\n\n\nAAP\n16.286963\n10.596635\n14.576316\n0.131116\n1.36\nConsumer Cyclical\n\n\nAAPL\n19.595908\n11.898188\n16.508109\n0.196964\n1.22\nTechnology\n\n\n\n\n\n\n\nSince I have a lot of different sectors, I tried to order a bit the sectors from ‘Basic goods’ to ‘Services’.\n\n# Plot\nsns.pairplot(data=sector2[['Total Assets','Employee', 'Salary','Payout Ratio','Beta','Sector']],hue='Sector',diag_kind=\"hist\",palette='Spectral',height=3.5,hue_order=['Energy', 'Utilities', 'Basic Materials','Industrials','Consumer Cyclical','Consumer Defensive','Healthcare','Technology', 'Communication Services','Financial Services','Real Estate']);\n\n\n\n\n\n\n\n\nWe can see that the Real Estate and Financial Services are more concentrated. They also have a small number of employees. On the other hand, the Energy and Utilities sectors are more spread out. However, I can not see specific clusters."
  },
  {
    "objectID": "projet_2.html#b-other-features",
    "href": "projet_2.html#b-other-features",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "b) Other features",
    "text": "b) Other features\nWe can see the most positively and negatively correlated variables:\n\ndf = pd.concat([features, target['MC']],axis=1,sort=False)\n\n\n# Positive correlation\ndf.corr()['MC'].sort_values(ascending=False).head(10)\n\nMC                                         1.000000\nEmployee                                   0.543026\nShare Issued                               0.491213\nGoodwill And Other Intangible Assets       0.484643\nOrdinary Shares Number                     0.480351\nSelling General and Administrative         0.455186\nTotal Assets                               0.445933\nTotal Liabilities Net Minority Interest    0.440479\nTotal Debt                                 0.436274\nTotal Revenue                              0.417677\nName: MC, dtype: float64\n\n\nThese variables are logically positively correlated with the market cap. I would have expected even higher correlations for the variables total assets and total revenue.\n\n# Negative correlation\ndf.corr()['MC'].sort_values(ascending=False).tail(10)\n\nQuarterly Revenue Growth (yoy)   -0.003006\nConsumer Cyclical                -0.003878\nUtilities                        -0.004873\nFinancing Cash Flow              -0.005512\nInvesting Cash Flow              -0.021394\nAssets VS Liabilities            -0.022224\nIndustrials                      -0.068172\nReal Estate                      -0.101355\nBeta &gt; 1                         -0.197493\nBeta                             -0.215937\nName: MC, dtype: float64\n\n\nIt is interesting to see that the correlations are much closer to 0 with this bigger sample (during the first proposal, I used a smaller sample - SP500 companies - and the correlation were different).\nWe can also see plot scatter plots to analysis the relation between the features and the target:\n\n# Plot\nfig,axes = plt.subplots(nrows=11,ncols=4,figsize=(20,60))\nfor col,ax in zip(df.columns[:-18],axes.ravel()):\n    ax.scatter(df[col],df['MC'], marker=\"+\",alpha=0.4)\n    ax.set_title(col+str(' Versus MC'))\n\n\n\n\n\n\n\n\n\n#fig, axs = plt.subplots(ncols=2,nrows=21,figsize=(20,200))\n#for i in range(0,21):\n#    sns.regplot(log_numerical_wo.columns.to_list()[i],'Market capitalisation',data=log_numerical_wo,ax=axs[i,0])\n#for i in range(0,21):\n#    sns.regplot(log_numerical_wo.columns.to_list()[i+21],'Market capitalisation',data=log_numerical_wo,ax=axs[i,1])\n\nWe basically have two types of plots: - a scatter plot that shows a positive and linear correlation (e.g. Total assets) - a scatter plot very concentrated in a very narrow range of values (e.g. Financing cash flow)\nWe can also see that many variables are still similar. Therefore, I will use PCA to create a second feature space. Then I will have these two different features space and see which one performs better."
  },
  {
    "objectID": "projet_2.html#c-classification-and-binary-variables",
    "href": "projet_2.html#c-classification-and-binary-variables",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "c) Classification and binary variables",
    "text": "c) Classification and binary variables\nWe can check the biggest and smallest companies in terms of market capitalisation:\nLargest companies\n\n# Import the sector and size variable from the categorical dataset\ndf['Sector'] = categorical['Sector']\ndf['Size'] = categorical['Size']\n\n\ndf.sort_values(by='MC',ascending=False)['Sector'][0:10]\n\nIndex\nAAPL                 Technology\nMSFT                 Technology\nAMZN          Consumer Cyclical\nGOOGL    Communication Services\nFB       Communication Services\nTCTZF    Communication Services\nTSLA          Consumer Cyclical\nBABA          Consumer Cyclical\nBRK-A        Financial Services\nTSM                  Technology\nName: Sector, dtype: object\n\n\nSo the biggest companies are Apple, Microsoft, Amazon, Google, Facebook, Tencent, Tesla, Alibaba, Berkshire Hathaway and Taiwan Semiconductor Manufacture. They are mainly from the Technology, Communication services and Consumer Cyclical sectors.\nSmallest companies\n\ndf.sort_values(by='MC',ascending=True)['Sector'][0:10]\n\nIndex\nLMNX             Healthcare\nCASH     Financial Services\nCODI            Industrials\nMSC       Consumer Cyclical\nHRTX             Healthcare\nFGROF           Industrials\nSTKL     Consumer Defensive\nNSHBY           Industrials\nWARFY           Real Estate\nSWM         Basic Materials\nName: Sector, dtype: object\n\n\nSo the smallest companies are Luminex Corporation, Meta Financial Group, D/B/A Compass Diversified Holdi, MSC Industrial Direct Company, Heron Therapeutics, FIRSTGROUP, SunOpta, NISSHINBO HOLDINGS INC, WHARF and Schweitzer-Mauduit International. They are from various sectors.\nWe can plot boxplot and categorize by Size:\n\n# Plot\nfig,axes = plt.subplots(nrows=11,ncols=4,figsize=(20,63))\nfor col,ax in zip(df.columns[:-18],axes.ravel()):\n    sns.boxplot(x=df['Size'], y=df[col], ax=ax ,order=['small','mid','large'])\n    ax.set_title(col+str(' VS Size'))\n\n\n\n\n\n\n\n\nAs expected, we can see differences for some features, in particular for the following features: - share issued - total revenue - total expenses - employee\nBut nothing very strong unfortunately.\nI can also plot different variables and put the size as the hue.\n\n# Create smaller dataframe\nsmall_df = df[['Total Assets', 'Employee', 'Salary', 'Payout Ratio','Beta','Size']]\n\n\n# Plot\nsns.pairplot(data=small_df,hue='Size',height=3.5,hue_order=['small','mid','large']);\n\n\n\n\n\n\n\n\nWe can see that large companies (in green) are often in the right part of the plots (for the part below the diagonal of the distributions). This is consistent with the positive correlation between the selected features variables.\n\nBinary variables\n\nNow we can plot boxplots again:\n\n# Plot\nfig,axes = plt.subplots(nrows=1,ncols=5,figsize=(20,5))\nfor col,ax in zip(df.columns[-18:-13],axes.ravel()):\n    sns.boxplot(x=df[col], y=df['MC'], ax=ax,palette='Set1' )\n    ax.set_title(col+str(' Versus Size'))\n\n\n\n\n\n\n\n\nWe can see small intuitive differences: - companies with more liabilities have a lower market cap - companies with positive cash flow have a larger market cap - companies with a positive earnings per share ratio have a larger cap - companies with a beta bigger than 1 (which means companies that are more volatile than the market) have a lower maket cap - companies with positive revenue growth have a slightly larger market cap But nothing seems to be very significant."
  },
  {
    "objectID": "projet_2.html#a-baseline",
    "href": "projet_2.html#a-baseline",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "a) Baseline",
    "text": "a) Baseline\nFirst, let’s create the baseline.\n\n# Function that computes the Mean Absolute Error\ndef MAE(y, y_pred):\n    return np.mean(np.abs(y - y_pred))\n\n\n# Compute the Mean Absolute Error of the baseline\nmae_baseline = MAE(np.exp(y_test['MC']), np.exp(np.median(y_train['MC'])))\nprint('MAE Baseline: {:.5f}'.format(mae_baseline))\n\nMAE Baseline: 22590390.26206\n\n\nAs expected, the Mean Absolute Error of the baseline is quite important.\nAs stated during the proposal, I want to create three feature spaces: - the original dataset - a subset of variable selected with SelectKBest - a PCA subset"
  },
  {
    "objectID": "projet_2.html#b-model-fitting-with-the-original-datset",
    "href": "projet_2.html#b-model-fitting-with-the-original-datset",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "b) Model fitting with the original datset",
    "text": "b) Model fitting with the original datset\n\n# Convert as float values to use algorithms \nX_train = X_train.astype(float)\nX_valid = X_valid.astype(float)\nX_test = X_test.astype(float)\ny_train = y_train.astype(float)\ny_valid = y_valid.astype(float)\ny_test = y_test.astype(float)\n\n\nK-nearest neighbors (KNN)\n\n\n# Import packages\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nn_neighbors=[]\nvalidation_score=[]\n\n# Fit the model and tune the parameters\nfor n in np.arange(1, 50, step=2):\n\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsRegressor(n_neighbors=n))\n    ])\n\n    # Fit estimator \n    knn_pipe.fit(X_train, y_train['MC'])\n    \n    # Compute validation score on the validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(knn_pipe.predict(X_valid))))\n    \n    n_neighbors.append(n)\n\n\n# Sort by validation score\nresults_knn = pd.DataFrame({'n_neighbors':n_neighbors,'validation MAE':validation_score})\nresults_knn.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_neighbors\nvalidation MAE\n\n\n\n\n0\n1\n2.175721e+07\n\n\n1\n3\n2.920062e+07\n\n\n3\n7\n2.988612e+07\n\n\n2\n5\n2.989151e+07\n\n\n4\n9\n3.023298e+07\n\n\n\n\n\n\n\nThe best result is obtained with 1 neighbor.\n\n# Pick the best number of neighbors \nknn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsRegressor(n_neighbors=1))\n    ])\n\n# Fit estimator \nknn_pipe.fit(X_train, y_train['MC'])\n\n# Compute the test score\ntest_score_KNN = (MAE(np.exp(y_test['MC']), np.exp(knn_pipe.predict(X_test))))\ntest_score_KNN\n\n16627359.503122352\n\n\nWith 1 neighbor, we obtained a test score of 16’627’359, which is fortunately a bit better than the baseline.\n\nDecision tree and random forest\n\nAs a graphical purpose, we can plot the beginning of the tree (depth of 3):\n\n# Import package\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create decision tree\ndt = DecisionTreeRegressor(max_depth=3, criterion = 'mae', random_state=0)\n\n\n# Fit the tree\ndt.fit(X_train, y_train['MC'])\n\nDecisionTreeRegressor(criterion='mae', max_depth=3, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=0, splitter='best')\n\n\n\n# Import package\nfrom sklearn.tree import export_graphviz\n\n# Export decision tree with good parameters\ndot_data = export_graphviz(\n    dt, out_file=None,\n    feature_names=X_train.columns,\n    filled=True, rounded=True, proportion=True,\n    max_depth=3)\n\n\n# Import package\nimport graphviz\n\n# Display decision tree\ngraphviz.Source(dot_data)\n\n\n\n\n\n\n\n\nWe can see that the path that lead to the biggest value is a large invested capital, a large number of employee and an important amount of FCF, which makes sense and remains coherent with the previous analyses.\nWe can now fit and tune our random forest model:\n\n# Import the package\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Fit the model and tune the parameters\nn_trees=[]\nvalidation_score=[]\n\nfor n in np.arange(1, 100, step=2):\n\n    # Create a random forest pipeline\n    rf_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('rf', RandomForestRegressor (n_estimators=n , max_depth=None, random_state=0))\n    ])\n\n    # Fit the estimator \n    rf_pipe.fit(X_train, y_train['MC'])\n    \n    # Save validation score on the validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(rf_pipe.predict(X_valid))))\n    \n    n_trees.append(n)\n\n\n# Sort by validation score\nresults_rf = pd.DataFrame({'n_trees':n_trees,'validation MAE':validation_score})\nresults_rf.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_trees\nvalidation MAE\n\n\n\n\n47\n95\n2.385272e+07\n\n\n42\n85\n2.386110e+07\n\n\n44\n89\n2.387669e+07\n\n\n45\n91\n2.388805e+07\n\n\n31\n63\n2.388990e+07\n\n\n\n\n\n\n\nThe best number of trees is close to 100.\n\n# Pick the best number of trees\nrf_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('rf', RandomForestRegressor (n_estimators=95 , max_depth=None, random_state=0))\n    ])\n\n# Fit estimator \nrf_pipe.fit(X_train, y_train['MC'])\n\n# Compute the test score\ntest_score_RF = (MAE(np.exp(y_test['MC']), np.exp(rf_pipe.predict(X_test))))\ntest_score_RF\n\n13813525.218626602\n\n\nWe obtain a test score of 13’813’525 with the random forest.\n\nRidge and lasso regression\n\nNow, let’s try with regularization.\nAs discussed during the proposal, I am going to try the penalty ‘elasticnet’ since it is mixed between L1 and L2 penalty. Moreover, ‘elasticnet’ allows features selection, which is not possible with L2 penalty.\n\n# Import the package\nfrom sklearn.linear_model import SGDRegressor\n\n# Fit the model and tune the parameter\nalpha =[]\nvalidation_score=[]\n\nfor i in np.arange(0.001, 0.05, step=0.001):\n\n    # Create a random forest pipeline\n    reg_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('SGDRegressor', SGDRegressor (alpha=i,max_iter=1000,penalty='elasticnet' ,tol=1e-3, loss='huber', random_state=0))\n    ])\n\n    # Fit estimator \n    reg_pipe.fit(X_train, y_train['MC'])\n    \n    # Compute validation score\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(reg_pipe.predict(X_valid))))\n    \n    alpha.append(i)\n\n\n# Sort by validation score\nresults_reg = pd.DataFrame({'alpha':alpha,'validation MAE':validation_score})\nresults_reg.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nalpha\nvalidation MAE\n\n\n\n\n0\n0.001\n3.174063e+07\n\n\n1\n0.002\n3.179318e+07\n\n\n2\n0.003\n3.216834e+07\n\n\n3\n0.004\n3.251821e+07\n\n\n4\n0.005\n3.265793e+07\n\n\n\n\n\n\n\n\n# Pick the best alpha \nreg_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('SGDRegressor', SGDRegressor (alpha=0.001,max_iter=1000,penalty='elasticnet' ,tol=1e-3, loss='huber', random_state=0))\n    ])\n\n# Fit estimator on the original data\nreg_pipe.fit(X_train, y_train['MC'])\n\n# Compute the test score\ntest_score_reg = (MAE(np.exp(y_test['MC']), np.exp(reg_pipe.predict(X_test))))\ntest_score_reg\n\n20221795.601256844\n\n\nWe obtain a score of 20’221’795 with the regularization."
  },
  {
    "objectID": "projet_2.html#c-model-fitting-with-a-subset---selectkbest",
    "href": "projet_2.html#c-model-fitting-with-a-subset---selectkbest",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "c) Model fitting with a subset - SelectKBest",
    "text": "c) Model fitting with a subset - SelectKBest\nI now want to apply the same models with a subset of features. To select this subset, I use the package SelectKBest of sklearn.\n\n# Import package\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\n# Create a selector for k=10\nselector = SelectKBest( f_regression, k=10)\n\n# Fit the selector to the trainset \nselector.fit(X_train, y_train['MC'])\n\nSelectKBest(k=10, score_func=&lt;function f_regression at 0x000002328EA5E158&gt;)\n\n\n\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\n\n\n# Create subset dataframes\nX_train_KB= X_train.iloc[:,cols]\nX_valid_KB= X_valid.iloc[:,cols]\nX_test_KB= X_test.iloc[:,cols]\n\n\nK-nearest neighbors (KNN)\n\n\n# Fit the model and tune parameter\nn_neighbors=[]\nvalidation_score=[]\n\nfor n in np.arange(1, 50, step=2):\n\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsRegressor(n_neighbors=n))\n    ])\n\n    # Fit estimator \n    knn_pipe.fit(X_train_KB, y_train['MC'])\n    \n    # Compute validation score on validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(knn_pipe.predict(X_valid_KB))))\n    \n    n_neighbors.append(n)\n\n\n# Sort by validation accuracy \nresults_knn = pd.DataFrame({'n_neighbors':n_neighbors,'validation MAE':validation_score})\nresults_knn.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_neighbors\nvalidation MAE\n\n\n\n\n1\n3\n2.821323e+07\n\n\n2\n5\n2.825228e+07\n\n\n3\n7\n2.851210e+07\n\n\n4\n9\n2.864460e+07\n\n\n0\n1\n2.869202e+07\n\n\n\n\n\n\n\n\n# Pick the best n \nknn_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsRegressor(n_neighbors=3))\n    ])\n\n# Fit estimator on the original data\nknn_pipe.fit(X_train_KB, y_train['MC'])\n\n# Compute the test score\ntest_score_KNN_KB = (MAE(np.exp(y_test['MC']), np.exp(knn_pipe.predict(X_test_KB))))\ntest_score_KNN_KB\n\n17582652.409168214\n\n\nWe obtain a test score of 17’582’652.\n\nDecision tree and random forest\n\n\n# Fit the model and tune the parameter\nn_trees=[]\nvalidation_score=[]\n\nfor n in np.arange(1, 100, step=2):\n\n    # Create a random forest pipeline\n    rf_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('rf', RandomForestRegressor (n_estimators=n , max_depth=None, random_state=0))\n    ])\n\n    # Fit estimator \n    rf_pipe.fit(X_train_KB, y_train['MC'])\n    \n    # Compute validation score on validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(rf_pipe.predict(X_valid_KB))))\n\n    n_trees.append(n)\n\n\n# Sort by validation score\nresults_rf = pd.DataFrame({'n_trees':n_trees,'validation MAE':validation_score})\nresults_rf.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_trees\nvalidation MAE\n\n\n\n\n46\n93\n2.738031e+07\n\n\n49\n99\n2.738949e+07\n\n\n48\n97\n2.740769e+07\n\n\n44\n89\n2.741849e+07\n\n\n31\n63\n2.741961e+07\n\n\n\n\n\n\n\n\n# Pick the best number of ntrees\nrf_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('rf', RandomForestRegressor (n_estimators=93 , max_depth=None, random_state=0))\n    ])\n\n# Fit estimator \nrf_pipe.fit(X_train_KB, y_train['MC'])\n\n# Compute the test score\ntest_score_RF_KB = (MAE(np.exp(y_test['MC']), np.exp(rf_pipe.predict(X_test_KB))))\ntest_score_RF_KB\n\n16938380.214340754\n\n\nWe obtain a test score of 16’938’380 with the random forest.\n\nRidge and lasso regression\n\n\n# Fit the model and tune parameters\nalpha =[]\nvalidation_score=[]\n\nfor i in np.arange(0.001, 0.05, step=0.001):\n\n    # Create a random forest pipeline\n    reg_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('SGDRegressor', SGDRegressor (alpha=i,max_iter=1000,penalty='elasticnet' ,tol=1e-3, loss='huber', random_state=0))\n    ])\n\n    # Fit estimator\n    reg_pipe.fit(X_train_KB, y_train['MC'])\n    \n    # Compute validation score on validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(reg_pipe.predict(X_valid_KB))))\n    \n    alpha.append(i)\n\n\n# Sort by validation score\nresults_reg_KB = pd.DataFrame({'alpha':alpha,'validation MAE':validation_score})\nresults_reg_KB.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nalpha\nvalidation MAE\n\n\n\n\n2\n0.003\n3.332810e+07\n\n\n0\n0.001\n3.333645e+07\n\n\n3\n0.004\n3.334584e+07\n\n\n1\n0.002\n3.335439e+07\n\n\n4\n0.005\n3.336677e+07\n\n\n\n\n\n\n\n\n# Pick the best alpha \nreg_pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('SGDRegressor', SGDRegressor (alpha=0.003,max_iter=1000,penalty='elasticnet' ,tol=1e-3, loss='huber', random_state=0))\n    ])\n\n# Fit estimator\nreg_pipe.fit(X_train_KB, y_train['MC'])\n\n# Compute the test score\ntest_score_reg_KB = (MAE(np.exp(y_test['MC']), np.exp(reg_pipe.predict(X_test_KB))))\ntest_score_reg_KB\n\n21328567.545771167\n\n\nWe obtain a test score of 21’345’491 with the regularization."
  },
  {
    "objectID": "projet_2.html#d-model-fitting-with-a-subset---pca",
    "href": "projet_2.html#d-model-fitting-with-a-subset---pca",
    "title": "Estimating market capitalisation of companies using Machine Learning models",
    "section": "d) Model fitting with a subset - PCA",
    "text": "d) Model fitting with a subset - PCA\nWe saw before that I had many correlated variables. Therefore, in order to solve this problem, I am going to try to apply the PCA technique. First, I standardize the features:\n\n# standardize the variables \nscaler = StandardScaler()\nX_train_rescaled = scaler.fit_transform(X_train)\n\n\n# Import package\nfrom sklearn.decomposition import PCA\n\n# Create a PCA transformer\npca = PCA(n_components=None)\n\n# Apply PCA to the rescaled data\nX_train_pca=pca.fit_transform(X_train_rescaled)\nX_train_pca.shape\n\n(1840, 56)\n\n\nNow I am going to check the proportion of variance explained by the components.\n\n# Proportion of variance explained\npve = pca.explained_variance_ratio_\n\n# Cumulative sum of variances\npve_cumsum = np.cumsum(pve)\n\n\n# Plot\nfigure(figsize=(10, 6), dpi=80)\nxcor = np.arange(1, len(pve) + 1) \nplt.bar(xcor, pve,color='orange')\nplt.xticks(xcor)\npve_cumsum = np.cumsum(pve)\nplt.step(xcor+0.5, pve_cumsum, label='cumulative')\nplt.axhline(y=0.9, color='r', linestyle='-')\nplt.title('Proportion of the variance explained by the principal components')\nplt.xlabel('Principal Components')\nplt.ylabel('Proportion of the variance explained')\nplt.xticks(np.arange(0, pve_cumsum.shape[0], 10))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that about 20 components can explain 90% of the variance. Therefore, I am going to use this number of components.\n\nfrom sklearn.decomposition import PCA\n\n# create a PCA transformer with 0,90 of variance explained\npca = PCA(n_components = 0.90)\n\n# apply PCA to the rescaled data\nX_train_pca=pca.fit_transform(X_train_rescaled)\nX_train_pca.shape\n\n(1840, 21)\n\n\nI can also visualize the features on a 2d-plot and use a different color whether the company is considered as small, mid or large cap:\n\n# Create a PCA with 2 components and fit \npca2 = PCA(n_components=2)\nX_2d = pca2.fit_transform(X_train)\n\n\n# Plot\nfigure(figsize=(10, 6), dpi=80)\nsns.scatterplot(x=X_2d[:,0], y=X_2d[:,1],hue=categorical.loc[X_train.index,'Size'].values, hue_order = ['small', 'mid','large'])\nplt.title('First two components and size of companies')\nplt.xlabel('1st component')\nplt.ylabel('2nd component')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that ‘small companies’ have smaller 1st and 2nd components (bottom left corner in orange). In the other side, the largest companies in green are located on the top right corner.\nI need to apply the same transformations (standardization and PCA) to the test and validation set.\n\n# Standardize \nX_valid_rescaled = scaler.transform(X_valid)\nX_test_rescaled = scaler.transform(X_test)\n\n# Transform with the pca (90%)\nX_valid_pca = pca.transform(X_valid_rescaled)\nX_test_pca = pca.transform(X_test_rescaled)\n\n\nK-nearest neighbors (KNN)\n\n\n# Fit the model and tune parameters\nn_neighbors=[]\nvalidation_score=[]\n\nfor n in np.arange(1, 50, step=2):\n\n    # Create a k-NN pipeline\n    knn_pipe = Pipeline([\n        ('knn', KNeighborsRegressor(n_neighbors=n))\n    ])\n\n    # Fit estimator\n    knn_pipe.fit(X_train_pca, y_train['MC'])\n    \n    # Compute validation score on validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(knn_pipe.predict(X_valid_pca))))\n    \n    n_neighbors.append(n)\n\n\n# Sort by validation score\nresults_knn = pd.DataFrame({'n_neighbors':n_neighbors,'validation MAE':validation_score})\nresults_knn.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_neighbors\nvalidation MAE\n\n\n\n\n0\n1\n2.329356e+07\n\n\n3\n7\n2.954182e+07\n\n\n2\n5\n2.978214e+07\n\n\n4\n9\n3.001491e+07\n\n\n5\n11\n3.034130e+07\n\n\n\n\n\n\n\n\n# Pick the best n \nknn_pipe = Pipeline([\n        ('knn', KNeighborsRegressor(n_neighbors=1))\n    ])\n\n# Fit estimator \nknn_pipe.fit(X_train_pca, y_train['MC'])\n\n# Compute the test score\ntest_score_KNN_pca = (MAE(np.exp(y_test['MC']), np.exp(knn_pipe.predict(X_test_pca))))\ntest_score_KNN_pca\n\n16460404.860219397\n\n\nWe obtain a test score of 16’460’404 with the KNN model on the pca feature space.\n\nDecision tree and random forest\n\n\n# Fit the model and tune parameters\nn_trees=[]\nvalidation_score=[]\n\nfor n in np.arange(1,100,step=1):\n\n    # Create a random forest pipeline\n    rf_pipe = Pipeline([\n        ('rf', RandomForestRegressor (n_estimators=n , max_depth=None, random_state=0))\n    ])\n\n    # Fit estimator \n    rf_pipe.fit(X_train_pca, y_train['MC'])\n    \n    # Compute validation score on validation set\n    validation_score.append(MAE(np.exp(y_valid['MC']), np.exp(rf_pipe.predict(X_valid_pca))))\n    \n    n_trees.append(n)\n\n\n# Sort by validation score\nresults_rf = pd.DataFrame({'n_trees':n_trees,'validation MAE':validation_score})\nresults_rf.sort_values(by='validation MAE', ascending=True).head(5)\n\n\n\n\n\n\n\n\nn_trees\nvalidation MAE\n\n\n\n\n34\n35\n2.755373e+07\n\n\n9\n10\n2.756147e+07\n\n\n2\n3\n2.758124e+07\n\n\n35\n36\n2.763915e+07\n\n\n36\n37\n2.764401e+07\n\n\n\n\n\n\n\n\n# Pick the best number of trees\nrf_pipe = Pipeline([\n        ('rf', RandomForestRegressor (n_estimators=35 , max_depth=None, random_state=0))\n    ])\n\n# Fit estimator \nrf_pipe.fit(X_train_pca, y_train['MC'])\n\n# Compute the test score\ntest_score_RF_pca = (MAE(np.exp(y_test['MC']), np.exp(rf_pipe.predict(X_test_pca))))\ntest_score_RF_pca\n\n17197300.465914935\n\n\nWe obtain a test score of 17’197’300 with the random forest on the pca feature space.\nNo regularization for pca feature space: poor results of the model"
  },
  {
    "objectID": "projet_4.html",
    "href": "projet_4.html",
    "title": "Open food facts dataset: simple analysis",
    "section": "",
    "text": "The goal of this project was simply to clean and put the data in the best possible shape for analysis.\nThe dataset contains food products listing the ingredients and nutritional facts of more than 300’000 foods from over 150 countries in the world."
  },
  {
    "objectID": "projet_4.html#import-packages",
    "href": "projet_4.html#import-packages",
    "title": "Open food facts dataset: simple analysis",
    "section": "Import packages",
    "text": "Import packages\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "projet_4.html#import-the-data",
    "href": "projet_4.html#import-the-data",
    "title": "Open food facts dataset: simple analysis",
    "section": "Import the data",
    "text": "Import the data\nI opened the file in a text editor and I saw that the informations were separated by a tab. The file is really big so we have to use low_memory=False to open the file in chunks.\n\ndata = pd.read_csv('en.openfoodfacts.org.products.tsv', sep=\"\\t\", low_memory=False)"
  },
  {
    "objectID": "projet_4.html#basic-informations-on-the-database",
    "href": "projet_4.html#basic-informations-on-the-database",
    "title": "Open food facts dataset: simple analysis",
    "section": "Basic informations on the database",
    "text": "Basic informations on the database\nThe idea of this section was to inspect the database, understand how it is construct, the size etc.\n\ndata.shape\n\n(356027, 163)\n\n\n\ndata.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\nset_option is very usefull to look at all the elements we want to see. We can then have a great overview of the data and datatype.\n\npd.set_option('display.max_columns', 10)\ndata.head(1)\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\n...\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n0000000003087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows × 163 columns"
  },
  {
    "objectID": "projet_4.html#dealing-with-missing-values",
    "href": "projet_4.html#dealing-with-missing-values",
    "title": "Open food facts dataset: simple analysis",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nLet’s check first how many columns are completely empty. We delete them and check back if it is well done.\n\n(data.isnull().sum() == data.shape[0]).sum()\n\n16\n\n\n\ndata.dropna(how='all',inplace=True)\n\n\ndata.dropna(how='all', axis=1, inplace=True)\n\n\n(data.isnull().sum() == data.shape[0]).sum()\n\n0\n\n\n\npd.set_option('display.max_rows', 30)\ndata.isnull().sum()\n\ncode                                        26\nurl                                         26\ncreator                                      3\ncreated_t                                    3\ncreated_datetime                            10\nlast_modified_t                              0\nlast_modified_datetime                       0\nproduct_name                             17512\ngeneric_name                            298313\nquantity                                236742\npackaging                               266068\npackaging_tags                          266068\nbrands                                   29050\nbrands_tags                              29070\ncategories                              252728\n                                         ...  \nfluoride_100g                           355928\nselenium_100g                           354846\nchromium_100g                           356004\nmolybdenum_100g                         356007\niodine_100g                             355739\ncaffeine_100g                           355936\ntaurine_100g                            355994\nph_100g                                 355975\nfruits-vegetables-nuts_100g             352799\nfruits-vegetables-nuts-estimate_100g    355623\ncollagen-meat-protein-ratio_100g        355845\ncocoa_100g                              354644\ncarbon-footprint_100g                   355749\nnutrition-score-fr_100g                 101171\nnutrition-score-uk_100g                 101171\nLength: 147, dtype: int64\n\n\nWe can see that many columnns are almost empty. We have to select a threshold to delete columns that do not have enough data. I first use an arbitrary threshold of 50%. I will adjust it later if I think it is not adequate.\n\ndata.dropna(axis=1, thresh=0.5*data.shape[0],inplace=True)\ndata.isnull().sum()\n\ncode                           26\nurl                            26\ncreator                         3\ncreated_t                       3\ncreated_datetime               10\nlast_modified_t                 0\nlast_modified_datetime          0\nproduct_name                17512\nbrands                      29050\nbrands_tags                 29070\ncountries                     275\ncountries_tags                275\ncountries_en                  275\ningredients_text            72134\nserving_size               139406\n                            ...  \nnutrition_grade_fr         101171\nstates                         52\nstates_tags                    52\nstates_en                      52\nenergy_100g                 60660\nfat_100g                    76530\nsaturated-fat_100g          92204\ncarbohydrates_100g          76807\nsugars_100g                 76841\nfiber_100g                 135344\nproteins_100g               61866\nsalt_100g                   66288\nsodium_100g                 66333\nnutrition-score-fr_100g    101171\nnutrition-score-uk_100g    101171\nLength: 34, dtype: int64\n\n\nWe went from 147 columns to 34 columns with this threshold. We can see that we still have many interesting columnns such as the brands, countries, ingredients and the main nutritional variables. And even if we still have columns with a lot of missing values, each of them have more than 200’000 values which I guess is sufficient for the analysis. So this threshold could be great.\n\ndata.shape\n\n(356027, 34)\n\n\nHere we drop columns that display the same information in multiple formats. We keep the simplest format.\n\ndata.drop(['brands','countries','countries_tags','states','states_tags'],axis=1,inplace=True)\ndata.shape\n\n(356027, 29)\n\n\nWe can see that some columns have exactly the same number of missing values. It could be that they have they are the same even if they do not have the same name. For example, let’s check if nutrition_grade_fr, nutrition-score-fr_100g and nutrition-score-uk_100g are the same.\n\ndata['nutrition-score-fr_100g'].equals(data['nutrition-score-uk_100g'])\n\nFalse\n\n\n\ndata['nutrition-score-fr_100g'].equals(data['nutrition_grade_fr'])\n\nFalse\n\n\n\ndata['nutrition-score-uk_100g'].equals(data['nutrition_grade_fr'])\n\nFalse\n\n\nActually nutrition-grade_fr contains string grades so it was obvious that these columns were not equals.\nIt is also possible that the scores are linear combinations. In this case we could maybe delete one of them. Let’s check.\n\ndata.loc[data['nutrition-score-fr_100g'] != data['nutrition-score-uk_100g'], ['nutrition-score-fr_100g','nutrition-score-uk_100g']].dropna()[0:5]\n\n\n\n\n\n\n\n\nnutrition-score-fr_100g\nnutrition-score-uk_100g\n\n\n\n\n185\n18.0\n3.0\n\n\n190\n2.0\n0.0\n\n\n231\n13.0\n2.0\n\n\n238\n14.0\n2.0\n\n\n249\n13.0\n2.0\n\n\n\n\n\n\n\nIt is clearly not the case.\nWe can also check what is behind the columns additives_n and additives, and ingredients_from_palm_oil_n and ingredients_that_may_be_from_palm_oil_n. \n\ndata[['additives_n','additives']][0:5]\n\n\n\n\n\n\n\n\nadditives_n\nadditives\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n0.0\n[ bananas -&gt; en:bananas ] [ vegetable-oil -...\n\n\n2\n0.0\n[ peanuts -&gt; en:peanuts ] [ wheat-flour -&gt; ...\n\n\n3\n0.0\n[ organic-hazelnuts -&gt; en:organic-hazelnuts ...\n\n\n4\n0.0\n[ organic-polenta -&gt; en:organic-polenta ] [...\n\n\n\n\n\n\n\nWe can see that the two columns display two different informations. So we should keep boths columns. However, we will probably have to modify the structure since it looks like a bit messy.\n\ndata[['ingredients_from_palm_oil_n','ingredients_that_may_be_from_palm_oil_n']][0:5]\n\n\n\n\n\n\n\n\ningredients_from_palm_oil_n\ningredients_that_may_be_from_palm_oil_n\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n0.0\n0.0\n\n\n2\n0.0\n0.0\n\n\n3\n0.0\n0.0\n\n\n4\n0.0\n0.0\n\n\n\n\n\n\n\nIt looks like these two columns are equal. Let’s check.\n\ndata.loc[data['ingredients_from_palm_oil_n'] != data['ingredients_that_may_be_from_palm_oil_n'], ['ingredients_from_palm_oil_n','ingredients_that_may_be_from_palm_oil_n']].dropna()[0:5]\n\n\n\n\n\n\n\n\ningredients_from_palm_oil_n\ningredients_that_may_be_from_palm_oil_n\n\n\n\n\n48\n0.0\n3.0\n\n\n177\n0.0\n1.0\n\n\n205\n0.0\n1.0\n\n\n209\n0.0\n1.0\n\n\n214\n0.0\n1.0\n\n\n\n\n\n\n\nThese columns are different so we can keep it both.\nLet’s see one row without missing values to check if everything seems OK.\n\ndata.loc[data.isnull().sum(axis=1)==0][0:1] # I will often use this code to remember how my data are displayed. \n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\n...\nproteins_100g\nsalt_100g\nsodium_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\n\n\n\n\n2\n0000000004559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n...\n17.86\n0.635\n0.25\n0.0\n0.0\n\n\n\n\n1 rows × 29 columns\n\n\n\nAt first glance, everything seems OK except for the columns: - (1) created_t and last_modified_t: the date format is not very intuitive and the same information is disclose in the columns created_datetime and last_modified_datetime. Then I suggest to delete these columns. - (2) serving_size: it could be easier to extract a single column to only have the quantity in grams - (3) additives: since the structure does not look very simple, we should probably modify it\n\n\n\n\ndata.drop(['created_t','last_modified_t'],axis=1,inplace=True)\n\n\n(2)\n\ndata['serving_size'].head(10)\n\n0                NaN\n1       28 g (1 ONZ)\n2    28 g (0.25 cup)\n3    28 g (0.25 cup)\n4    35 g (0.25 cup)\n5     52 g (0.5 cup)\n6    45 g (0.25 cup)\n7     64 g (0.5 cup)\n8        40 g (40 g)\n9      14 g (1 Tbsp)\nName: serving_size, dtype: object\n\n\nI want to have a column which indicate the gramms. It could be easier for the analysis to display this column this way.\n\ndata['serving_size'] = data['serving_size'].str.split('g', expand=True)[0] # Actually, it would have been more logical to use replace here. \ndata['serving_size'].head(10)\n\n0    NaN\n1    28 \n2    28 \n3    28 \n4    35 \n5    52 \n6    45 \n7    64 \n8    40 \n9    14 \nName: serving_size, dtype: object\n\n\n\n\n\n\ndata['additives'].head(10)\n\n0                                                  NaN\n1     [ bananas -&gt; en:bananas  ]  [ vegetable-oil -...\n2     [ peanuts -&gt; en:peanuts  ]  [ wheat-flour -&gt; ...\n3     [ organic-hazelnuts -&gt; en:organic-hazelnuts  ...\n4     [ organic-polenta -&gt; en:organic-polenta  ]  [...\n5     [ rolled-oats -&gt; en:rolled-oats  ]  [ oats -&gt;...\n6     [ organic-long-grain-white-rice -&gt; en:organic...\n7     [ org-oats -&gt; en:org-oats  ]  [ oats -&gt; en:oa...\n8     [ organic-chocolate-liquor -&gt; en:organic-choc...\n9     [ organic-expeller-pressed -&gt; en:organic-expe...\nName: additives, dtype: object\n\n\n\ndata.loc[1,'additives']\n\n' [ bananas -&gt; en:bananas  ]  [ vegetable-oil -&gt; en:vegetable-oil  ]  [ oil -&gt; en:oil  ]  [ coconut-oil -&gt; en:coconut-oil  ]  [ oil -&gt; en:oil  ]  [ corn-oil-and-or-palm-oil-sugar -&gt; en:corn-oil-and-or-palm-oil-sugar  ]  [ oil-and-or-palm-oil-sugar -&gt; en:oil-and-or-palm-oil-sugar  ]  [ and-or-palm-oil-sugar -&gt; en:and-or-palm-oil-sugar  ]  [ or-palm-oil-sugar -&gt; en:or-palm-oil-sugar  ]  [ palm-oil-sugar -&gt; en:palm-oil-sugar  ]  [ oil-sugar -&gt; en:oil-sugar  ]  [ sugar -&gt; en:sugar  ]  [ natural-banana-flavor -&gt; en:natural-banana-flavor  ]  [ banana-flavor -&gt; en:banana-flavor  ]  [ flavor -&gt; en:flavor  ] '\n\n\nIt seems that we have an original name and a translation in english. I think that keeping the original is enough.\n\ndata['additives'] = data['additives'].str.replace('\\[','')\ndata['additives'] = data['additives'].str.replace('\\]',',')\ndata['additives'] = data['additives'].str.replace('-&gt;','')\ndata['additives'] = data['additives'].str.replace('[\\w]+:[\\w-]+','')\ndata['additives'] = data['additives'].str.replace('    ,',',')\ndata['additives'] = data['additives'].str.replace(',   ',', ')\ndata['additives']\n\n0                                                       NaN\n1           bananas, vegetable-oil, oil, coconut-oil, oi...\n2           peanuts, wheat-flour, flour, sugar, rice-flo...\n3           organic-hazelnuts, hazelnuts, organic-cashew...\n4                                organic-polenta, polenta, \n5           rolled-oats, oats, grape-concentrate, concen...\n6           organic-long-grain-white-rice, long-grain-wh...\n7           org-oats, oats, org-hemp-granola, hemp-grano...\n8           organic-chocolate-liquor, chocolate-liquor, ...\n9           organic-expeller-pressed, expeller-pressed, ...\n10              organic-adzuki-beans, adzuki-beans, beans, \n11          organic-refined-durum-semolina-wheat-flour, ...\n12          roasted-peanuts, peanuts, peanuts, peanut-or...\n13          organic-golden-flax-seeds, golden-flax-seeds...\n14          organic-dry-roasted-pumpkin-seeds, dry-roast...\n                                ...                        \n356012                                                  NaN\n356013                                                  NaN\n356014                                                  NaN\n356015                                                  NaN\n356016                                                  NaN\n356017      the-vert, the, arome-naturel-bergamote-avec-...\n356018                                                  NaN\n356019      viande-d-oie, viande-d, viande, graisse-de-c...\n356020                                                  NaN\n356021                                                  NaN\n356022      organic-peppermint, peppermint, organic-lemo...\n356023                                                  NaN\n356024                                                  NaN\n356025                                                  NaN\n356026      citric-acid     exists  -- ok  , maltodextri...\nName: additives, Length: 356027, dtype: object"
  },
  {
    "objectID": "projet_4.html#dealing-with-duplicates",
    "href": "projet_4.html#dealing-with-duplicates",
    "title": "Open food facts dataset: simple analysis",
    "section": "Dealing with duplicates",
    "text": "Dealing with duplicates\nNow that the columns are more or less cleaned, we have to check for duplicates. We first delete duplicated rows. Only two rows were exactly the same.\n\ndata.shape\n\n(356027, 27)\n\n\n\ndata.drop_duplicates(inplace=True)\ndata.shape\n\n(356026, 27)"
  },
  {
    "objectID": "projet_4.html#duplicated-values-corrected",
    "href": "projet_4.html#duplicated-values-corrected",
    "title": "Open food facts dataset: simple analysis",
    "section": "Duplicated values (corrected)",
    "text": "Duplicated values (corrected)\n\ndata['code'].is_unique\n\nFalse\n\n\n\ndata.duplicated(subset=['product_name','brands_tags','countries_en','additives_n', 'additives', 'fat_100g','proteins_100g',\n                        'carbohydrates_100g','sugars_100g','carbohydrates_100g','energy_100g','fiber_100g','sodium_100g',\n                       'nutrition_grade_fr']).sum()\n\n18281\n\n\n\ndata.drop_duplicates(subset=['product_name','brands_tags','countries_en','additives_n', 'additives', 'fat_100g','proteins_100g',\n                        'carbohydrates_100g','sugars_100g','carbohydrates_100g','energy_100g','fiber_100g','sodium_100g',\n                       'nutrition_grade_fr'], inplace= True)\n\n\ndata.shape\n\n(337745, 27)"
  },
  {
    "objectID": "t_animated.html",
    "href": "t_animated.html",
    "title": "How to animate your plots with R",
    "section": "",
    "text": "In progress\nThere are many ways to animate plots in R but I mainly the three following ways:\n\nusing gganimate package\nusing ggiraph package\nusing plotly package\n\n\n1. Gganimate\nChange between two states:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gapminder)\nlibrary(gifski)\n\ndata(\"gapminder\")\n\n\ndata &lt;- gapminder %&gt;%\n  mutate(gdpPercap = round(gdpPercap,0)) %&gt;%\n  filter(year == 1952 | year == 2007,\n         country %in% c(\"Switzerland\", \"France\", \"Germany\", \"Spain\", \"Italy\", \"Portugal\")) %&gt;%\n  arrange(desc(gdpPercap)) %&gt;%\n  mutate(country = factor(country, level = rev(unique(country))))\n\nplot_1 &lt;- ggplot(data) +\n  geom_col(aes(country, gdpPercap, fill = country)) +\n  scale_fill_manual(values = c(\"Switzerland\" = \"red\"), na.value = \"grey85\") +\n  scale_y_continuous(limits = c(0,40000),\n                     expand = c(0,0)) +\n  coord_flip(clip = \"off\") + \n  labs( title = \"Evolution du PIB entre 1952 et 2007\",\n        y = \"PIB par habitant (en $)\") +\n  geom_text(aes(x = 1, y = max(gdpPercap) , label = as.factor(year)), col = \"grey65\", size = 14) +\n  theme(\n    panel.background = element_blank(), \n    legend.position = \"none\",\n    title = element_text(size = 14),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_text(size = 12),\n    axis.line.x = element_line(color = \"grey55\"),\n    axis.text.x = element_text( color = \"grey55\", size = 12),\n    axis.ticks.x = element_line(color = \"grey55\"),\n    axis.title.x = element_text(color = \"grey35\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\")\n  ) +\n  transition_states(year, wrap = FALSE)\n\nanimate(plot_1, fps = 30, duration = 5, rewind = FALSE)\n\n\n\n\n\n\n\n\nChange between multiples states:\n\ndata &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  arrange(year, desc(gdpPercap)) %&gt;%\n  mutate(ranking = row_number()) %&gt;%\n  filter(ranking &lt;=15) %&gt;%\n  mutate(gdpPercap = round(gdpPercap,0))\n\n# 5. Créer l'animation avec une transition_states\nplot_2 &lt;- ggplot(data) +\n  geom_col(aes(ranking, gdpPercap, fill = country)) +\n  scale_fill_manual(values = c(\"Switzerland\" = \"red\"), na.value = \"grey85\") +\n  geom_text(aes(ranking, gdpPercap, label = as.character(gdpPercap)), color = \"grey25\", hjust=-0.1) +\n  geom_text(aes(ranking, y=0 , label = country), hjust=1.1) + \n  geom_text(aes(x=15, y=max(gdpPercap), hjust = 1, label = as.factor(year)),  col = \"grey65\", size = 11) +\n  labs(title = \"Evolution du top 15 des pays par PIB par habitant\") +\n  coord_flip(clip = \"off\") + \n  scale_x_reverse() +\n  theme_minimal() + \n  theme(\n    panel.grid = element_blank(), \n    legend.position = \"none\",\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    plot.title = element_text(size = 15),\n    plot.margin = margin(0.1, 0.1, 0.1, 0.2, \"npc\")\n  ) +\n  transition_states(year, transition_length = 5, state_length = 1) +\n  enter_fade() +\n  exit_fade() + \n  ease_aes('quadratic-in-out') \n\nanimate(plot_2, fps = 30, duration = 15, rewind = FALSE)\n\n\n\n\n\n\n\n\nTransition reveal:\ngeom_point() + transition_reveal(year)\n\ndata &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% summarise(lifeExp_mean = mean(lifeExp))\n\nggplot(data,\n       aes(year, lifeExp_mean, color = factor(continent), group = continent)) +\n  geom_line() +\n  scale_color_viridis_d() +\n  labs(x = \"\", \n       y = \"Average life expectancy\") +\n  theme_bw() +\n  geom_point() +\n  transition_reveal(year)\n\n\n\n\n\n\n\n\nTransition time:\n\nlibrary(tidyverse)\nlibrary(transformr)\n\ndf &lt;- read_csv(\"data/4_viz_6_line_data.csv\")\n\ndf &lt;- df %&gt;%\n  mutate( highlight=ifelse(year==\"2016\", \"2016\", \"Other\"))\n\nggplot(df,  aes(x=month, y=Mean, group=year, color= Mean )) +\n  geom_line() +\n  scale_color_gradient2(low = \"#68bd65\", mid = \"#fa792d\" , high = \"#da0116\" , midpoint = 0.5) +\n  scale_y_continuous(breaks = c(-0.5, 0, 0.5, 1, 1.5),\n                     labels = c(\"-0.5\", \"0\", \"+0.5\", \"+1\", \"+1.5\")) +\n  scale_x_continuous(breaks = c(1:12), labels = c(1:12), limits = c(1,12)) +\n  theme_minimal() +\n  geom_hline( yintercept = 0, linetype=\"dotted\", color = \"gray30\", size = 0.5 ) +\n  labs(title = \"Anomalies de tempéatures dans le monde de 1880 à 2016\", \n       subtitle = 'Année: {round(frame_time,0)}', \n       x = \"Mois de l'année\", y = 'Degrés') +\n  theme(legend.position=\"none\",\n        plot.title = element_text( face = \"bold\", size =14),\n        panel.grid.minor = element_blank()) +\n  transition_time(year) +\n  ease_aes(\"linear\")\n\n\n\n\n\n\n\n\n\n\n2. Ggiraph\n\nlibrary(ggiraph)\nlibrary(datasets)\nlibrary(htmltools)\n\n\n# 1. Graphique de point de base\nggplot(iris, aes(x = Petal.Length, y = Petal.Width,color = Species)) + \n  geom_point( ) +\n  labs(title = \"Different species of Iris\") +\n  scale_color_manual(values = c(\"indianred1\",\"darkolivegreen3\",\"cyan3\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Version améliorée\nplot_1 &lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point_interactive(aes(tooltip = Species,\n                             data_id = Species,\n                             onclick = sprintf(\"window.open(\\\"http://google.com/search?q=iris+%s\\\")\", Species)),\n                         hover_nearest = TRUE) +\n  labs(title = \"Different species of Iris\") +\n  scale_color_manual_interactive(values = c(\"indianred1\",\"darkolivegreen3\",\"cyan3\")) +\n  theme_minimal()\n\ngirafe(ggobj = plot_1,\n       options = list(opts_zoom(min = .7, max = 2),\n         opts_tooltip(use_fill = TRUE),\n         opts_hover(css = \"fill:white;stroke:black;stroke-width:1px;\"),\n         opts_hover_inv(css = \"opacity:0.1;\"),\n         opts_toolbar(position = \"bottom\", pngname = \"mon_premier_graphique\")))\n\n\n\n\n\n\ndata &lt;- iris %&gt;% \n  group_by(Species) %&gt;% \n  summarise(mean_petal = mean(Petal.Length),\n            mean_sepal = mean(Sepal.Length)) %&gt;%\n  pivot_longer(-Species, values_to = \"values\", names_to = \"type\")\n\nggplot(data, aes(x = Species, y = values, fill = type)) +\n  geom_bar( position=\"dodge\", stat=\"identity\") +\n  scale_fill_manual(values = c(\"antiquewhite2\",\"darksalmon\")) +\n  labs( title = \"Taille moyenne des pétales et sépales des Iris\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Version améliorée\nfont_family_exists(\"comic sans ms\")\n\n[1] TRUE\n\nplot_2 &lt;- ggplot(data, aes(x = Species, y = values, fill = type)) +\n  geom_bar_interactive(aes(tooltip = type,\n                           data_id = type), position=\"dodge\", stat=\"identity\") +\n  scale_fill_manual(values = c(\"antiquewhite2\",\"darksalmon\")) +\n  labs( title = \"Taille moyenne des pétales et sépales des Iris\") +\n  theme_minimal() +\n  theme(plot.title = element_text_interactive(data_id = \"plot.title\",\n                                              tooltip = \"Source: dataset Iris\",\n                                              hover_css = \"fill:#1273aa;stroke:none;\"))\n\ngirafe(ggobj = plot_2,\n       options = list(opts_hover(css = \"fill:#b0e7ee;stroke:#1273aa;stroke-width:1px;\"),\n       opts_hover_inv(css = \"opacity:0;\"),\n       opts_tooltip(css = \"fill:white\")),\n       fonts = list(sans = \"comic sans ms\"))\n\n\n\n\n\n\n# 3. Graphique de ligne de base\ndata &lt;- iris %&gt;% \n  mutate(id = row_number())\n\nggplot(data, aes(Petal.Length, Sepal.Length)) + \n  geom_point(aes(color = Petal.Length)) +\n  scale_colour_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n# Version améliorée\npreselection &lt;- data$id[35:65]\n  \nplot_3 &lt;- ggplot(data, aes(Petal.Length, Sepal.Length)) + \n  geom_point_interactive(aes(color = Petal.Length, \n                             data_id = id,\n                             tooltip = id)) +\n  scale_colour_viridis_c_interactive() +\n  theme_minimal()\n\ngirafe(ggobj = plot_3, \n       options = list(\n       opts_hover(css = \"fill:red;stroke:#1273aa\"),\n       opts_hover_inv(css = \"opacity:0.1;\"),\n       opts_tooltip(use_fill = TRUE),\n       opts_selection(selected = preselection, type = \"multiple\", only_shiny = FALSE)))\n\n\n\n\n\n\n\n3. Plotly"
  },
  {
    "objectID": "visualization_using_Tableau.html#bump-chart",
    "href": "visualization_using_Tableau.html#bump-chart",
    "title": "Visualization using Tableau",
    "section": "2.1. Bump chart",
    "text": "2.1. Bump chart"
  },
  {
    "objectID": "visualization_using_Tableau.html#lolipop-chart",
    "href": "visualization_using_Tableau.html#lolipop-chart",
    "title": "Visualization using Tableau",
    "section": "2.2. Lolipop chart",
    "text": "2.2. Lolipop chart"
  },
  {
    "objectID": "visualization_using_Tableau.html#area-chart",
    "href": "visualization_using_Tableau.html#area-chart",
    "title": "Visualization using Tableau",
    "section": "2.3. Area chart",
    "text": "2.3. Area chart"
  },
  {
    "objectID": "visualization_using_Tableau.html#simple-bar-charts",
    "href": "visualization_using_Tableau.html#simple-bar-charts",
    "title": "Visualization using Tableau",
    "section": "3.1. Simple bar charts",
    "text": "3.1. Simple bar charts"
  },
  {
    "objectID": "visualization_using_Tableau.html#stacked-bar-charts",
    "href": "visualization_using_Tableau.html#stacked-bar-charts",
    "title": "Visualization using Tableau",
    "section": "3.2. Stacked bar charts",
    "text": "3.2. Stacked bar charts"
  },
  {
    "objectID": "visualization_using_Tableau.html#lolipop-chart-1",
    "href": "visualization_using_Tableau.html#lolipop-chart-1",
    "title": "Visualization using Tableau",
    "section": "3.3. Lolipop chart",
    "text": "3.3. Lolipop chart"
  },
  {
    "objectID": "visualization_using_Tableau.html#waffle-chart",
    "href": "visualization_using_Tableau.html#waffle-chart",
    "title": "Visualization using Tableau",
    "section": "4.1. Waffle chart",
    "text": "4.1. Waffle chart"
  },
  {
    "objectID": "visualization_using_Tableau.html#tables",
    "href": "visualization_using_Tableau.html#tables",
    "title": "Visualization using Tableau",
    "section": "4.2. Tables",
    "text": "4.2. Tables"
  },
  {
    "objectID": "visualization_using_Tableau.html#treemap",
    "href": "visualization_using_Tableau.html#treemap",
    "title": "Visualization using Tableau",
    "section": "4.3. Treemap",
    "text": "4.3. Treemap"
  },
  {
    "objectID": "visualization_using_Tableau.html#scatter-plots",
    "href": "visualization_using_Tableau.html#scatter-plots",
    "title": "Visualization using Tableau",
    "section": "5.1. Scatter plots",
    "text": "5.1. Scatter plots"
  },
  {
    "objectID": "visualization_using_Tableau.html#boxplots",
    "href": "visualization_using_Tableau.html#boxplots",
    "title": "Visualization using Tableau",
    "section": "6.1. Boxplots",
    "text": "6.1. Boxplots"
  },
  {
    "objectID": "w_projet_2.html",
    "href": "w_projet_2.html",
    "title": "Performance analysis of marathon runners using Strava data (In progress)",
    "section": "",
    "text": "In development"
  },
  {
    "objectID": "t_maps.html",
    "href": "t_maps.html",
    "title": "How to plot maps with R ?",
    "section": "",
    "text": "There are many ways to plot maps in R but I mainly the three following ways:\n\nusing shapefiles, ggplot and sf packages\nusing rayshader packages\nusing ggmap (some functions depreciated in october 2023)\n\n\n1. Shapefiles, ggplot and sf packages\nInstall packages:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"sf\")\ninstall.packages(\"rcartocolor\")\n\nLoad packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rcartocolor)\n\nSet environment:\n\nSys.setenv(SHAPE_RESTORE_SHX=\"YES\")\n\nShapefile is a special format for geospatial data. It can describes many different features such as polygons, points or lines to create the underlying elements of a map. I have here two different shapefiles, one describing lakes and another one describing swiss cantons.\nImport shapefiles “1_5_ch_lac.shp” and “1_5_ch_canton.shp” using the function read_sf():\n\nlac &lt;- read_sf(\"data/1_5_ch_lac.shp\")\ncanton &lt;- read_sf(\"data/1_5_ch_canton.shp\")\n\nImport csv file containing my features values “1_5_tabac.csv”:\n\nvotation &lt;- read_csv(\"data/1_5_tabac.csv\")\n\nJoin both datasets by canton:\n\ndataset  &lt;- canton %&gt;% \n  left_join(votation, c(\"KURZ\"=\"KURZ\")) \n\nCreate a map of Switzerland using the ggplot and geom_sf() functions to show the % of votes per canton:\n\ngraph_1 &lt;- ggplot(dataset) +\n  geom_sf(aes(fill=`Oui en %`))\n\ngraph_1\n\n\n\n\n\n\n\n\nAdd a color palette using the scale_fill_carto_c() function:\n\ngraph_2 &lt;- graph_1 +\n  scale_fill_carto_c(palette=\"Geyser\", direction=-1, limit=c(25,75))\n\ngraph_2\n\n\n\n\n\n\n\n\nAdd lakes using geom_sf():\n\ngraph_3 &lt;- graph_2 +\n  geom_sf(data=lac,  fill=\"lightblue2\") \n\ngraph_3\n\n\n\n\n\n\n\n\nAdd a title, subtitle and caption, then a theme:\n\ngraph_4 &lt;- graph_3 +\n  labs(subtitle=\"Oui à la protection des enfants et des jeunes contre la publicité pour le tabac\",\n       title=\"Résultats de la votation populaire du 13 février 2022.\",\n       x=\"\",\n       y=\"\",\n       caption=\"Source: Office fédéral de la statistique \") +\n  theme_void() + \n  theme(plot.title=element_text( face=\"bold\", size =14),\n        plot.subtitle=element_text( face=\"italic\"))\n\ngraph_4\n\n\n\n\n\n\n\n\nAdd labels:\n\ngraph_5 &lt;- ggplot(dataset, aes(fill=`Oui en %`, label=round(`Oui en %`,0))) +\n  scale_fill_carto_c(palette=\"Geyser\", direction=-1, limit=c(25,75)) +\n  labs(subtitle=\"Oui à la protection des enfants et des jeunes contre la publicité pour le tabac\",\n       title=\"Résultats de la votation populaire du 13 février 2022.\",\n       x=\"\",\n       y=\"\",\n       caption=\"Source: Office fédéral de la statistique \") +\n  theme_void() + \n  theme(plot.title=element_text( face=\"bold\", size =14),\n        plot.subtitle=element_text( face=\"italic\")) +\n  geom_sf() +\n  geom_sf_label(alpha=0.8, color=\"grey10\", size=3, position=\"jitter\")\n\ngraph_5\n\n\n\n\n\n\n\n\nSave plot:\n\nggsave(\"images/1_5_map.png\")\n\n\n\n2. Rayshader package\nFirst I create the map.\nInstall packages:\n\ninstall.packages(\"sf\")\ninstall.packages(\"tigris\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"stars\")\ninstall.packages(\"MetBrewer\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"colorspace\")\ninstall.packages(\"rgl\")\ninstall.packages(\"rayshader\")\ninstall.packages(\"rayrender\")\ninstall.packages(\"rayvertex\")\ninstall.packages(\"RColorBrewer\")\n\nLoad packages:\n\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(MetBrewer)\nlibrary(RColorBrewer)\nlibrary(colorspace)\nlibrary(rgl)\nlibrary(rayshader)\nlibrary(rayrender)\nlibrary(rayvertex)\nlibrary(RColorBrewer)\n\nImport kontur dataset:\n\ndata &lt;- st_read(\"data/kontur_population_CH_20220630.gpkg\")\n\nReading layer `population' from data source \n  `C:\\Users\\grego\\Desktop\\GregoireUrvoy_maintenance\\data\\kontur_population_CH_20220630.gpkg' \n  using driver `GPKG'\nSimple feature collection with 44089 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 662371.5 ymin: 5750282 xmax: 1168910 ymax: 6075544\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ntemp_shapefile &lt;- tempfile()\ndownload.file(\"https://data.geo.admin.ch/ch.swisstopo.swissboundaries3d/swissboundaries3d_2023-01/swissboundaries3d_2023-01_2056_5728.shp.zip\", temp_shapefile)\nunzip(temp_shapefile)\n\nDefine the aspect ratios:\n\nbb &lt;- st_bbox(data)\n\nbottom_left &lt;- st_point(c(bb[[\"xmin\"]], bb[[\"ymin\"]])) |&gt; \n  st_sfc(crs=st_crs(data))\n\nbottom_right &lt;- st_point(c(bb[[\"xmax\"]], bb[[\"ymin\"]])) |&gt; \n  st_sfc(crs=st_crs(data))\nwidth &lt;- st_distance(bottom_left, bottom_right)\n\ntop_left &lt;- st_point(c(bb[[\"xmin\"]], bb[[\"ymax\"]])) |&gt; \n  st_sfc(crs=st_crs(data))\n\nheight &lt;- st_distance(bottom_left, top_left)\n\nif (width &gt; height) {\n  w_ratio &lt;- 1\n  h_ratio &lt;- height / width\n} else {\n  h_ration &lt;- 1\n  w_ratio &lt;- width / height\n}\n\nsize &lt;- 3000\n\ndata_rast &lt;- st_rasterize(data, \n                             nx=floor(size * w_ratio),\n                             ny=floor(size * h_ratio))\n\nmat &lt;- matrix(data_rast$population, \n              nrow=floor(size * w_ratio),\n              ncol=floor(size * h_ratio))\n\nDefine colors:\n\nc1 &lt;- met.brewer(\"Hokusai2\")\nswatchplot(c1)\n\n\n\n\n\n\n\ntexture &lt;- grDevices::colorRampPalette(c1, bias=2)(256)\nswatchplot(texture)\n\n\n\n\n\n\n\n\nCreate plot:\n\nmat |&gt; \n height_shade(texture=texture) |&gt; \n plot_3d(heightmap=mat,\n          zscale=100 / 5,\n          solid=FALSE,\n          shadow= FALSE,\n          background=\"#e9f5f9\") \nrender_camera(theta=0, phi=65, zoom=.9) \nrender_snapshot(filename=\"images/test_final.png\")\nrender_highquality(filename=\"images/ch_map.png\")\n\n\n\n\nThen, I had additional aesthetic elements.\nInstall packages:\n\ninstall.packages(magick)\ninstall.packages(glue)\ninstall.packages(stringr)\n\nLoad packages:\n\nlibrary(magick)\nlibrary(glue)\nlibrary(stringr)\n\nImport map:\n\nimg &lt;- image_read(\"images/ch_map.png\")\n\nCreate colors:\n\ncolors &lt;- met.brewer(\"Hokusai2\")\n\nAdd annotations to the plot and write image:\n\nimg %&gt;% \n  image_crop(gravity=\"center\",\n             geometry=\"6000x3500+0-150\") %&gt;% \n  image_annotate(\"Switzerland\",\n                 gravity=\"north\",\n                 location=\"+0+50\",\n                 color=\"#3E69AB\",\n                 size=45,\n                 weight=70,\n                 font=\"Palatino\") %&gt;% \n  image_annotate(\"Population Density\",\n                 gravity=\"north\",\n                 location=\"+0+100\",\n                 color=\"#3E69AB\",\n                 size=30,\n                 weight=70,\n                 font=\"Palatino\") %&gt;% \n  image_annotate(glue(\"Graphic by Grégoire Urvoy | \",\n                      \"Data: Kontur Population \"),\n                 gravity=\"south\",\n                 location=\"+0+40\",\n                 font=\"El Messiri\",\n                 color=\"#808080\",\n                 size=10)  %&gt;%  \n  image_write(\"images/final_ch_map.png\")\n\n\n\n\n\n\n3. Ggpmap package\nUpdate: Stamen has been depreciated since October 2023. More information here."
  }
]